# main.py - ÙˆØ§Ø¬Ù‡Ø© Streamlit Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
import streamlit as st
import os
from pathlib import Path
from src.rag_pipeline import RAGPipeline
from src.document_processor import DocumentProcessor
import tempfile

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ğŸ”¥",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
st.title("ğŸ”¥ Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø°ÙƒÙŠÙŠÙ†")
st.markdown("---")

# Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
st.sidebar.title("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")

# Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
uploaded_files = st.sidebar.file_uploader(
    "ğŸ“ Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª",
    type=['pdf', 'txt', 'docx'],
    accept_multiple_files=True
)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
temperature = st.sidebar.slider("ğŸŒ¡ï¸ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©", 0.0, 1.0, 0.1)
max_tokens = st.sidebar.slider("ğŸ“ Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ù…ÙˆØ²", 100, 4000, 2000)

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
@st.cache_resource
def initialize_rag():
    return RAGPipeline()

rag_system = initialize_rag()

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©
if uploaded_files:
    st.sidebar.success(f"âœ… ØªÙ… Ø±ÙØ¹ {len(uploaded_files)} Ù…Ù„Ù")
    
    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø¤Ù‚ØªØ§Ù‹
    temp_dir = tempfile.mkdtemp()
    file_paths = []
    
    for uploaded_file in uploaded_files:
        file_path = os.path.join(temp_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        file_paths.append(file_path)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
    if st.sidebar.button("ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"):
        with st.spinner("Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª..."):
            try:
                rag_system.load_documents(file_paths)
                st.sidebar.success("âœ… ØªÙ…Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­!")
            except Exception as e:
                st.sidebar.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}")

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
st.header("ğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ")

# ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
if "messages" not in st.session_state:
    st.session_state.messages = []

# Ø¹Ø±Ø¶ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§..."):
    # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø¯
    with st.chat_message("assistant"):
        with st.spinner("Ø¬Ø§Ø±Ù Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„..."):
            try:
                response = rag_system.query(
                    prompt,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                st.markdown(response["answer"])
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø±
                if response.get("sources"):
                    with st.expander("ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø±"):
                        for i, source in enumerate(response["sources"], 1):
                            st.markdown(f"**Ø§Ù„Ù…ØµØ¯Ø± {i}:** {source}")
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø¯ Ù„Ù„ØªØ§Ø±ÙŠØ®
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response["answer"]
                })
                
            except Exception as e:
                error_msg = f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": error_msg
                })

# Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
with st.sidebar.expander("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"):
    st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª", rag_system.get_document_count())
    st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹", rag_system.get_chunk_count())

# Ø²Ø± Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
if st.sidebar.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
    st.session_state.messages = []
    st.rerun()

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø·ÙˆØ±
st.sidebar.markdown("---")
st.sidebar.markdown("**Ø§Ù„Ù…Ø·ÙˆØ±:** Ù…Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø£ÙˆØ§Ù…Ø± ğŸ‘©â€ğŸ’»")
st.sidebar.markdown("**Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:** Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")

if __name__ == "__main__":
    st.markdown("""
    ### ğŸš€ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    1. **Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª** Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    2. **Ø§Ø¶ØºØ· Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª** Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§
    3. **Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ** ÙÙŠ ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©
    4. **Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø©** Ù…Ø¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±
    """)

# src/rag_pipeline.py - Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù…
import os
from typing import List, Dict, Any
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from .document_processor import DocumentProcessor
from .agents.planning_agent import PlanningAgent
from .agents.retrieval_agent import RetrievalAgent
from .agents.synthesis_agent import SynthesisAgent

class RAGPipeline:
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        self.embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)
        self.llm = OpenAI(
            temperature=0.1,
            openai_api_key=self.api_key,
            model_name="gpt-3.5-turbo-instruct"
        )
        
        self.vector_store = None
        self.document_processor = DocumentProcessor()
        
        # Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø°ÙƒÙŠÙŠÙ†
        self.planning_agent = PlanningAgent(self.llm)
        self.retrieval_agent = RetrievalAgent()
        self.synthesis_agent = SynthesisAgent(self.llm)
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )

    def load_documents(self, file_paths: List[str]):
        """ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        all_documents = []
        
        for file_path in file_paths:
            documents = self.document_processor.process_document(file_path)
            all_documents.extend(documents)
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ
        texts = self.text_splitter.split_documents(all_documents)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©
        self.vector_store = Chroma.from_documents(
            documents=texts,
            embedding=self.embeddings,
            persist_directory="./chroma_db"
        )
        
        self.retrieval_agent.set_vector_store(self.vector_store)
        
        return len(texts)

    def query(self, question: str, temperature: float = 0.1, max_tokens: int = 2000) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        if not self.vector_store:
            return {
                "answer": "âŒ ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø£ÙˆÙ„Ø§Ù‹ Ù‚Ø¨Ù„ Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©",
                "sources": []
            }
        
        try:
            # Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù„ÙŠÙ„ ÙˆØªØ®Ø·ÙŠØ· Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            plan = self.planning_agent.analyze_query(question)
            
            # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
            retrieved_docs = self.retrieval_agent.retrieve_documents(
                question, 
                plan.get("retrieval_strategy", "semantic")
            )
            
            # Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            response = self.synthesis_agent.synthesize_answer(
                question=question,
                documents=retrieved_docs,
                plan=plan,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response
            
        except Exception as e:
            return {
                "answer": f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}",
                "sources": []
            }

    def get_document_count(self) -> int:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        if self.vector_store:
            return len(self.vector_store.get()["ids"])
        return 0

    def get_chunk_count(self) -> int:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""
        if self.vector_store:
            return len(self.vector_store.get()["ids"])
        return 0

# src/document_processor.py - Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
import os
from pathlib import Path
from typing import List, Dict
from langchain.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
from langchain.schema import Document

class DocumentProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
    
    def __init__(self):
        self.supported_formats = {'.txt', '.pdf', '.docx'}
    
    def process_document(self, file_path: str) -> List[Document]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³ØªÙ†Ø¯ ÙˆØ§Ø­Ø¯"""
        file_extension = Path(file_path).suffix.lower()
        
        if file_extension not in self.supported_formats:
            raise ValueError(f"ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ù„Ù {file_extension} ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…")
        
        try:
            if file_extension == '.txt':
                loader = TextLoader(file_path, encoding='utf-8')
            elif file_extension == '.pdf':
                loader = PyPDFLoader(file_path)
            elif file_extension == '.docx':
                loader = Docx2txtLoader(file_path)
            
            documents = loader.load()
            
            # Ø¥Ø¶Ø§ÙØ© metadata
            for doc in documents:
                doc.metadata.update({
                    'source': file_path,
                    'file_type': file_extension,
                    'file_name': Path(file_path).name
                })
            
            return documents
            
        except Exception as e:
            raise Exception(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù {file_path}: {str(e)}")
    
    def process_multiple_documents(self, file_paths: List[str]) -> List[Document]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¯Ø© Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        all_documents = []
        
        for file_path in file_paths:
            documents = self.process_document(file_path)
            all_documents.extend(documents)
        
        return all_documents

# src/agents/planning_agent.py - ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ®Ø·ÙŠØ·
from typing import Dict, Any
from langchain.llms.base import LLM
from langchain.prompts import PromptTemplate

class PlanningAgent:
    """ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ®Ø·ÙŠØ· - ÙŠØ­Ù„Ù„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙˆÙŠØ®Ø·Ø· Ù„Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹"""
    
    def __init__(self, llm: LLM):
        self.llm = llm
        self.planning_template = PromptTemplate(
            input_variables=["question"],
            template="""
            Ø­Ù„Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ­Ø¯Ø¯ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©:
            
            Ø§Ù„Ø³Ø¤Ø§Ù„: {question}
            
            Ø­Ø¯Ø¯:
            1. Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ (ÙˆØ§Ù‚Ø¹ÙŠØŒ ØªØ­Ù„ÙŠÙ„ÙŠØŒ Ù…Ù‚Ø§Ø±Ù†ØŒ Ø¥Ø¬Ø±Ø§Ø¦ÙŠ)
            2. Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ (semantic, keyword, hybrid)
            3. Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (1-10)
            4. Ù‡Ù„ ÙŠØ­ØªØ§Ø¬ ØªÙÙƒÙŠØ± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§ØªØŸ
            
            Ø§Ù„Ø±Ø¯ Ø¨ØµÙŠØºØ© JSON:
            """
        )
    
    def analyze_query(self, question: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø®Ø·Ø©"""
        try:
            prompt = self.planning_template.format(question=question)
            response = self.llm(prompt)
            
            # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· (ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… JSON parsing)
            plan = {
                "query_type": self._determine_query_type(question),
                "retrieval_strategy": self._determine_retrieval_strategy(question),
                "num_documents": self._determine_num_documents(question),
                "multi_step": self._requires_multi_step(question)
            }
            
            return plan
            
        except Exception as e:
            # Ø®Ø·Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
            return {
                "query_type": "factual",
                "retrieval_strategy": "semantic",
                "num_documents": 5,
                "multi_step": False
            }
    
    def _determine_query_type(self, question: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„"""
        analytical_keywords = ['Ù„Ù…Ø§Ø°Ø§', 'ÙƒÙŠÙ', 'ØªØ­Ù„ÙŠÙ„', 'Ù…Ù‚Ø§Ø±Ù†Ø©', 'ØªÙØ³ÙŠØ±']
        factual_keywords = ['Ù…Ø§ Ù‡Ùˆ', 'Ù…ØªÙ‰', 'Ø£ÙŠÙ†', 'Ù…Ù† Ù‡Ùˆ']
        
        question_lower = question.lower()
        
        for keyword in analytical_keywords:
            if keyword in question_lower:
                return "analytical"
        
        for keyword in factual_keywords:
            if keyword in question_lower:
                return "factual"
        
        return "general"
    
    def _determine_retrieval_strategy(self, question: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹"""
        if len(question.split()) > 10:
            return "hybrid"
        elif any(word in question for word in ['ØªØ­Ø¯ÙŠØ¯Ø§Ù‹', 'Ø¨Ø§Ù„Ø¶Ø¨Ø·', 'ØªÙ…Ø§Ù…Ø§Ù‹']):
            return "keyword"
        else:
            return "semantic"
    
    def _determine_num_documents(self, question: str) -> int:
        """ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
        if 'Ù…Ù‚Ø§Ø±Ù†Ø©' in question or 'Ù…Ù‚Ø§Ø±Ù†' in question:
            return 8
        elif 'ØªØ­Ù„ÙŠÙ„' in question or 'Ø´Ø±Ø­ Ù…ÙØµÙ„' in question:
            return 6
        else:
            return 4
    
    def _requires_multi_step(self, question: str) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ­ØªØ§Ø¬ ØªÙÙƒÙŠØ± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª"""
        multi_step_indicators = [
            'Ø«Ù…', 'Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ', 'Ø£ÙˆÙ„Ø§Ù‹', 'Ø«Ø§Ù†ÙŠØ§Ù‹', 
            'Ø®Ø·ÙˆØ§Øª', 'Ù…Ø±Ø§Ø­Ù„', 'Ø¹Ù…Ù„ÙŠØ©'
        ]
        
        return any(indicator in question for indicator in multi_step_indicators)

# src/agents/retrieval_agent.py - ÙˆÙƒÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
from typing import List, Dict, Any
from langchain.vectorstores.base import VectorStore
from langchain.schema import Document

class RetrievalAgent:
    """ÙˆÙƒÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ - ÙŠØ¨Ø­Ø« ÙˆÙŠØ³ØªØ±Ø¬Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©"""
    
    def __init__(self):
        self.vector_store = None
    
    def set_vector_store(self, vector_store: VectorStore):
        """ØªØ¹ÙŠÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©"""
        self.vector_store = vector_store
    
    def retrieve_documents(
        self, 
        query: str, 
        strategy: str = "semantic",
        k: int = 4
    ) -> List[Document]:
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©"""
        if not self.vector_store:
            return []
        
        try:
            if strategy == "semantic":
                return self._semantic_retrieval(query, k)
            elif strategy == "keyword":
                return self._keyword_retrieval(query, k)
            elif strategy == "hybrid":
                return self._hybrid_retrieval(query, k)
            else:
                return self._semantic_retrieval(query, k)
                
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {e}")
            return []
    
    def _semantic_retrieval(self, query: str, k: int) -> List[Document]:
        """Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        return self.vector_store.similarity_search(query, k=k)
    
    def _keyword_retrieval(self, query: str, k: int) -> List[Document]:
        """Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        # ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (Ù…Ø¨Ø³Ø·)
        docs = self.vector_store.similarity_search(query, k=k*2)
        
        # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ø§Ù„Ø³Ø¤Ø§Ù„
        query_words = query.lower().split()
        filtered_docs = []
        
        for doc in docs:
            doc_text = doc.page_content.lower()
            score = sum(1 for word in query_words if word in doc_text)
            if score > 0:
                filtered_docs.append((doc, score))
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø£ÙØ¶Ù„ k
        filtered_docs.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in filtered_docs[:k]]
    
    def _hybrid_retrieval(self, query: str, k: int) -> List[Document]:
        """Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·"""
        # Ø¯Ù…Ø¬ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        semantic_docs = self._semantic_retrieval(query, k//2)
        keyword_docs = self._keyword_retrieval(query, k//2)
        
        # Ø¯Ù…Ø¬ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        all_docs = semantic_docs + keyword_docs
        unique_docs = []
        seen_content = set()
        
        for doc in all_docs:
            if doc.page_content not in seen_content:
                unique_docs.append(doc)
                seen_content.add(doc.page_content)
        
        return unique_docs[:k]

# src/agents/synthesis_agent.py - ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ±ÙƒÙŠØ¨
from typing import List, Dict, Any
from langchain.schema import Document
from langchain.llms.base import LLM
from langchain.prompts import PromptTemplate

class SynthesisAgent:
    """ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ±ÙƒÙŠØ¨ - ÙŠØ¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆÙŠÙ†Ø´Ø¦ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª"""
    
    def __init__(self, llm: LLM):
        self.llm = llm
        self.synthesis_template = PromptTemplate(
            input_variables=["question", "context", "plan_info"],
            template="""
            Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ§Ù„ÙŠØŒ Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¯Ù‚Ø© ÙˆÙˆØ¶ÙˆØ­.
            
            Ø§Ù„Ø³Ø¤Ø§Ù„: {question}
            
            Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ®Ø·ÙŠØ·: {plan_info}
            
            Ø§Ù„Ø³ÙŠØ§Ù‚:
            {context}
            
            ØªØ¹Ù„ÙŠÙ…Ø§Øª:
            1. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø·
            2. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙƒØ§Ù…Ù„Ø©ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ
            3. Ø§Ø°ÙƒØ± Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
            4. ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙ…ÙØµÙ„Ø§Ù‹
            5. ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
            
            Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
            """
        )
    
    def synthesize_answer(
        self,
        question: str,
        documents: List[Document],
        plan: Dict[str, Any],
        temperature: float = 0.1,
        max_tokens: int = 2000
    ) -> Dict[str, Any]:
        """ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©"""
        
        if not documents:
            return {
                "answer": "âŒ Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.",
                "sources": []
            }
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚
        context = self._prepare_context(documents)
        plan_info = self._format_plan_info(plan)
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ Ù‚Ø¨Ù„ Ø§Ù„ØªØ±ÙƒÙŠØ¨
        if not self._validate_context_quality(question, context):
            return {
                "answer": "âš ï¸ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø¯Ù‚Ø©.",
                "sources": self._extract_sources(documents)
            }
        
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            prompt = self.synthesis_template.format(
                question=question,
                context=context,
                plan_info=plan_info
            )
            
            answer = self.llm(prompt, temperature=temperature, max_tokens=max_tokens)
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            final_answer = self._post_process_answer(answer, question)
            
            return {
                "answer": final_answer,
                "sources": self._extract_sources(documents),
                "confidence": self._calculate_confidence(documents, question)
            }
            
        except Exception as e:
            return {
                "answer": f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {str(e)}",
                "sources": []
            }
    
    def _prepare_context(self, documents: List[Document]) -> str:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        context_parts = []
        
        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get('file_name', f'Ù…Ø³ØªÙ†Ø¯ {i}')
            content = doc.page_content.strip()
            
            context_parts.append(f"Ø§Ù„Ù…ØµØ¯Ø± {i} ({source}):\n{content}\n")
        
        return "\n---\n".join(context_parts)
    
    def _format_plan_info(self, plan: Dict[str, Any]) -> str:
        """ØªÙ†Ø³ÙŠÙ‚ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ø·Ø©"""
        return f"""
        Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„: {plan.get('query_type', 'Ø¹Ø§Ù…')}
        Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨Ø­Ø«: {plan.get('retrieval_strategy', 'Ø¯Ù„Ø§Ù„ÙŠ')}
        Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª: {'Ù†Ø¹Ù…' if plan.get('multi_step', False) else 'Ù„Ø§'}
        """
    
    def _validate_context_quality(self, question: str, context: str) -> bool:
        """ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø³ÙŠØ§Ù‚"""
        # ÙØ­Øµ Ø£Ø³Ø§Ø³ÙŠ Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø³ÙŠØ§Ù‚
        if len(context.strip()) < 100:
            return False
        
        # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ø³Ø¤Ø§Ù„
        question_words = set(question.lower().split())
        context_words = set(context.lower().split())
        
        # ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ ØªØ·Ø§Ø¨Ù‚ ÙÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 20% Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        overlap = len(question_words.intersection(context_words))
        return overlap >= max(1, len(question_words) * 0.2)
    
    def _post_process_answer(self, answer: str, question: str) -> str:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ
        answer = answer.strip()
        
        # Ø¥Ø¶Ø§ÙØ© Ø±Ù…ÙˆØ² ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        if "Ù„Ø§ Ø£Ø¹Ø±Ù" in answer or "ØºÙŠØ± Ù…ØªØ£ÙƒØ¯" in answer:
            answer = f"ğŸ¤” {answer}"
        elif "Ø®Ø·Ø£" in answer or "Ù…Ø´ÙƒÙ„Ø©" in answer:
            answer = f"âš ï¸ {answer}"
        else:
            answer = f"âœ… {answer}"
        
        return answer
    
    def _extract_sources(self, documents: List[Document]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        sources = []
        for doc in documents:
            file_name = doc.metadata.get('file_name', 'Ù…Ø³ØªÙ†Ø¯ ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            source_info = f"ğŸ“„ {file_name}"
            if source_info not in sources:
                sources.append(source_info)
        return sources
    
    def _calculate_confidence(self, documents: List[Document], question: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
        if not documents:
            return 0.0
        
        # Ø­Ø³Ø§Ø¨ Ø¨Ø³ÙŠØ· Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰:
        # 1. Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
        # 2. Ø·ÙˆÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        # 3. Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„
        
        doc_count_score = min(len(documents) / 5, 1.0)  # Ù…Ø«Ø§Ù„ÙŠ Ø¹Ù†Ø¯ 5 Ù…Ø³ØªÙ†Ø¯Ø§Øª
        
        total_content = sum(len(doc.page_content) for doc in documents)
        content_score = min(total_content / 2000, 1.0)  # Ù…Ø«Ø§Ù„ÙŠ Ø¹Ù†Ø¯ 2000 Ø­Ø±Ù
        
        # Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ·Ø§Ø¨Ù‚
        question_words = set(question.lower().split())
        match_score = 0
        for doc in documents:
            doc_words = set(doc.page_content.lower().split())
            overlap = len(question_words.intersection(doc_words))
            match_score += overlap / len(question_words) if question_words else 0
        
        match_score = min(match_score / len(documents), 1.0)
        
        # Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Ù‚Ø§Ø·
        confidence = (doc_count_score + content_score + match_score) / 3
        return round(confidence, 2)

# config/config.yaml - Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
app:
  name: "Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"
  version: "1.0.0"
  description: "Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø²Ø² Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"

models:
  default_llm: "gpt-3.5-turbo-instruct"
  embedding_model: "text-embedding-ada-002"
  temperature: 0.1
  max_tokens: 2000

retrieval:
  chunk_size: 1000
  chunk_overlap: 200
  similarity_threshold: 0.7
  max_documents: 10

agents:
  planning:
    enabled: true
    strategy: "adaptive"
  
  retrieval:
    strategies: ["semantic", "keyword", "hybrid"]
    default_strategy: "semantic"
  
  synthesis:
    verification_enabled: true
    confidence_threshold: 0.6

vector_store:
  type: "chroma"
  persist_directory: "./chroma_db"
  collection_name: "rag_documents"

logging:
  level: "INFO"
  file: "rag_system.log"

# tests/test_rag_pipeline.py - Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
import unittest
import tempfile
import os
from pathlib import Path
from src.rag_pipeline import RAGPipeline
from src.document_processor import DocumentProcessor

class TestRAGPipeline(unittest.TestCase):
    """Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ø¸Ø§Ù… RAG"""
    
    def setUp(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
        # ØªØ¹ÙŠÙŠÙ† Ù…ÙØªØ§Ø­ API Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± (ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…ØªØ§Ø­Ø§Ù‹)
        os.environ["OPENAI_API_KEY"] = "test-key"
        
        self.rag = RAGPipeline()
        self.test_content = "Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±. ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù‡Ù…Ø©."
    
    def test_document_processing(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù†ØµÙŠ Ù…Ø¤Ù‚Øª
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(self.test_content)
            temp_path = f.name
        
        try:
            processor = DocumentProcessor()
            docs = processor.process_document(temp_path)
            
            self.assertIsNotNone(docs)
            self.assertGreater(len(docs), 0)
            self.assertIn(self.test_content, docs[0].page_content)
            
        finally:
            os.unlink(temp_path)
    
    def test_query_without_documents(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø¯ÙˆÙ† Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        response = self.rag.query("Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ØŸ")
        
        self.assertIn("answer", response)
        self.assertIn("ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª", response["answer"])
    
    def test_supported_file_formats(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ø¹Ù… ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª"""
        processor = DocumentProcessor()
        
        self.assertIn('.txt', processor.supported_formats)
        self.assertIn('.pdf', processor.supported_formats)
        self.assertIn('.docx', processor.supported_formats)

if __name__ == '__main__':
    unittest.main()

# notebooks/demo.ipynb - Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥ Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\n",
    "\n",
    "Ù‡Ø°Ø§ Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙŠÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø°ÙƒÙŠÙŠÙ†."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\nimport sys\nsys.path.append('..')\n\nfrom src.rag_pipeline import RAGPipeline\nfrom src.document_processor import DocumentProcessor\nimport os\nfrom dotenv import load_dotenv\n\n# ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©\nload_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG\nrag_system = RAGPipeline()\nprint(\"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø¨Ù†Ø¬Ø§Ø­!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ØªØ­Ù…ÙŠÙ„ Ù…Ø³ØªÙ†Ø¯Ø§Øª\n# (Ø§Ø³ØªØ¨Ø¯Ù„ Ø¨Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ)\nfile_paths = [\n    \"../data/documents/sample_document.txt\"\n]\n\n# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\nif os.path.exists(file_paths[0]):\n    chunk_count = rag_system.load_documents(file_paths)\n    print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {chunk_count} Ù…Ù‚Ø·Ø¹ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\")\nelse:\n    print(\"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ø¹ÙŠÙ†Ø©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…\nquestions = [\n    \"Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø³ØªÙ†Ø¯ØŸ\",\n    \"Ø§Ø´Ø±Ø­ Ù„ÙŠ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ù‡Ù…Ø©\",\n    \"Ù…Ø§ Ù‡ÙŠ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŸ\"\n]\n\nfor question in questions:\n    print(f\"\\nğŸ¤” Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\")\n    response = rag_system.query(question)\n    print(f\"âœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {response['answer']}\")\n    \n    if response.get('sources'):\n        print(\"ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø±:\")\n        for source in response['sources']:\n            print(f\"  - {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…\nprint(f\"ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:\")\nprint(f\"  - Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: {rag_system.get_document_count()}\")\nprint(f\"  - Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {rag_system.get_chunk_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ©\n",
    "\n",
    "### ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ®Ø·ÙŠØ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø§Ø®ØªØ¨Ø§Ø± ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ®Ø·ÙŠØ·\nfrom src.agents.planning_agent import PlanningAgent\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\nplanning_agent = PlanningAgent(llm)\n\ntest_question = \"Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯\"\nplan = planning_agent.analyze_query(test_question)\n\nprint(f\"ğŸ“‹ Ø®Ø·Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù„Ø³Ø¤Ø§Ù„: '{test_question}'\")\nfor key, value in plan.items():\n    print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÙˆÙƒÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\nif rag_system.vector_store:\n    strategies = [\"semantic\", \"keyword\", \"hybrid\"]\n    test_query = \"Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©\"\n    \n    for strategy in strategies:\n        docs = rag_system.retrieval_agent.retrieve_documents(test_query, strategy, k=2)\n        print(f\"\\nğŸ” Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© {strategy}:\")\n        print(f\"  - Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©: {len(docs)}\")\n        if docs:\n            print(f\"  - Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ÙˆÙ„: {docs[0].page_content[:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# API Implementation (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
# src/api.py - FastAPI Ù„Ù„Ù…Ø´Ø±ÙˆØ¹
from fastapi import FastAPI, UploadFile, File, HTTPException
from typing import List
import tempfile
import os
from pydantic import BaseModel
from src.rag_pipeline import RAGPipeline

app = FastAPI(title="Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù… API", version="1.0.0")

# Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
class QueryRequest(BaseModel):
    question: str
    temperature: float = 0.1
    max_tokens: int = 2000

class QueryResponse(BaseModel):
    answer: str
    sources: List[str]
    confidence: float = 0.0

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
rag_system = RAGPipeline()

@app.post("/upload-documents/")
async def upload_documents(files: List[UploadFile] = File(...)):
    """Ø±ÙØ¹ ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
    try:
        temp_files = []
        
        for file in files:
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªØ§Ù‹
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f"_{file.filename}")
            content = await file.read()
            temp_file.write(content)
            temp_file.close()
            temp_files.append(temp_file.name)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
        chunk_count = rag_system.load_documents(temp_files)
        
        # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        for temp_file in temp_files:
            os.unlink(temp_file)
        
        return {
            "message": "ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù†Ø¬Ø§Ø­",
            "files_processed": len(files),
            "chunks_created": chunk_count
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/query/", response_model=QueryResponse)
async def query_documents(request: QueryRequest):
    """Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
    try:
        response = rag_system.query(
            request.question,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        return QueryResponse(
            answer=response["answer"],
            sources=response.get("sources", []),
            confidence=response.get("confidence", 0.0)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status/")
async def get_status():
    """Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…"""
    return {
        "system": "Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…",
        "status": "ÙŠØ¹Ù…Ù„",
        "document_count": rag_system.get_document_count(),
        "chunk_count": rag_system.get_chunk_count()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
