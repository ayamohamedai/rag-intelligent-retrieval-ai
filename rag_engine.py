import os
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
import PyPDF2
from docx import Document
import tempfile
from typing import List, Dict
import json

class RAGEngine:
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ RAG"""
        self.embeddings = None
        self.vectorstore = None
        self.qa_chain = None
        self.documents = []
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        self._setup_models()
    
    def _setup_models(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª"""
        try:
            # Ø¥Ø¹Ø¯Ø§Ø¯ Embeddings
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ LLM (ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI Ø£Ùˆ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ù„ÙŠ)
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                self.llm = OpenAI(openai_api_key=openai_api_key, temperature=0.7)
            else:
                st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ OpenAI API Key")
                
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")
    
    def extract_text_from_pdf(self, pdf_file) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† PDF"""
        try:
            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                tmp_file.write(pdf_file.read())
                tmp_file_path = tmp_file.name
            
            text = ""
            with open(tmp_file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
            
            os.unlink(tmp_file_path)
            return text
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© PDF: {e}")
            return ""
    
    def extract_text_from_docx(self, docx_file) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† DOCX"""
        try:
            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                tmp_file.write(docx_file.read())
                tmp_file_path = tmp_file.name
            
            doc = Document(tmp_file_path)
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
            
            os.unlink(tmp_file_path)
            return text
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© DOCX: {e}")
            return ""
    
    def process_documents(self, uploaded_files) -> bool:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©"""
        try:
            all_texts = []
            
            for file in uploaded_files:
                if file.type == "application/pdf":
                    text = self.extract_text_from_pdf(file)
                elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    text = self.extract_text_from_docx(file)
                elif file.type == "text/plain":
                    text = str(file.read(), "utf-8")
                else:
                    continue
                
                if text.strip():
                    all_texts.append({
                        "content": text,
                        "filename": file.name,
                        "type": file.type
                    })
            
            if not all_texts:
                return False
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", " ", ""]
            )
            
            documents = []
            for doc in all_texts:
                chunks = text_splitter.split_text(doc["content"])
                for chunk in chunks:
                    documents.append({
                        "content": chunk,
                        "metadata": {
                            "filename": doc["filename"],
                            "type": doc["type"]
                        }
                    })
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©
            texts = [doc["content"] for doc in documents]
            metadatas = [doc["metadata"] for doc in documents]
            
            self.vectorstore = FAISS.from_texts(
                texts, 
                self.embeddings,
                metadatas=metadatas
            )
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø©
            if hasattr(self, 'llm'):
                self.qa_chain = RetrievalQA.from_chain_type(
                    llm=self.llm,
                    chain_type="stuff",
                    retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3})
                )
            
            self.documents = documents
            return True
            
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: {e}")
            return False
    
    def search_documents(self, query: str, k: int = 5) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        if not self.vectorstore:
            return []
        
        try:
            results = self.vectorstore.similarity_search_with_score(query, k=k)
            
            formatted_results = []
            for doc, score in results:
                formatted_results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "similarity_score": float(score)
                })
            
            return formatted_results
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
            return []
    
    def get_answer(self, question: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù„Ù„Ø³Ø¤Ø§Ù„"""
        if not self.qa_chain:
            # Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø³ÙŠØ·Ø© Ø¨Ø¯ÙˆÙ† LLM
            results = self.search_documents(question, k=3)
            if results:
                answer = f"ðŸ” **Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ø³Ø¤Ø§Ù„:** {question}\n\n"
                for i, result in enumerate(results, 1):
                    answer += f"**Ø§Ù„Ù†ØªÙŠØ¬Ø© {i}:**\n"
                    answer += f"{result['content'][:500]}...\n"
                    answer += f"**Ø§Ù„Ù…ØµØ¯Ø±:** {result['metadata'].get('filename', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}\n\n"
                return answer
            else:
                return "âŒ Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©."
        
        try:
            response = self.qa_chain.run(question)
            return response
        except Exception as e:
            # Ø§Ø­ØªÙŠØ§Ø·ÙŠ: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¨Ø³ÙŠØ·
            results = self.search_documents(question, k=3)
            if results:
                return f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø©:\n\n{results[0]['content'][:1000]}..."
            else:
                return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {e}"
    
    def get_stats(self) -> Dict:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ"""
        return {
            "total_documents": len(self.documents),
            "total_chunks": len(self.documents),
            "has_vectorstore": self.vectorstore is not None,
            "has_qa_chain": self.qa_chain is not None
        }
