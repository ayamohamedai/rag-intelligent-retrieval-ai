"""
Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ - ØªØ·Ø¨ÙŠÙ‚ Ù…Ø­Ø³Ù†
Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:
- ÙˆØ§Ø¬Ù‡Ø© Ø¹Ø±Ø¨ÙŠØ© ÙƒØ§Ù…Ù„Ø©
- Ø¯Ø¹Ù… OCR Ù„Ù„ØµÙˆØ±
- Ø¨Ø­Ø« ØµÙˆØªÙŠ
- ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
- Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ©
- ØªØµØ¯ÙŠØ± Ù…Ø­Ø³Ù†
- ØªØ­Ù„ÙŠÙ„ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
- Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø°ÙƒÙŠØ©
- ØªØªØ¨Ø¹ Ø§Ù„ØªÙ‚Ø¯Ù…
- ØªØ­Ù„ÙŠÙ„Ø§Øª Ø¨ØµØ±ÙŠØ©
- Ø­ÙØ¸ Ø§Ù„Ø¬Ù„Ø³Ø§Øª
- ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
"""
import os
import io
import time
import math
import tempfile
import traceback
import json
import base64
import hashlib
import pickle
import re
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime, timedelta
from collections import defaultdict, Counter

import streamlit as st
import pandas as pd
import numpy as np

# Ø§Ù„ÙˆØ§Ø±Ø¯Ø§Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
try:
    from googletrans import Translator
    TRANSLATOR_AVAILABLE = True
except Exception:
    TRANSLATOR_AVAILABLE = False

try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
except Exception:
    SPEECH_RECOGNITION_AVAILABLE = False

try:
    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except Exception:
    PLOTLY_AVAILABLE = False

try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except Exception:
    OCR_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except Exception:
    OPENCV_AVAILABLE = False

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except Exception:
    NETWORKX_AVAILABLE = False

try:
    from wordcloud import WordCloud
    import arabic_reshaper
    from bidi.algorithm import get_display
    WORDCLOUD_AVAILABLE = True
except Exception:
    WORDCLOUD_AVAILABLE = False

try:
    import textstat
    TEXTSTAT_AVAILABLE = True
except Exception:
    TEXTSTAT_AVAILABLE = False

# Ø§Ù„ÙˆØ§Ø±Ø¯Ø§Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©
try:
    from modules.file_handler import load_file
    from modules.utils import clean_text, chunk_text, sentences_from_text, tfidf_sentence_ranking
    from modules.ai_engine import RAGEngine, get_embedding as module_get_embedding, chat_with_ai as module_chat_with_ai
    from modules.exporter import export_txt, export_docx, export_pdf
    from modules.ui_components import render_upload_box, render_chat_ui
    MODULES_AVAILABLE = True
except Exception:
    MODULES_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

try:
    import faiss
except Exception:
    faiss = None

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

try:
    from PyPDF2 import PdfReader
except Exception:
    PdfReader = None

try:
    import docx
except Exception:
    docx = None

try:
    import openpyxl
except Exception:
    openpyxl = None

try:
    from pptx import Presentation
    from pptx.util import Inches, Pt
except Exception:
    Presentation = None

try:
    from docx import Document as DocxDoc
except Exception:
    DocxDoc = None

try:
    import whisper
    WHISPER_AVAILABLE = True
except Exception:
    WHISPER_AVAILABLE = False

# ---------------------- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„Ø£Ø³Ø±Ø§Ø± ----------------------
st.set_page_config(
    page_title="Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚", 
    page_icon="ğŸŒ", 
    layout="wide", 
    initial_sidebar_state="expanded"
)

# CSS Ù…Ø®ØµØµ Ù„Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
st.markdown("""
<style>
    .rtl-text {
        direction: rtl;
        text-align: right;
        font-family: 'Arial', sans-serif;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .success-card {
        background-color: #d4edda;
        border-color: #28a745;
    }
    .warning-card {
        background-color: #fff3cd;
        border-color: #ffc107;
    }
    .error-card {
        background-color: #f8d7da;
        border-color: #dc3545;
    }
    .sidebar .sidebar-content {
        background-image: linear-gradient(#2e7bcf,#2e7bcf);
    }
    .stProgress .st-bo {
        background-color: #1f77b4;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .user-message {
        background-color: #e3f2fd;
        margin-left: 2rem;
    }
    .assistant-message {
        background-color: #f5f5f5;
        margin-right: 2rem;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸŒ Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ â€” Ù…Ù†ØµØ© RAG Ø§Ù„Ù…ØªØ·ÙˆØ±Ø©")

# Ù‚Ø±Ø§Ø¡Ø© Ù…ÙØªØ§Ø­ OpenAI
OPENAI_API_KEY = None
try:
    OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")
except Exception:
    pass
if not OPENAI_API_KEY:
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙŠÙ„ OpenAI
openai_client = None
if OPENAI_API_KEY and OpenAI is not None:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception:
        openai_client = None

# Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ±Ø¬Ù…
translator = None
if TRANSLATOR_AVAILABLE:
    try:
        translator = Translator()
    except Exception:
        translator = None

# ---------------------- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ----------------------
def initialize_session_state():
    """ØªÙ‡ÙŠØ¦Ø© Ø­Ø§Ù„Ø© Ø§Ù„Ø¬Ù„Ø³Ø© Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©"""
    defaults = {
        "docs": [],
        "index_built": False,
        "faiss_index": None,
        "embeddings": None,
        "id_map": [],
        "model_name": "all-MiniLM-L6-v2",
        "chat_history": [],
        "query_count": 0,
        "processing_stats": {
            "total_docs": 0,
            "total_chars": 0,
            "total_words": 0,
            "avg_query_time": 0,
            "languages_detected": set(),
            "file_types": {},
            "sessions_count": 0,
            "total_queries": 0,
            "successful_queries": 0
        },
        "user_preferences": {
            "language": "ar",
            "theme": "light",
            "chunk_size": 300,
            "chunk_overlap": 50,
            "top_k_results": 5,
            "temperature": 0.7,
            "max_tokens": 1000
        },
        "current_session": {
            "start_time": datetime.now(),
            "queries": [],
            "docs_processed": 0
        },
        "advanced_analytics": {
            "query_patterns": defaultdict(int),
            "response_ratings": [],
            "most_accessed_docs": defaultdict(int),
            "search_history": [],
            "user_feedback": []
        },
        "cache": {},
        "bookmarks": [],
        "export_history": []
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

initialize_session_state()

# ---------------------- Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ----------------------
def app_log(msg: str):
    """ØªØ³Ø¬ÙŠÙ„ Ù…Ø­Ø³Ù† Ù…Ø¹ Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ ÙˆØ§Ù„Ù…Ø³ØªÙˆÙ‰"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    log_entry = {
        "timestamp": timestamp,
        "message": msg,
        "level": "INFO"
    }
    st.session_state.setdefault("_logs", []).append(log_entry)
    if len(st.session_state["_logs"]) > 100:  # Ø§Ù„Ø­Ø¯ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„Ø³Ø¬Ù„
        st.session_state["_logs"] = st.session_state["_logs"][-100:]

def generate_doc_hash(content: str) -> str:
    """ØªÙˆÙ„ÙŠØ¯ hash Ù„Ù„ÙˆØ«ÙŠÙ‚Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±"""
    return hashlib.md5(content.encode()).hexdigest()

def detect_language(text: str) -> str:
    """ÙƒØ´Ù Ù„ØºØ© Ø§Ù„Ù†Øµ Ù…Ø­Ø³Ù†"""
    if not translator:
        # ÙƒØ´Ù Ø¨Ø³ÙŠØ· Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        arabic_chars = sum(1 for c in text[:200] if '\u0600' <= c <= '\u06FF')
        if arabic_chars > 10:
            return "ar"
        return "en"
    
    try:
        result = translator.detect(text[:200])
        st.session_state.processing_stats["languages_detected"].add(result.lang)
        return result.lang
    except Exception:
        return "unknown"

def translate_text(text: str, target_lang: str = "ar") -> str:
    """ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª"""
    if not translator:
        return text
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
    cache_key = f"translate_{hash(text)}_{target_lang}"
    if cache_key in st.session_state.cache:
        return st.session_state.cache[cache_key]
    
    try:
        result = translator.translate(text, dest=target_lang)
        translated = result.text
        st.session_state.cache[cache_key] = translated
        return translated
    except Exception as e:
        app_log(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø©: {e}")
        return text

def extract_text_from_image(image) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OCR Ù…Ø­Ø³Ù†"""
    if not OCR_AVAILABLE:
        return "[OCR ØºÙŠØ± Ù…ØªØ§Ø­: ØªØ«Ø¨ÙŠØª pytesseract Ù…Ø·Ù„ÙˆØ¨]"
    
    try:
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù€ PIL
        if isinstance(image, bytes):
            img = Image.open(io.BytesIO(image))
        else:
            img = Image.open(image)
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù€ OCR
        if OPENCV_AVAILABLE:
            img_array = np.array(img)
            if len(img_array.shape) == 3:
                img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¨Ø§ÙŠÙ†
            img_array = cv2.equalizeHist(img_array)
            img = Image.fromarray(img_array)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ø¹ Ø¯Ø¹Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª
        text = pytesseract.image_to_string(img, lang='ara+eng+fra')
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    except Exception as e:
        app_log(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©: {e}")
        return f"[Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©: {e}]"

def process_audio_input(audio_file) -> str:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ"""
    if not WHISPER_AVAILABLE and not SPEECH_RECOGNITION_AVAILABLE:
        return "[Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª ØºÙŠØ± Ù…ØªØ§Ø­Ø©]"
    
    try:
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ Ù…Ø¤Ù‚ØªØ§Ù‹
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_file.write(audio_file.getvalue())
            tmp_path = tmp_file.name
        
        if WHISPER_AVAILABLE:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Whisper Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª
            model = whisper.load_model("base")
            result = model.transcribe(tmp_path, language="ar")
            text = result["text"]
        elif SPEECH_RECOGNITION_AVAILABLE:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… speech_recognition
            r = sr.Recognizer()
            with sr.AudioFile(tmp_path) as source:
                audio = r.record(source)
                text = r.recognize_google(audio, language='ar-SA')
        else:
            text = "[ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª]"
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
        os.unlink(tmp_path)
        
        return text
    except Exception as e:
        app_log(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª: {e}")
        return f"[Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª: {e}]"

def analyze_text_statistics(text: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
    stats = {
        "char_count": len(text),
        "word_count": len(text.split()),
        "sentence_count": len(fallback_sentences_from_text(text)),
        "paragraph_count": len([p for p in text.split('\n\n') if p.strip()]),
        "language": detect_language(text)
    }
    
    if TEXTSTAT_AVAILABLE:
        try:
            stats.update({
                "readability_score": textstat.flesch_reading_ease(text),
                "grade_level": textstat.flesch_kincaid_grade(text),
                "avg_sentence_length": textstat.avg_sentence_length(text),
                "difficult_words": textstat.difficult_words(text)
            })
        except Exception:
            pass
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹
    words = re.findall(r'\b\w+\b', text.lower())
    word_freq = Counter(words)
    stats["most_common_words"] = word_freq.most_common(10)
    
    return stats

def fallback_load_file_bytes(fileobj) -> Tuple[str, Dict[str, Any]]:
    """Ù‚Ø§Ø±Ø¦ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"""
    name = getattr(fileobj, "name", "Ù…Ø±ÙÙˆØ¹")
    ext = os.path.splitext(name)[1].lower()
    
    try:
        raw = fileobj.read()
    except Exception as e:
        return f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}]", {"error": str(e)}
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù„Ù
    file_size = len(raw)
    file_info = {
        "name": name,
        "extension": ext,
        "size_bytes": file_size,
        "size_mb": file_size / (1024 * 1024),
        "processed_at": datetime.now().isoformat()
    }
    
    st.session_state.processing_stats["file_types"][ext] = st.session_state.processing_stats["file_types"].get(ext, 0) + 1
    
    content = ""
    
    if ext == ".pdf":
        if PdfReader is None:
            content = "[Ù‚Ø±Ø§Ø¡Ø© PDF ØºÙŠØ± Ù…ØªØ§Ø­Ø©: ØªØ«Ø¨ÙŠØª PyPDF2 Ù…Ø·Ù„ÙˆØ¨]"
        else:
            try:
                reader = PdfReader(io.BytesIO(raw))
                pages = []
                file_info["pages"] = len(reader.pages)
                
                for i, page in enumerate(reader.pages):
                    txt = page.extract_text() or ""
                    if txt.strip():
                        pages.append(f"--- ØµÙØ­Ø© {i+1} ---\n{txt}")
                
                content = "\n\n".join(pages)
                file_info["extracted_pages"] = len(pages)
            except Exception as e:
                content = f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© PDF: {e}]"
                file_info["error"] = str(e)
    
    elif ext == ".docx":
        if docx is None:
            content = "[Ù‚Ø±Ø§Ø¡Ø© DOCX ØºÙŠØ± Ù…ØªØ§Ø­Ø©: ØªØ«Ø¨ÙŠØª python-docx Ù…Ø·Ù„ÙˆØ¨]"
        else:
            try:
                tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
                tmp.write(raw)
                tmp.flush()
                tmp.close()
                
                d = docx.Document(tmp.name)
                paragraphs = [p.text for p in d.paragraphs if p.text.strip()]
                content = "\n".join(paragraphs)
                
                file_info["paragraphs"] = len(paragraphs)
                
                try:
                    os.unlink(tmp.name)
                except Exception:
                    pass
                    
            except Exception as e:
                content = f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© DOCX: {e}]"
                file_info["error"] = str(e)
    
    elif ext in [".txt", ".md"]:
        try:
            content = raw.decode("utf-8", errors="ignore")
        except Exception as e:
            content = f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†Øµ: {e}]"
    
    elif ext in [".xls", ".xlsx", ".csv"]:
        try:
            if ext == ".csv":
                df = pd.read_csv(io.BytesIO(raw))
            else:
                if openpyxl is None:
                    content = "[Ù‚Ø±Ø§Ø¡Ø© Excel ØºÙŠØ± Ù…ØªØ§Ø­Ø©: ØªØ«Ø¨ÙŠØª openpyxl Ù…Ø·Ù„ÙˆØ¨]"
                else:
                    df = pd.read_excel(io.BytesIO(raw))
            
            if isinstance(df, pd.DataFrame):
                content = df.fillna("").to_string()
                file_info["rows"] = len(df)
                file_info["columns"] = len(df.columns)
                
        except Exception as e:
            content = f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„: {e}]"
            file_info["error"] = str(e)
    
    elif ext in [".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".gif"]:
        content = extract_text_from_image(raw)
        file_info["type"] = "image"
    
    elif ext == ".pptx":
        if Presentation is None:
            content = "[Ù‚Ø±Ø§Ø¡Ø© PowerPoint ØºÙŠØ± Ù…ØªØ§Ø­Ø©: ØªØ«Ø¨ÙŠØª python-pptx Ù…Ø·Ù„ÙˆØ¨]"
        else:
            try:
                tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pptx")
                tmp.write(raw)
                tmp.flush()
                tmp.close()
                
                prs = Presentation(tmp.name)
                slides_text = []
                
                for i, slide in enumerate(prs.slides):
                    slide_text = f"--- Ø´Ø±ÙŠØ­Ø© {i+1} ---\n"
                    for shape in slide.shapes:
                        if hasattr(shape, "text") and shape.text.strip():
                            slide_text += shape.text + "\n"
                    if slide_text.strip() != f"--- Ø´Ø±ÙŠØ­Ø© {i+1} ---":
                        slides_text.append(slide_text)
                
                content = "\n\n".join(slides_text)
                file_info["slides"] = len(prs.slides)
                file_info["extracted_slides"] = len(slides_text)
                
                try:
                    os.unlink(tmp.name)
                except Exception:
                    pass
                    
            except Exception as e:
                content = f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© PowerPoint: {e}]"
                file_info["error"] = str(e)
    
    else:
        try:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ†Øµ
            content = raw.decode("utf-8", errors="ignore")
        except Exception:
            content = "[Ù†ÙˆØ¹ Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… Ø£Ùˆ Ù…Ø­ØªÙˆÙ‰ Ø«Ù†Ø§Ø¦ÙŠ]"
    
    return content, file_info

def fallback_clean_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ù…Ø­Ø³Ù† Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    if not text:
        return ""
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø²Ø§Ø¦Ø¯ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    text = re.sub(r'[\u064B-\u0652]', '', text)
    
    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„ÙÙˆØ§ØµÙ„
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"[\r\n]+", "\n", text)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ© Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ù…Ø¹ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ù…Ù‡Ù…Ø©
    text = re.sub(r'[^\w\s\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFE70-\uFEFF\u0750-\u077FØŒØ›ØŸ!.()ØŒ]', ' ', text)
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def fallback_chunk_text(text: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø³ÙŠØ§Ù‚"""
    if not text:
        return []
    
    # ØªÙ‚Ø³ÙŠÙ… Ø¨Ø§Ù„Ø¬Ù…Ù„ Ø£ÙˆÙ„Ø§Ù‹
    sentences = fallback_sentences_from_text(text)
    
    if len(sentences) <= 3:
        return [text]
    
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence_words = len(sentence.split())
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¬Ù…Ù„Ø© Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ù‚Ø³Ù…Ù‡Ø§
        if sentence_words > chunk_size:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_length = 0
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
            words = sentence.split()
            for i in range(0, len(words), chunk_size - overlap):
                chunk_words = words[i:i + chunk_size]
                chunks.append(" ".join(chunk_words))
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ù…Ù„Ø© Ø³ØªØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
        elif current_length + sentence_words > chunk_size and current_chunk:
            chunks.append(" ".join(current_chunk))
            # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¨Ø¹Ø¶ Ø§Ù„Ø¬Ù…Ù„ Ù„Ù„ØªØ¯Ø§Ø®Ù„
            overlap_sentences = current_chunk[-min(2, len(current_chunk)):]
            current_chunk = overlap_sentences + [sentence]
            current_length = sum(len(s.split()) for s in current_chunk)
        else:
            current_chunk.append(sentence)
            current_length += sentence_words
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 20]

from typing import List, Dict, Any
import streamlit as st
import re
from collections import Counter
import math

def fallback_sentences_from_text(text: str) -> List[str]:
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ù„Ø¬Ù…Ù„ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    if not text:
        return []
    
    try:
        import nltk
        nltk.download('punkt', quiet=True)
        from nltk.tokenize import sent_tokenize
        sentences = sent_tokenize(text)
        return [s.strip() for s in sentences if len(s.strip()) > 15]
    except Exception:
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù…Ø­Ø³Ù† Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        # Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        text = text.replace('ØŸ', 'ØŸ\n')
        text = text.replace('!', '!\n') 
        text = text.replace('.', '.\n')
        text = text.replace('Ø›', 'Ø›\n')
        text = text.replace(':', ':\n')  # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ù‚Ø·ØªÙŠÙ†
        text = text.replace('ØŒ', 'ØŒ\n')  # Ø§Ù„ÙØ§ØµÙ„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
        
        sentences = [s.strip() for s in text.split('\n') if s.strip()]
        return [s for s in sentences if len(s.strip()) > 15]

def clean_arabic_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    if not text:
        return ""
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’]', '', text)
    
    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
    text = text.replace('Ø£', 'Ø§').replace('Ø¥', 'Ø§').replace('Ø¢', 'Ø§')
    text = text.replace('Ø©', 'Ù‡')
    text = text.replace('Ù‰', 'ÙŠ')
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø±Ù…ÙˆØ² ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
    text = re.sub(r'[0-9]+', '', text)
    text = re.sub(r'[^\u0600-\u06FF\s]', ' ', text)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    text = ' '.join(text.split())
    
    return text.strip()

def fallback_tfidf_sentence_ranking(document_texts: List[str], top_k_sentences_per_doc: int = 3) -> Dict[str, List[Dict]]:
    """ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    if not document_texts:
        return {}
    
    try:
        # Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ø¬Ù…Ù„ Ù…Ù† ÙƒÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
        all_sentences = []
        doc_sentence_map = {}
        
        for doc_idx, doc_text in enumerate(document_texts):
            sentences = fallback_sentences_from_text(doc_text)
            doc_id = f"doc_{doc_idx + 1}"
            doc_sentence_map[doc_id] = sentences
            all_sentences.extend(sentences)
        
        if not all_sentences:
            return {}
        
        # Ø­Ø³Ø§Ø¨ TF-IDF Ø¨Ø³ÙŠØ·
        # 1. Ø­Ø³Ø§Ø¨ ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        word_doc_count = Counter()
        sentence_word_counts = []
        
        for sentence in all_sentences:
            clean_sentence = clean_arabic_text(sentence)
            words = clean_sentence.split()
            word_count = Counter(words)
            sentence_word_counts.append(word_count)
            
            # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„ ÙƒÙ„Ù…Ø©
            for word in set(words):
                if len(word) > 2:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
                    word_doc_count[word] += 1
        
        # 2. Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· TF-IDF Ù„Ù„Ø¬Ù…Ù„
        sentence_scores = []
        total_docs = len(all_sentences)
        
        for word_count in sentence_word_counts:
            score = 0
            total_words = sum(word_count.values())
            
            if total_words > 0:
                for word, count in word_count.items():
                    if len(word) > 2 and word_doc_count[word] > 0:
                        tf = count / total_words
                        idf = math.log(total_docs / word_doc_count[word])
                        score += tf * idf
            
            sentence_scores.append(score)
        
        # 3. ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„ Ù„ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©
        results = {}
        sentence_idx = 0
        
        for doc_id, sentences in doc_sentence_map.items():
            doc_sentences_with_scores = []
            
            for sentence in sentences:
                if sentence_idx < len(sentence_scores):
                    doc_sentences_with_scores.append({
                        'text': sentence,
                        'score': sentence_scores[sentence_idx],
                        'length': len(sentence)
                    })
                sentence_idx += 1
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·
            doc_sentences_with_scores.sort(key=lambda x: x['score'], reverse=True)
            
            # Ø£Ø®Ø° Ø£ÙØ¶Ù„ Ø§Ù„Ø¬Ù…Ù„
            results[doc_id] = doc_sentences_with_scores[:top_k_sentences_per_doc]
        
        return results
    
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„: {e}")
        return {}

def process_documents(documents_list=None):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØªÙ†Ø¸ÙŠÙÙ‡Ø§"""
    try:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…ÙÙ…Ø±Ø± Ø£Ùˆ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† document_texts
        if documents_list is None:
            if 'document_texts' in st.session_state:
                documents_list = st.session_state.document_texts
            elif 'document_texts' in globals():
                documents_list = document_texts
            else:
                st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ ÙˆØ«Ø§Ø¦Ù‚ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
                return []
        
        if not documents_list:
            st.warning("Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙØ§Ø±ØºØ©.")
            return []
            
        processed_docs = []
        progress_bar = st.progress(0)
        
        for i, doc in enumerate(documents_list):
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
            clean_doc = doc.strip().replace("\n", " ").replace("\r", "")
            clean_doc = clean_arabic_text(clean_doc)  # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
            doc_id = f"doc_{i+1}"
            
            # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ø¬Ù…Ù„
            sentences = fallback_sentences_from_text(doc)
            
            processed_docs.append({
                "id": doc_id,
                "text": clean_doc,
                "original_text": doc,
                "sentences": sentences,
                "length": len(clean_doc),
                "sentence_count": len(sentences)
            })
            
            # ØªØ­Ø¯ÙŠØ« Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
            progress_bar.progress((i + 1) / len(documents_list))
            st.write(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© {doc_id} - Ø§Ù„Ø·ÙˆÙ„: {len(clean_doc)} Ø­Ø±Ù - Ø§Ù„Ø¬Ù…Ù„: {len(sentences)}")
            
        st.success(f"ğŸ‰ ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {len(processed_docs)} ÙˆØ«ÙŠÙ‚Ø© Ø¨Ù†Ø¬Ø§Ø­!")
        
        # Ø­ÙØ¸ ÙÙŠ session state
        st.session_state.processed_docs = processed_docs
        
        return processed_docs
        
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚: {e}")
        return []

def extract_key_sentences(processed_docs: List[Dict], top_k: int = 3) -> Dict:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ù‡Ù… Ø§Ù„Ø¬Ù…Ù„ Ù…Ù† ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©"""
    if not processed_docs:
        return {}
    
    # ØªØ­Ø¶ÙŠØ± Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„ØªØ­Ù„ÙŠÙ„
    document_texts = [doc['original_text'] for doc in processed_docs]
    
    # ØªØ­Ù„ÙŠÙ„ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„
    ranked_sentences = fallback_tfidf_sentence_ranking(document_texts, top_k)
    
    return ranked_sentences

def create_searchable_index(processed_docs: List[Dict]) -> Dict:
    """Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù„Ù„Ø¨Ø­Ø«"""
    search_index = {
        'documents': {},
        'words': {},
        'sentences': []
    }
    
    for doc in processed_docs:
        doc_id = doc['id']
        clean_text = doc['text']
        sentences = doc['sentences']
        
        # ÙÙ‡Ø±Ø³Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©
        search_index['documents'][doc_id] = {
            'text': clean_text,
            'sentences': sentences,
            'word_count': len(clean_text.split())
        }
        
        # ÙÙ‡Ø±Ø³Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        words = clean_text.split()
        for word in words:
            if len(word) > 2:
                if word not in search_index['words']:
                    search_index['words'][word] = []
                search_index['words'][word].append(doc_id)
        
        # ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ø¬Ù…Ù„
        for sentence in sentences:
            clean_sentence = clean_arabic_text(sentence)
            search_index['sentences'].append({
                'text': sentence,
                'clean_text': clean_sentence,
                'doc_id': doc_id,
                'words': clean_sentence.split()
            })
    
    return search_index

# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
def main():
    st.title("ğŸŒ Ù…Ù†ØµØ© RAG Ø§Ù„Ù…ØªØ·ÙˆØ±Ø© Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
    if 'document_texts' not in st.session_state:
        st.session_state.document_texts = [
            "Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø© Ø¬Ù…Ù„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±.",
            "Ø§Ù„Ù†Øµ Ø§Ù„Ø«Ø§Ù†ÙŠ ÙŠØªØ¶Ù…Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø®ØªÙ„ÙØ©. Ù‡Ø¯ÙÙ‡ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù….",
            "Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù…ØªÙ†ÙˆØ¹ ÙˆÙ…ÙÙŠØ¯ Ù„Ù„ØºØ§ÙŠØ©."
        ]
    
    if st.button("Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"):
        processed_docs = process_documents()
        
        if processed_docs:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©
            key_sentences = extract_key_sentences(processed_docs)
            
            st.subheader("ğŸ” Ø£Ù‡Ù… Ø§Ù„Ø¬Ù…Ù„ Ù…Ù† ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©:")
            for doc_id, sentences in key_sentences.items():
                st.write(f"**{doc_id}:**")
                for i, sent_info in enumerate(sentences, 1):
                    st.write(f"  {i}. {sent_info['text']} (Ù†Ù‚Ø§Ø·: {sent_info['score']:.3f})")
                st.write("")
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«
            search_index = create_searchable_index(processed_docs)
            st.session_state.search_index = search_index
            
            st.success("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø« Ø¨Ù†Ø¬Ø§Ø­!")

if __name__ == "__main__":
    main()
