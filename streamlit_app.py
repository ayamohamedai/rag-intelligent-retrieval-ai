"""
ูุธุงู RAG ุงุญุชุฑุงูู ูุชูุฏู ูููุซุงุฆู ุงูุนุฑุจูุฉ
ูุณุชุฎุฏู ุฃุญุฏุซ ุงูุชูููุงุช ูุงูููุชุจุงุช ุงููุชูุฏูุฉ
"""

import streamlit as st
import pandas as pd
import numpy as np
import json
import asyncio
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
import requests
import os
import time
import logging
from pathlib import Path
import tempfile
import io
import base64
from concurrent.futures import ThreadPoolExecutor
import sqlite3
from contextlib import asynccontextmanager

# ููุชุจุงุช ูุชูุฏูุฉ ูููุนุงูุฌุฉ
try:
    from sentence_transformers import SentenceTransformer
    import chromadb
    from chromadb.config import Settings
    import faiss
    import nltk
    from nltk.tokenize import sent_tokenize, word_tokenize
    from nltk.corpus import stopwords
    import PyPDF2
    import docx2txt
    import magic
    HAS_ADVANCED_LIBS = True
except ImportError:
    HAS_ADVANCED_LIBS = False

# ุฅุนุฏุงุฏ Streamlit
st.set_page_config(
    page_title="ูุธุงู RAG ุงููุชูุฏู",
    page_icon="๐",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ุฅุนุฏุงุฏ ุงูุชุณุฌูู
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# CSS ูุชูุฏู
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@200;300;400;600;700&display=swap');
    
    * {
        font-family: 'Cairo', sans-serif;
    }
    
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 20px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 20px 60px rgba(102, 126, 234, 0.4);
        position: relative;
        overflow: hidden;
    }
    
    .main-header::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: linear-gradient(45deg, transparent 40%, rgba(255,255,255,0.1) 50%, transparent 60%);
        animation: shimmer 3s infinite;
    }
    
    @keyframes shimmer {
        0% { transform: translateX(-100%); }
        100% { transform: translateX(100%); }
    }
    
    .metric-card {
        background: linear-gradient(145deg, #ffffff 0%, #f8f9fa 100%);
        padding: 2rem;
        border-radius: 15px;
        box-shadow: 
            0 10px 30px rgba(0,0,0,0.1),
            0 1px 8px rgba(0,0,0,0.2);
        text-align: center;
        border: 1px solid rgba(255,255,255,0.2);
        transition: transform 0.3s ease, box-shadow 0.3s ease;
        position: relative;
        overflow: hidden;
    }
    
    .metric-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 20px 40px rgba(0,0,0,0.15);
    }
    
    .metric-card::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 4px;
        background: linear-gradient(90deg, #667eea, #764ba2);
    }
    
    .chat-container {
        max-height: 600px;
        overflow-y: auto;
        padding: 1rem;
        background: #fafafa;
        border-radius: 15px;
        border: 1px solid #e0e0e0;
    }
    
    .message-user {
        background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
        margin: 1rem 0;
        padding: 1.5rem;
        border-radius: 20px 20px 5px 20px;
        border-left: 4px solid #2196f3;
        box-shadow: 0 4px 12px rgba(33, 150, 243, 0.2);
        position: relative;
    }
    
    .message-ai {
        background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
        margin: 1rem 0;
        padding: 1.5rem;
        border-radius: 20px 20px 20px 5px;
        border-left: 4px solid #9c27b0;
        box-shadow: 0 4px 12px rgba(156, 39, 176, 0.2);
        position: relative;
    }
    
    .source-card {
        background: linear-gradient(135deg, #fff3e0 0%, #ffcc02 20%, #fff3e0 100%);
        padding: 1rem;
        border-radius: 12px;
        margin: 0.5rem 0;
        border-left: 4px solid #ff9800;
        box-shadow: 0 2px 8px rgba(255, 152, 0, 0.2);
        transition: transform 0.2s ease;
    }
    
    .source-card:hover {
        transform: scale(1.02);
    }
    
    .status-indicator {
        display: inline-block;
        width: 12px;
        height: 12px;
        border-radius: 50%;
        margin-right: 8px;
        animation: pulse 2s infinite;
    }
    
    .status-online { background: #4caf50; }
    .status-offline { background: #f44336; }
    .status-processing { background: #ff9800; }
    
    @keyframes pulse {
        0% { opacity: 1; }
        50% { opacity: 0.5; }
        100% { opacity: 1; }
    }
    
    .progress-bar {
        width: 100%;
        height: 8px;
        background: #e0e0e0;
        border-radius: 4px;
        overflow: hidden;
        position: relative;
    }
    
    .progress-fill {
        height: 100%;
        background: linear-gradient(90deg, #667eea, #764ba2);
        border-radius: 4px;
        transition: width 0.3s ease;
        position: relative;
        overflow: hidden;
    }
    
    .progress-fill::after {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        bottom: 0;
        right: 0;
        background-image: linear-gradient(
            -45deg,
            rgba(255, 255, 255, .2) 25%,
            transparent 25%,
            transparent 50%,
            rgba(255, 255, 255, .2) 50%,
            rgba(255, 255, 255, .2) 75%,
            transparent 75%,
            transparent
        );
        background-size: 50px 50px;
        animation: move 2s linear infinite;
    }
    
    @keyframes move {
        0% { background-position: 0 0; }
        100% { background-position: 50px 50px; }
    }
    
    .rtl {
        direction: rtl;
        text-align: right;
    }
    
    .sidebar .stSelectbox label {
        font-weight: 600;
        color: #333;
    }
    
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border: none;
        border-radius: 10px;
        color: white;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(102, 126, 234, 0.4);
    }
    
    .upload-zone {
        border: 2px dashed #667eea;
        border-radius: 15px;
        padding: 2rem;
        text-align: center;
        background: linear-gradient(135deg, rgba(102, 126, 234, 0.05) 0%, rgba(118, 75, 162, 0.05) 100%);
        transition: all 0.3s ease;
    }
    
    .upload-zone:hover {
        border-color: #764ba2;
        background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%);
    }
</style>
""", unsafe_allow_html=True)

# ======================== ููุงุณุงุช ูุชูุฏูุฉ ========================

class AdvancedEmbedding:
    """ูุธุงู ุชุดููุฑ ูุชูุฏู ุจุงุณุชุฎุฏุงู Sentence Transformers"""
    
    def __init__(self):
        self.model = None
        self.model_name = "paraphrase-multilingual-MiniLM-L12-v2"
        self.dimension = 384
        self.is_loaded = False
    
    @st.cache_resource
    def load_model(self):
        """ุชุญููู ุงููููุฐุฌ ูุน ุงูุชุฎุฒูู ุงููุคูุช"""
        try:
            if HAS_ADVANCED_LIBS:
                self.model = SentenceTransformer(self.model_name)
                self.is_loaded = True
                return True
            else:
                st.error("ุงูููุชุจุงุช ุงููุชูุฏูุฉ ุบูุฑ ูุซุจุชุฉ. ูุฑุฌู ุชุซุจูุช sentence-transformers")
                return False
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ุชุญููู ุงููููุฐุฌ: {e}")
            return False
    
    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """ุชุดููุฑ ุงููุตูุต ุฅูู ูููุชูุฑุงุช"""
        if not self.is_loaded:
            if not self.load_model():
                return np.array([])
        
        try:
            # ุชูุธูู ุงููุตูุต
            clean_texts = [self._clean_text(text) for text in texts]
            
            # ุชุดููุฑ ุจุฏูุนุงุช
            embeddings = self.model.encode(
                clean_texts,
                batch_size=batch_size,
                show_progress_bar=True,
                convert_to_numpy=True
            )
            
            return embeddings
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ุงูุชุดููุฑ: {e}")
            return np.array([])
    
    def _clean_text(self, text: str) -> str:
        """ุชูุธูู ุงููุต"""
        if not text:
            return ""
        
        # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
        text = ' '.join(text.split())
        
        # ุชุญุฏูุฏ ุงูุทูู ุงูุฃูุตู
        max_length = 512
        if len(text) > max_length:
            text = text[:max_length]
        
        return text

class ChromaVectorStore:
    """ูุฎุฒู ูููุชูุฑุงุช ูุชูุฏู ุจุงุณุชุฎุฏุงู ChromaDB"""
    
    def __init__(self, collection_name: str = "documents"):
        self.collection_name = collection_name
        self.client = None
        self.collection = None
        self.embedding_model = AdvancedEmbedding()
    
    def initialize(self) -> bool:
        """ุชููุฆุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช"""
        try:
            if HAS_ADVANCED_LIBS:
                # ุฅูุดุงุก ูุฌูุฏ ูุคูุช ููุงุนุฏุฉ ุงูุจูุงูุงุช
                db_path = tempfile.mkdtemp()
                
                self.client = chromadb.PersistentClient(path=db_path)
                
                # ุฅูุดุงุก ุฃู ุงูุญุตูู ุนูู ุงููุฌููุนุฉ
                try:
                    self.collection = self.client.get_collection(name=self.collection_name)
                except:
                    self.collection = self.client.create_collection(
                        name=self.collection_name,
                        metadata={"hnsw:space": "cosine"}
                    )
                
                return True
            else:
                return False
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ุชููุฆุฉ ChromaDB: {e}")
            return False
    
    def add_documents(self, chunks: List[Dict[str, Any]]) -> bool:
        """ุฅุถุงูุฉ ุงููุซุงุฆู"""
        if not self.collection:
            if not self.initialize():
                return False
        
        try:
            texts = [chunk['text'] for chunk in chunks]
            ids = [str(chunk['id']) for chunk in chunks]
            metadatas = [
                {
                    'doc_name': chunk.get('doc_name', ''),
                    'chunk_id': chunk.get('chunk_id', ''),
                    'word_count': chunk.get('word_count', 0),
                    'timestamp': datetime.now().isoformat(),
            'word_count': len(clean_text.split()),
            'char_count': len(clean_text),
            'processed': False
        }
        
        st.session_state.documents.append(doc_data)
        st.session_state.processing_stats['documents_processed'] += 1
        
        st.success("โ ุชู ุฅุถุงูุฉ ุงููุต ุงููุจุงุดุฑ ุจูุฌุงุญ!")
        st.rerun()
        
    except Exception as e:
        st.error(f"โ ุฎุทุฃ ูู ุฅุถุงูุฉ ุงููุต: {str(e)}")

def create_search_index():
    """ุฅูุดุงุก ููุฑุณ ุงูุจุญุซ ุงููุชูุฏู"""
    if not st.session_state.documents:
        st.warning("ูุง ุชูุฌุฏ ูุซุงุฆู ูููุนุงูุฌุฉ")
        return
    
    with st.spinner("๐ ุฅูุดุงุก ููุฑุณ ุงูุจุญุซ ุงููุชูุฏู..."):
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # ุชููุฆุฉ ูุฎุฒู ุงููููุชูุฑุงุช
            if not st.session_state.vector_store.initialize():
                st.error("โ ูุดู ูู ุชููุฆุฉ ูุงุนุฏุฉ ุงููููุชูุฑุงุช")
                return
            
            all_chunks = []
            
            # ูุนุงูุฌุฉ ูู ูุซููุฉ
            for i, doc in enumerate(st.session_state.documents):
                status_text.text(f"ูุนุงูุฌุฉ: {doc['name']} ({i+1}/{len(st.session_state.documents)})")
                
                # ุชูุณูู ุงููุต ููุทุน ุฐููุฉ
                chunks = st.session_state.doc_processor.intelligent_chunk(
                    doc['content'],
                    chunk_size=500,
                    overlap=50
                )
                
                # ุฅุถุงูุฉ ูุนูููุงุช ุงููุซููุฉ ููู ูุทุนุฉ
                for j, chunk in enumerate(chunks):
                    chunk.update({
                        'doc_id': doc['id'],
                        'doc_name': doc['name'],
                        'doc_type': doc['type'],
                        'chunk_id': f"{doc['id']}_{j}",
                        'global_id': len(all_chunks)
                    })
                    all_chunks.append(chunk)
                
                # ุชุญุฏูุซ ุงููุซููุฉ ููุนุงูุฌุฉ
                doc['processed'] = True
                
                progress_bar.progress((i + 1) / len(st.session_state.documents))
            
            # ุฅุถุงูุฉ ุงููุทุน ููุฎุฒู ุงููููุชูุฑุงุช
            status_text.text("๐ ุฅูุดุงุก ููุฑุณ ุงูุจุญุซ...")
            success = st.session_state.vector_store.add_documents(all_chunks)
            
            if success:
                st.session_state.processing_stats['chunks_created'] = len(all_chunks)
                st.session_state.processing_stats['last_update'] = datetime.now().isoformat()
                
                st.success(f"โ ุชู ุฅูุดุงุก ููุฑุณ ุงูุจุญุซ ุจูุฌุงุญ! ({len(all_chunks)} ูุทุนุฉ)")
                
                # ุนุฑุถ ุฅุญุตุงุฆูุงุช ุณุฑูุนุฉ
                stats = st.session_state.vector_store.get_stats()
                if stats:
                    st.json(stats)
            else:
                st.error("โ ูุดู ูู ุฅูุดุงุก ููุฑุณ ุงูุจุญุซ")
            
        except Exception as e:
            st.error(f"โ ุฎุทุฃ ูู ุฅูุดุงุก ุงูููุฑุณ: {str(e)}")
        finally:
            progress_bar.empty()
            status_text.empty()

def render_chat_tab():
    """ุชุจููุจ ุงููุญุงุฏุซุฉ ุงููุชูุฏู"""
    st.header("๐ฌ ุงููุญุงุฏุซุฉ ุงูุฐููุฉ ูุน ุงููุซุงุฆู")
    
    # ุงูุชุญูู ูู ุงูุฌุงูุฒูุฉ
    requirements = check_system_requirements()
    
    if not requirements['api_connection']:
        st.warning("โ๏ธ ูุฑุฌู ุฅุนุฏุงุฏ ุงุชุตุงู AI ูู ุงูุดุฑูุท ุงูุฌุงูุจู")
        return
    
    if not requirements['documents_loaded']:
        st.warning("โ๏ธ ูุฑุฌู ุชุญููู ููุนุงูุฌุฉ ุงููุซุงุฆู ุฃููุงู")
        return
    
    if not requirements['vector_store']:
        st.warning("โ๏ธ ูุฑุฌู ุฅูุดุงุก ููุฑุณ ุงูุจุญุซ ูู ุชุจููุจ ุงููุซุงุฆู")
        if st.button("๐ ุฅูุดุงุก ุงูููุฑุณ ุงูุขู"):
            create_search_index()
        return
    
    st.success("โ ุงููุธุงู ุฌุงูุฒ ูููุญุงุฏุซุฉ!")
    
    # ุนุฑุถ ุงููุญุงุฏุซุงุช ุงูุณุงุจูุฉ
    conversations = st.session_state.conversation_manager.get_recent_conversations(5)
    
    if conversations:
        st.subheader("๐ฌ ุงููุญุงุฏุซุงุช ุงูุฃุฎูุฑุฉ")
        
        # ุญุงูู ุงููุญุงุฏุซุงุช ูุน ุชูุฑูุฑ
        st.markdown('<div class="chat-container">', unsafe_allow_html=True)
        
        for conv in conversations:
            # ุฑุณุงูุฉ ุงููุณุชุฎุฏู
            st.markdown(f"""
            <div class="message-user">
                <strong>๐ค ุฃูุช:</strong><br>
                {conv['query']}
                <br><small>โฐ {conv['timestamp'][:16].replace('T', ' ')}</small>
            </div>
            """, unsafe_allow_html=True)
            
            # ุฑุฏ ุงูุฐูุงุก ุงูุงุตุทูุงุนู
            st.markdown(f"""
            <div class="message-ai">
                <strong>๐ค ุงููุณุงุนุฏ:</strong><br>
                {conv['response']}
            </div>
            """, unsafe_allow_html=True)
            
            # ุงููุตุงุฏุฑ
            if conv.get('sources'):
                with st.expander(f"๐ ุงููุตุงุฏุฑ ({len(conv['sources'])}) - ุงููุฑ ููุนุฑุถ"):
                    for i, source in enumerate(conv['sources'], 1):
                        st.markdown(f"""
                        <div class="source-card">
                            <strong>ูุตุฏุฑ {i}: {source['metadata'].get('doc_name', 'ุบูุฑ ูุญุฏุฏ')}</strong>
                            <span style="float: right; background: #4caf50; color: white; padding: 2px 8px; border-radius: 4px; font-size: 0.8em;">
                                ุชุดุงุจู: {source['score']:.2f}
                            </span>
                            <br><br>
                            {source['text'][:200]}...
                        </div>
                        """, unsafe_allow_html=True)
        
        st.markdown('</div>', unsafe_allow_html=True)
        st.divider()
    
    # ูุฑุจุน ุงูุณุคุงู ุงูุฌุฏูุฏ
    st.subheader("โ ุงุทุฑุญ ุณุคุงูู")
    
    with st.form("advanced_question_form", clear_on_submit=True):
        col1, col2 = st.columns([3, 1])
        
        with col1:
            user_question = st.text_area(
                "ุณุคุงูู:",
                height=120,
                placeholder="ูุซุงู: ูุง ูู ุงูููุงุท ุงูุฑุฆูุณูุฉ ูู ุงููุซุงุฆูุ ุฃู ุงุดุฑุญ ูู ููุถูุน ูุนูู...",
                help="ุงูุชุจ ุณุคุงูู ุจูุถูุญ ููุญุตูู ุนูู ุฃูุถู ุฅุฌุงุจุฉ"
            )
        
        with col2:
            st.markdown("**ุฅุนุฏุงุฏุงุช ุงูุจุญุซ:**")
            search_depth = st.slider("ุนูู ุงูุจุญุซ", 3, 15, 8)
            min_similarity = st.slider("ุญุฏ ุงูุชุดุงุจู", 0.1, 0.9, 0.4, 0.1)
            response_length = st.selectbox("ุทูู ุงูุฅุฌุงุจุฉ", 
                ["ูุตูุฑุฉ (400)", "ูุชูุณุทุฉ (800)", "ููุตูุฉ (1200)"])
        
        # ุฃุฒุฑุงุฑ ุงูุฅุฑุณุงู ูุงูุฎูุงุฑุงุช
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            submitted = st.form_submit_button("๐ ุฅุฑุณุงู ุงูุณุคุงู", type="primary")
        
        with col2:
            search_only = st.form_submit_button("๐ ุจุญุซ ููุท")
        
        with col3:
            advanced_mode = st.checkbox("ุงููุถุน ุงููุชูุฏู")
    
    # ูุนุงูุฌุฉ ุงูุทูุจ
    if (submitted or search_only) and user_question.strip():
        process_user_query(
            user_question, 
            search_depth, 
            min_similarity, 
            response_length,
            search_only,
            advanced_mode
        )

def process_user_query(question: str, depth: int, min_sim: float, 
                      length: str, search_only: bool, advanced: bool):
    """ูุนุงูุฌุฉ ุงุณุชุนูุงู ุงููุณุชุฎุฏู"""
    start_time = time.time()
    
    with st.spinner("๐ ุฌุงุฑู ุงูุจุญุซ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ..."):
        # ุงูุจุญุซ ูู ุงููููุชูุฑุงุช
        search_results = st.session_state.vector_store.search(
            question, 
            k=depth, 
            min_score=min_sim
        )
        
        search_time = time.time() - start_time
        
        if not search_results:
            st.error("โ ูู ุฃุฌุฏ ูุนูููุงุช ุฐุงุช ุตูุฉ ุจุณุคุงูู. ุฌุฑุจ:")
            st.markdown("""
            - ุชูููู ุญุฏ ุงูุชุดุงุจู
            - ุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู ุจูููุงุช ุฃุฎุฑู
            - ุงูุชุฃูุฏ ูู ูุฌูุฏ ูุนูููุงุช ุฐุงุช ุตูุฉ ูู ุงููุซุงุฆู
            """)
            return
        
        # ุนุฑุถ ูุชุงุฆุฌ ุงูุจุญุซ
        st.success(f"โ ุชู ุงูุนุซูุฑ ุนูู {len(search_results)} ูุตุฏุฑ ุฐู ุตูุฉ ูู {search_time:.2f} ุซุงููุฉ")
        
        with st.expander(f"๐ ูุชุงุฆุฌ ุงูุจุญุซ ({len(search_results)})"):
            for i, result in enumerate(search_results, 1):
                st.markdown(f"""
                **ูุชูุฌุฉ {i}**: {result['metadata'].get('doc_name', 'ุบูุฑ ูุญุฏุฏ')}
                **ุฏุฑุฌุฉ ุงูุชุดุงุจู**: {result['score']:.3f}
                **ุงููุต**: {result['text'][:150]}...
                """)
                st.divider()
        
        # ุฅุฐุง ูุงู ุงูุจุญุซ ููุทุ ูุชููู ููุง
        if search_only:
            return
        
        # ุชุญุถูุฑ ุงูุณูุงู ููุฐูุงุก ุงูุงุตุทูุงุนู
        context_parts = []
        sources_info = []
        
        for result in search_results:
            context_parts.append(f"ุงููุตุฏุฑ: {result['metadata'].get('doc_name', 'ุบูุฑ ูุญุฏุฏ')}\n{result['text']}")
            sources_info.append({
                'text': result['text'],
                'metadata': result['metadata'],
                'score': result['score'],
                'id': result['id']
            })
        
        context = '\n\n---\n\n'.join(context_parts)
        
        # ุชุญุฏูุฏ ุทูู ุงูุฅุฌุงุจุฉ
        max_tokens = {
            "ูุตูุฑุฉ (400)": 400,
            "ูุชูุณุทุฉ (800)": 800,
            "ููุตูุฉ (1200)": 1200
        }.get(length, 800)
        
        # ุชูููุฏ ุงูุฅุฌุงุจุฉ
        with st.spinner("๐ค ุฌุงุฑู ุชูููุฏ ุงูุฅุฌุงุจุฉ ุงูุฐููุฉ..."):
            response_start = time.time()
            
            answer = st.session_state.api_client.generate_response(
                question, 
                context, 
                max_tokens=max_tokens
            )
            
            response_time = time.time() - response_start
            total_time = time.time() - start_time
            
            if answer.startswith("ุฎุทุฃ"):
                st.error(f"โ {answer}")
                return
            
            # ุนุฑุถ ุงูุฅุฌุงุจุฉ
            st.markdown("### ๐ค ุงูุฅุฌุงุจุฉ:")
            st.markdown(f"""
            <div class="message-ai" style="margin: 1rem 0;">
                {answer}
            </div>
            """, unsafe_allow_html=True)
            
            # ูุนูููุงุช ุงูุฃุฏุงุก
            if advanced:
                st.markdown("### โก ูุนูููุงุช ุงูุฃุฏุงุก:")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ููุช ุงูุจุญุซ", f"{search_time:.2f}s")
                with col2:
                    st.metric("ููุช ุงูุฅุฌุงุจุฉ", f"{response_time:.2f}s")
                with col3:
                    st.metric("ุงูููุช ุงูุฅุฌูุงูู", f"{total_time:.2f}s")
                with col4:
                    st.metric("ุงููุตุงุฏุฑ ุงููุณุชุฎุฏูุฉ", len(search_results))
            
            # ุญูุธ ุงููุญุงุฏุซุฉ
            st.session_state.conversation_manager.add_conversation(
                question, 
                answer, 
                sources_info,
                {
                    'search_time': search_time,
                    'response_time': response_time,
                    'total_time': total_time,
                    'sources_count': len(search_results),
                    'settings': {
                        'depth': depth,
                        'min_similarity': min_sim,
                        'max_tokens': max_tokens
                    }
                }
            )
            
            # ุชุญุฏูุซ ุงูุฅุญุตุงุฆูุงุช
            st.session_state.processing_stats['queries_processed'] += 1
            st.session_state.processing_stats['average_response_time'] = (
                (st.session_state.processing_stats['average_response_time'] * 
                 (st.session_state.processing_stats['queries_processed'] - 1) + total_time) / 
                st.session_state.processing_stats['queries_processed']
            )
            
            # ุฎูุงุฑุงุช ุงููุชุงุจุนุฉ
            st.markdown("### ๐ ุฎูุงุฑุงุช ุงููุชุงุจุนุฉ:")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("๐ ุฅุฌุงุจุฉ ูููุฏุฉ"):
                    st.success("ุดูุฑุงู ูุชููููู!")
            
            with col2:
                if st.button("๐ ุฃุนุฏ ุงูุตูุงุบุฉ"):
                    st.info("ุฌุฑุจ ุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู ุจุทุฑููุฉ ูุฎุชููุฉ")
            
            with col3:
                if st.button("๐ ูุตุงุฏุฑ ุฃูุซุฑ"):
                    # ุจุญุซ ููุณุน
                    expanded_results = st.session_state.vector_store.search(
                        question, k=depth*2, min_score=min_sim*0.8
                    )
                    st.info(f"ุชู ุงูุนุซูุฑ ุนูู {len(expanded_results)} ูุตุฏุฑ ุฅุถุงูู")

def render_analytics_tab():
    """ุชุจููุจ ุงูุชุญูููุงุช ุงููุชูุฏู"""
    st.header("๐ ุงูุชุญูููุงุช ูุงูุฅุญุตุงุฆูุงุช ุงููุชูุฏูุฉ")
    
    # ุฅุญุตุงุฆูุงุช ุงููุธุงู ุงูุฑุฆูุณูุฉ
    st.subheader("๐ฅ๏ธ ุญุงูุฉ ุงููุธุงู")
    
    col1, col2, col3, col4 = st.columns(4)
    
    stats = st.session_state.processing_stats
    conv_stats = st.session_state.conversation_manager.get_statistics()
    
    with col1:
        st.markdown("""
        <div class="metric-card">
            <h3>๐</h3>
            <h2>%d</h2>
            <p>ูุซููุฉ ูุนุงูุฌุฉ</p>
        </div>
        """ % stats['documents_processed'], unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="metric-card">
            <h3>๐</h3>
            <h2>%d</h2>
            <p>ุงุณุชุนูุงู</p>
        </div>
        """ % stats['queries_processed'], unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="metric-card">
            <h3>โก</h3>
            <h2>%.1fs</h2>
            <p>ูุชูุณุท ุงูุงุณุชุฌุงุจุฉ</p>
        </div>
        """ % stats['average_response_time'], unsafe_allow_html=True)
    
    with col4:
        total_chunks = stats['chunks_created']
        st.markdown("""
        <div class="metric-card">
            <h3>๐</h3>
            <h2>%d</h2>
            <p>ูุทุนุฉ ูุตูุฉ</p>
        </div>
        """ % total_chunks, unsafe_allow_html=True)
    
    st.divider()
    
    # ุชุญููู ุงููุซุงุฆู
    if st.session_state.documents:
        st.subheader("๐ ุชุญููู ุงููุซุงุฆู")
        
        # ุฅุนุฏุงุฏ ุงูุจูุงูุงุช ูููุฎุทุทุงุช
        doc_data = []
        for doc in st.session_state.documents:
            doc_data.append({
                'ุงูุงุณู': doc['name'][:20] + '...' if len(doc['name']) > 20 else doc['name'],
                'ุงููููุงุช': doc.get('word_count', 0),
                'ุงูุฃุญุฑู': doc.get('char_count', 0),
                'ุงูููุน': doc.get('type', 'ุบูุฑ ูุญุฏุฏ')
            })
        
        df_docs = pd.DataFrame(doc_data)
        
        # ูุฎุทุทุงุช ุชูุงุนููุฉ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("๐ ุชูุฒูุน ุงููููุงุช")
            if len(df_docs) > 1:
                st.bar_chart(df_docs.set_index('ุงูุงุณู')['ุงููููุงุช'])
            else:
                st.info("ูุญุชุงุฌ ุฃูุซุฑ ูู ูุซููุฉ ูุงุญุฏุฉ ูุนุฑุถ ุงููุฎุทุท")
        
        with col2:
            st.subheader("๐ ุชูุฒูุน ุงูุฃููุงุน")
            type_counts = df_docs['ุงูููุน'].value_counts()
            st.bar_chart(type_counts)
        
        # ุฌุฏูู ููุตู
        st.subheader("๐ ุชูุงุตูู ุงููุซุงุฆู")
        st.dataframe(df_docs, use_container_width=True)
    
    # ุชุญููู ุงููุญุงุฏุซุงุช
    conversations = st.session_state.conversation_manager.conversations
    if conversations:
        st.divider()
        st.subheader("๐ฌ ุชุญููู ุงููุญุงุฏุซุงุช")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            avg_query_len = conv_stats.get('avg_query_length', 0)
            st.metric("ูุชูุณุท ุทูู ุงูุณุคุงู", f"{avg_query_len} ูููุฉ")
        
        with col2:
            avg_response_len = conv_stats.get('avg_response_length', 0)
            st.metric("ูุชูุณุท ุทูู ุงูุฅุฌุงุจุฉ", f"{avg_response_len} ูููุฉ")
        
        with col3:
            avg_sources = conv_stats.get('avg_sources_per_query', 0)
            st.metric("ูุชูุณุท ุงููุตุงุฏุฑ", f"{avg_sources}")
        
        # ูุฎุทุท ุฒููู ููุงุณุชุนูุงูุงุช
        if len(conversations) > 1:
            st.subheader("๐ ูุดุงุท ุงูุงุณุชุนูุงูุงุช ุนุจุฑ ุงูููุช")
            
            # ุชุญุถูุฑ ุงูุจูุงูุงุช ุงูุฒูููุฉ
            time_data = []
            for conv in conversations:
                timestamp = datetime.fromisoformat(conv['timestamp'])
                time_data.append({
                    'ุงูููุช': timestamp.strftime('%H:%M'),
                    'ุงูุชุงุฑูุฎ': timestamp.strftime('%Y-%m-%d'),
                    'ุนุฏุฏ ุงููุตุงุฏุฑ': len(conv.get('sources', []))
                })
            
            df_time = pd.DataFrame(time_data)
            
            # ูุฎุทุท ุจูุงูู
            if len(df_time) > 2:
                daily_counts = df_time['ุงูุชุงุฑูุฎ'].value_counts().sort_index()
                st.line_chart(daily_counts)
    
    # ุชุญููู ุงูุฃุฏุงุก
    st.divider()
    st.subheader("โก ุชุญููู ุงูุฃุฏุงุก")
    
    if conversations:
        response_times = [
            conv['metadata'].get('total_time', 0) 
            for conv in conversations 
            if conv.get('metadata')
        ]
        
        if response_times:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("ุฃุณุฑุน ุงุณุชุฌุงุจุฉ", f"{min(response_times):.2f}s")
            
            with col2:
                st.metric("ุฃุจุทุฃ ุงุณุชุฌุงุจุฉ", f"{max(response_times):.2f}s")
            
            with col3:
                st.metric("ุงูุงูุญุฑุงู ุงููุนูุงุฑู", f"{np.std(response_times):.2f}s")
            
            # ุฑุณู ุจูุงูู ูุฃููุงุช ุงูุงุณุชุฌุงุจุฉ
            if len(response_times) > 2:
                st.subheader("๐ ุชูุฒูุน ุฃููุงุช ุงูุงุณุชุฌุงุจุฉ")
                st.bar_chart(response_times)
    
    # ุชุตุฏูุฑ ุงูุชูุงุฑูุฑ
    st.divider()
    st.subheader("๐ ุชุตุฏูุฑ ุงูุชูุงุฑูุฑ")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("๐ ุชูุฑูุฑ ุดุงูู (JSON)"):
            report = generate_comprehensive_report()
            st.download_button(
                "๐พ ุชุญููู ุงูุชูุฑูุฑ",
                data=report,
                file_name=f"rag_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
    
    with col2:
        if st.button("๐ ุฅุญุตุงุฆูุงุช (CSV)"):
            csv_data = export_stats_csv()
            st.download_button(
                "๐พ ุชุญููู CSV",
                data=csv_data,
                file_name=f"rag_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
    
    with col3:
        if st.button("๐ ุชุญุฏูุซ ุงูุฅุญุตุงุฆูุงุช"):
            st.rerun()

def render_advanced_settings_tab():
    """ุชุจููุจ ุงูุฅุนุฏุงุฏุงุช ุงููุชูุฏูุฉ"""
    st.header("โ๏ธ ุงูุฅุนุฏุงุฏุงุช ุงููุชูุฏูุฉ")
    
    # ุฅุนุฏุงุฏุงุช ุงููููุฐุฌ
    st.subheader("๐ค ุฅุนุฏุงุฏุงุช ุงููููุฐุฌ")
    
    with st.expander("๐ง ุฅุนุฏุงุฏุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู"):
        col1, col2 = st.columns(2)
        
        with col1:
            temperature = st.slider("ุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ (ุงูุฅุจุฏุงุน)", 0.0, 2.0, 0.3, 0.1)
            top_p = st.slider("Top P (ุงูุชููุน)", 0.0, 1.0, 0.9, 0.05)
        
        with col2:
            frequency_penalty = st.slider("ุนููุจุฉ ุงูุชูุฑุงุฑ", 0.0, 2.0, 0.1, 0.1)
            presence_penalty = st.slider("ุนููุจุฉ ุงููุฌูุฏ", 0.0, 2.0, 0.1, 0.1)
        
        st.info("๐ก ุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ ุงูููุฎูุถุฉ = ุฅุฌุงุจุงุช ุฃูุซุฑ ุฏูุฉุ ุงูุนุงููุฉ = ุฅุฌุงุจุงุช ุฃูุซุฑ ุฅุจุฏุงุนุงู")
    
    # ุฅุนุฏุงุฏุงุช ุงูุจุญุซ
    st.subheader("๐ ุฅุนุฏุงุฏุงุช ุงูุจุญุซ ุงููุชูุฏูุฉ")
    
    with st.expander("โ๏ธ ุฎูุงุฑุฒููุฉ ุงูุจุญุซ"):
        search_algorithm = st.selectbox(
            "ุฎูุงุฑุฒููุฉ ุงูุจุญุซ:",
            ["Cosine Similarity", "Euclidean Distance", "Dot Product"]
        )
        
        col1, col2 = st.columns(2)
        with col1:
            chunk_overlap_strategy = st.selectbox(
                "ุงุณุชุฑุงุชูุฌูุฉ ุงูุชุฏุงุฎู:",
                ["ุซุงุจุช", "ูุชุบูุฑ", "ุฐูู"]
            )
        
        with col2:
            rerank_results = st.checkbox("ุฅุนุงุฏุฉ ุชุฑุชูุจ ุงููุชุงุฆุฌ", value=True)
        
        max_context_length = st.slider("ุฃูุตู ุทูู ููุณูุงู", 1000, 8000, 4000, 200)
    
    # ุฅุนุฏุงุฏุงุช ุงููุนุงูุฌุฉ
    st.subheader("๐ ุฅุนุฏุงุฏุงุช ุงููุนุงูุฌุฉ")
    
    with st.expander("๐ ูุนุงูุฌุฉ ุงููุตูุต"):
        col1, col2 = st.columns(2)
        
        with col1:
            remove_stopwords = st.checkbox("ุฅุฒุงูุฉ ูููุงุช ุงูุฅููุงู", value=False)
            normalize_text = st.checkbox("ุชุทุจูุน ุงููุต", value=True)
        
        with col2:
            clean_html = st.checkbox("ุชูุธูู HTML", value=True)
            preserve_formatting = st.checkbox("ุงูุญูุงุธ ุนูู ุงูุชูุณูู", value=False)
        
        language_detection = st.selectbox(
            "ูุดู ุงููุบุฉ:",
            ["ุชููุงุฆู", "ุนุฑุจู ููุท", "ุฅูุฌููุฒู ููุท", "ูุฎุชูุท"]
        )
    
    # ุฅุนุฏุงุฏุงุช ุงูุฃุฏุงุก
    st.subheader("โก ุฅุนุฏุงุฏุงุช ุงูุฃุฏุงุก")
    
    with st.expander("๐ ุชุญุณูู ุงูุฃุฏุงุก"):
        col1, col2 = st.columns(2)
        
        with col1:
            enable_caching = st.checkbox("ุชูุนูู ุงูุชุฎุฒูู ุงููุคูุช", value=True)
            batch_processing = st.checkbox("ุงููุนุงูุฌุฉ ุงููุฌูุนุฉ", value=True)
        
        with col2:
            parallel_processing = st.checkbox("ุงููุนุงูุฌุฉ ุงููุชูุงุฒูุฉ", value=False)
            memory_optimization = st.checkbox("ุชุญุณูู ุงูุฐุงูุฑุฉ", value=True)
        
        cache_size = st.slider("ุญุฌู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช", 50, 1000, 200, 50)
        batch_size = st.slider("ุญุฌู ุงูุฏูุนุฉ", 8, 128, 32, 8)
    
    # ุฅุนุฏุงุฏุงุช ุงููุงุฌูุฉ
    st.subheader("๐จ ุฅุนุฏุงุฏุงุช ุงููุงุฌูุฉ")
    
    with st.expander("๐ญ ุชุฎุตูุต ุงููุงุฌูุฉ"):
        theme_color = st.color_picker("ููู ุงููุธูุฑ ุงูุฑุฆูุณู", "#667eea")
        
        col1, col2 = st.columns(2)
        with col1:
            show_source_preview = st.checkbox("ูุนุงููุฉ ุงููุตุงุฏุฑ", value=True)
            show_confidence_scores = st.checkbox("ุนุฑุถ ุฏุฑุฌุงุช ุงูุซูุฉ", value=True)
        
        with col2:
            auto_scroll = st.checkbox("ุงูุชูุฑูุฑ ุงูุชููุงุฆู", value=True)
            compact_mode = st.checkbox("ุงููุถุน ุงููุถุบูุท", value=False)
        
        results_per_page = st.slider("ุงููุชุงุฆุฌ ูู ุงูุตูุญุฉ", 5, 50, 10, 5)
    
    # ุฅุนุฏุงุฏุงุช ุงูุฃูุงู
    st.subheader("๐ ุฅุนุฏุงุฏุงุช ุงูุฃูุงู")
    
    with st.expander("๐ก๏ธ ุงูุฃูุงู ูุงูุฎุตูุตูุฉ"):
        col1, col2 = st.columns(2)
        
        with col1:
            content_filter = st.checkbox("ููุชุฑุฉ ุงููุญุชูู", value=True)
            rate_limiting = st.checkbox("ุชุญุฏูุฏ ูุนุฏู ุงูุทูุจุงุช", value=True)
        
        with col2:
            log_queries = st.checkbox("ุชุณุฌูู ุงูุงุณุชุนูุงูุงุช", value=False)
            encrypt_cache = st.checkbox("ุชุดููุฑ ุงูุชุฎุฒูู ุงููุคูุช", value=False)
        
        max_queries_per_hour = st.slider("ุฃูุตู ุงุณุชุนูุงู/ุณุงุนุฉ", 10, 1000, 100, 10)
    
    # ุญูุธ ูุฅุนุงุฏุฉ ุถุจุท
    st.divider()
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        if st.button("๐พ ุญูุธ ุงูุฅุนุฏุงุฏุงุช", type="primary"):
            # ุญูุธ ุงูุฅุนุฏุงุฏุงุช ูู session state
            settings = {
                'model_settings': {
                    'temperature': temperature,
                    'top_p': top_p,
                    'frequency_penalty': frequency_penalty,
                    'presence_penalty': presence_penalty
                },
                'search_settings': {
                    'algorithm': search_algorithm,
                    'chunk_overlap_strategy': chunk_overlap_strategy,
                    'rerank_results': rerank_results,
                    'max_context_length': max_context_length
                },
                'processing_settings': {
                    'remove_stopwords': remove_stopwords,
                    'normalize_text': normalize_text,
                    'clean_html': clean_html,
                    'preserve_formatting': preserve_formatting,
                    'language_detection': language_detection
                },
                'performance_settings': {
                    'enable_caching': enable_caching,
                    'batch_processing': batch_processing,
                    'parallel_processing': parallel_processing,
                    'memory_optimization': memory_optimization,
                    'cache_size': cache_size,
                    'batch_size': batch_size
                },
                'ui_settings': {
                    'theme_color': theme_color,
                    'show_source_preview': show_source_preview,
                    'show_confidence_scores': show_confidence_scores,
                    'auto_scroll': auto_scroll,
                    'compact_mode': compact_mode,
                    'results_per_page': results_per_page
                },
                'security_settings': {
                    'content_filter': content_filter,
                    'rate_limiting': rate_limiting,
                    'log_queries': log_queries,
                    'encrypt_cache': encrypt_cache,
                    'max_queries_per_hour': max_queries_per_hour
                }
            }
            
            st.session_state.advanced_settings = settings
            st.success("ุชู ุญูุธ ุงูุฅุนุฏุงุฏุงุช ุจูุฌุงุญ!")
    
    with col2:
        if st.button("๐ ุฅุนุงุฏุฉ ุงูุถุจุท"):
            if 'advanced_settings' in st.session_state:
                del st.session_state.advanced_settings
            st.info("ุชู ุฅุนุงุฏุฉ ุถุจุท ุงูุฅุนุฏุงุฏุงุช")
            st.rerun()
    
    with col3:
        st.info("ุชุทุจู ุงูุฅุนุฏุงุฏุงุช ุนูู ุงูุฌูุณุฉ ุงูุญุงููุฉ ููุท")

def render_help_tab():
    """ุชุจููุจ ุงููุณุงุนุฏุฉ ุงูุดุงูู"""
    st.header("โ ุงููุณุงุนุฏุฉ ูุงูุฏููู ุงูุดุงูู")
    
    # ุฏููู ุงูุจุฏุก ุงูุณุฑูุน
    st.subheader("๐ ุฏููู ุงูุจุฏุก ุงูุณุฑูุน")
    
    with st.expander("1๏ธโฃ ุงูุฅุนุฏุงุฏ ุงูุฃููู", expanded=True):
        st.markdown("""
        **ุงูุฎุทูุฉ ุงูุฃููู: ูุญุต ุงููุชุทูุจุงุช**
        
        ุชุฃูุฏ ูู ูุฌูุฏ ุงูุญุงูุฉ ุงูุชุงููุฉ ูู ุงูุดุฑูุท ุงูุฌุงูุจู:
        - โ ุงูููุชุจุงุช ุงููุชูุฏูุฉ: ูุชุตู
        - โ ุงุชุตุงู AI: ูุชุตู  
        - โ ูุงุนุฏุฉ ุงููููุชูุฑุงุช: ูุชุตู
        - โ ุงููุซุงุฆู ุงููุญููุฉ: ูุชุตู
        
        **ุฅุนุฏุงุฏ ุงูุฐูุงุก ุงูุงุตุทูุงุนู:**
        1. ุงุฎุชุฑ ููุฏู ุงูุฎุฏูุฉ (OpenAI ุฃู Groq)
        2. ุฃุฏุฎู ููุชุงุญ API ุงูุตุญูุญ
        3. ุงุถุบุท "ุงุชุตุงู" ูุงูุชุธุฑ ุงูุชุฃููุฏ
        
        **ุงูุญุตูู ุนูู ููุงุชูุญ API:**
        - **OpenAI**: [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
        - **Groq**: [console.groq.com/keys](https://console.groq.com/keys)
        """)
    
    with st.expander("2๏ธโฃ ุชุญููู ููุนุงูุฌุฉ ุงููุซุงุฆู"):
        st.markdown("""
        **ุฃููุงุน ุงููููุงุช ุงููุฏุนููุฉ:**
        - ๐ **TXT**: ูููุงุช ูุตูุฉ ุนุงุฏูุฉ
        - ๐ **PDF**: ูุณุชูุฏุงุช PDF (ูุน ุงุณุชุฎุฑุงุฌ ุงููุต)
        - ๐ **DOCX**: ูุณุชูุฏุงุช Microsoft Word
        - ๐ **CSV**: ูููุงุช ุงูุจูุงูุงุช ุงููุฌุฏููุฉ
        
        **ุฎุทูุงุช ุงููุนุงูุฌุฉ:**
        1. ุงุฎุชุฑ ุงููููุงุช ูู ุฌูุงุฒู ุฃู ุฃุฏุฎู ูุต ูุจุงุดุฑ
        2. ุงุถุบุท "ูุนุงูุฌุฉ ุฌููุน ุงููููุงุช"
        3. ุงูุชุธุฑ ุฅูุดุงุก ููุฑุณ ุงูุจุญุซ
        4. ุชุฃูุฏ ูู ุธููุฑ "ุงููุธุงู ุฌุงูุฒ ูููุญุงุฏุซุฉ"
        
        **ูุตุงุฆุญ ููุญุตูู ุนูู ุฃูุถู ูุชุงุฆุฌ:**
        - ุงุณุชุฎุฏู ูุตูุต ูุงุถุญุฉ ูููุธูุฉ
        - ุชุฌูุจ ุงููููุงุช ุงููููุฆุฉ ุจุงูุตูุฑ ููุท
        - ูููุตูุต ุงูุทูููุฉุ ูุณููุง ููููุงุช ุฃุตุบุฑ
        """)
    
    with st.expander("3๏ธโฃ ุงููุญุงุฏุซุฉ ุงููุนุงูุฉ"):
        st.markdown("""
        **ุฃููุงุน ุงูุฃุณุฆูุฉ ุงูููุงุณุจุฉ:**
        - ุฃุณุฆูุฉ ุนู ูุญุชูู ูุญุฏุฏ ูู ุงููุซุงุฆู
        - ุทูุจ ุชูุฎูุต ุฃู ุงุณุชุฎุฑุงุฌ ููุงุท ุฑุฆูุณูุฉ  
        - ุดุฑุญ ููุงููู ุฃู ูุตุทูุญุงุช
        - ููุงุฑูุงุช ุจูู ููุถูุนุงุช ูุฎุชููุฉ
        - ุชุญููู ุงูุจูุงูุงุช ูุงููุนูููุงุช
        
        **ุฃูุซูุฉ ุนูู ุฃุณุฆูุฉ ุฌูุฏุฉ:**
        - "ูุง ูู ุงูุชูุตูุงุช ุงูุฑุฆูุณูุฉ ูู ุงูุชูุฑูุฑุ"
        - "ุงุดุฑุญ ูู ููููู X ููุง ูุฑุฏ ูู ุงููุซุงุฆู"
        - "ูุงุฑู ุจูู ุงูููุฌ A ูุงูููุฌ B"
        - "ูุง ูู ุงูุชุญุฏูุงุช ุงููุฐููุฑุฉ ูู ุงููุดุฑูุนุ"
        
        **ุฅุนุฏุงุฏุงุช ุงูุจุญุซ:**
        - **ุนูู ุงูุจุญุซ**: 3-5 ููุฃุณุฆูุฉ ุงูุจุณูุทุฉุ 8-15 ูููุนูุฏุฉ
        - **ุญุฏ ุงูุชุดุงุจู**: 0.3-0.5 ููุจุญุซ ุงููุงุณุนุ 0.6-0.8 ููุฏููู
        - **ุทูู ุงูุฅุฌุงุจุฉ**: ุงุฎุชุฑ ุญุณุจ ูุณุชูู ุงูุชูุตูู ุงููุทููุจ
        """)
    
    # ูุดุงูู ุดุงุฆุนุฉ ูุญููููุง
    st.subheader("๐ง ูุดุงูู ุดุงุฆุนุฉ ูุญููููุง")
    
    issues = [
        {
            "title": "โ ุงูููุชุจุงุช ุงููุชูุฏูุฉ ุบูุฑ ูุซุจุชุฉ",
            "problem": "ูุธูุฑ ุชุญุฐูุฑ ุฃู ุงูููุชุจุงุช ุบูุฑ ูุชุงุญุฉ",
            "solution": """
            **ุงูุญู:**
            ```bash
            pip install sentence-transformers chromadb PyPDF2 python-docx nltk
            ```
            ุฃู ูู Colab:
            ```python
            !pip install sentence-transformers chromadb PyPDF2 python-docx nltk
            ```
            ุซู ุฃุนุฏ ุชุดุบูู ุงูุชุทุจูู.
            """
        },
        {
            "title": "โ ูุดู ุงูุงุชุตุงู ุจู API", 
            "problem": "ุฑุณุงูุฉ ุฎุทุฃ ุนูุฏ ูุญุงููุฉ ุงูุงุชุตุงู",
            "solution": """
            **ุชุญูู ูู:**
            - ุตุญุฉ ููุชุงุญ API (ุจุฏูู ูุณุงูุงุช ุฒุงุฆุฏุฉ)
            - ูุฌูุฏ ุฑุตูุฏ ูุงูู ูู ุญุณุงุจู
            - ุงูุงุชุตุงู ุจุงูุฅูุชุฑูุช
            - ุญุงูุฉ ุฎุฏูุฉ ููุฏู ุงูุฎุฏูุฉ
            
            **ุฌุฑุจ:**
            - ููุฏู ุฎุฏูุฉ ุขุฎุฑ (Groq ุจุฏูุงู ูู OpenAI)
            - ุฅุนุงุฏุฉ ุฅูุดุงุก ููุชุงุญ API ุฌุฏูุฏ
            """
        },
        {
            "title": "โ๏ธ ูู ุฃุฌุฏ ูุนูููุงุช ูุงููุฉ",
            "problem": "ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุง ูุฌุฏ ุฅุฌุงุจุงุช",
            "solution": """
            **ุฌุฑุจ:**
            - ุชูููู ุญุฏ ุงูุชุดุงุจู ุฅูู 0.2-0.3
            - ุฒูุงุฏุฉ ุนูู ุงูุจุญุซ ุฅูู 10-15
            - ุฅุนุงุฏุฉ ุตูุงุบุฉ ุงูุณุคุงู ุจูููุงุช ูุฎุชููุฉ
            - ุงูุชุฃูุฏ ูู ูุฌูุฏ ุงููุนูููุงุช ูุนูุงู ูู ุงููุซุงุฆู
            - ุงุณุชุฎุฏุงู ูููุงุช ููุชุงุญูุฉ ูู ุงููุต ุงูุฃุตูู
            """
        },
        {
            "title": "๐ ุงูุฃุฏุงุก ุจุทูุก",
            "problem": "ููุช ุงุณุชุฌุงุจุฉ ุทููู",
            "solution": """
            **ุชุญุณูู ุงูุฃุฏุงุก:**
            - ููู ุนูู ุงูุจุญุซ ุฅูู 5-8
            - ุงุณุชุฎุฏู ูุตูุต ุฃุตุบุฑ ุญุฌูุงู
            - ูุนู "ุชุญุณูู ุงูุฐุงูุฑุฉ" ูู ุงูุฅุนุฏุงุฏุงุช ุงููุชูุฏูุฉ
            - ุฃุนุฏ ุชุดุบูู ุงูุชุทุจูู ุฅุฐุง ุงูุชูุฃุช ุงูุฐุงูุฑุฉ
            """
        }
    ]
    
    for issue in issues:
        with st.expander(issue["title"]):
            st.markdown(f"**ุงููุดููุฉ:** {issue['problem']}")
            st.markdown(issue['solution'])
    
    # ูุตุงุฆุญ ูุชูุฏูุฉ
    st.subheader("๐ก ูุตุงุฆุญ ููุงุณุชุฎุฏุงู ุงููุชูุฏู")
    
    with st.expander("๐ฏ ุชุญุณูู ุฌูุฏุฉ ุงูุฅุฌุงุจุงุช"):
        st.markdown("""
        **ูุชุญุณูู ุฏูุฉ ุงููุชุงุฆุฌ:**
        1. **ุงูุชุจ ุฃุณุฆูุฉ ูุญุฏุฏุฉ**: ุจุฏูุงู ูู "ุฃุฎุจุฑูู ุนู ุงูููุถูุน"ุ ุงุณุฃู "ูุง ูู ููุงุฆุฏ X ุงููุฐููุฑุฉุ"
        2. **ุงุณุชุฎุฏู ุงูุณูุงู**: ุงุฐูุฑ ุงุณู ุงููุซููุฉ ุฃู ุงููุณู ุฅุฐุง ููุช ุชุนุฑูู
        3. **ุฌุฑุจ ุตูุงุบุงุช ูุฎุชููุฉ**: ููุณ ุงููุนูู ุจูููุงุช ูุฎุชููุฉ ูุฏ ูุนุทู ูุชุงุฆุฌ ุฃูุถู
        4. **ุงุณุชุฎุฏู "ุงููุถุน ุงููุชูุฏู"**: ูุนุฑุถ ูุนูููุงุช ุฅุถุงููุฉ ุนู ุงูุฃุฏุงุก
        
        **ูุชุญุณูู ุดููููุฉ ุงููุชุงุฆุฌ:**
        - ุฒุฏ ุนูู ุงูุจุญุซ ููููุงุถูุน ุงููุนูุฏุฉ
        - ููู ุญุฏ ุงูุชุดุงุจู ููุจุญุซ ุงูุงุณุชูุดุงูู  
        - ุงุณุชุฎุฏู "ุจุญุซ ููุท" ูุงุณุชูุดุงู ุงููุตุงุฏุฑ ุฃููุงู
        """)
    
    with st.expander("โ๏ธ ุงูุฅุนุฏุงุฏุงุช ุงููุซูู ูุญุงูุงุช ูุฎุชููุฉ"):
        st.markdown("""
        **ููุจุญุซ ูู ูุซุงุฆู ุชูููุฉ:**
        - ุนูู ุงูุจุญุซ: 8-12
        - ุญุฏ ุงูุชุดุงุจู: 0.5-0.7
        - ุทูู ุงูุฅุฌุงุจุฉ: ููุตูุฉ
        
        **ููุจุญุซ ุงูุนุงู ูู ูุซุงุฆู ูุชููุนุฉ:**
        - ุนูู ุงูุจุญุซ: 5-8  
        - ุญุฏ ุงูุชุดุงุจู: 0.3-0.5
        - ุทูู ุงูุฅุฌุงุจุฉ: ูุชูุณุทุฉ
        
        **ููุจุญุซ ุงูุณุฑูุน ุนู ูุนูููุฉ ูุญุฏุฏุฉ:**
        - ุนูู ุงูุจุญุซ: 3-5
        - ุญุฏ ุงูุชุดุงุจู: 0.6-0.8
        - ุทูู ุงูุฅุฌุงุจุฉ: ูุตูุฑุฉ
        """)
    
    # ูุนูููุงุช ุชูููุฉ
    st.subheader("๐ฌ ูุนูููุงุช ุชูููุฉ ูุชูุฏูุฉ")
    
    with st.expander("๐๏ธ ุจููุฉ ุงููุธุงู"):
        st.markdown("""
        **ุงูููููุงุช ุงูุฑุฆูุณูุฉ:**
        - **Sentence Transformers**: ุชุญููู ุงููุตูุต ููููุชูุฑุงุช ุฏูุงููุฉ
        - **ChromaDB**: ูุงุนุฏุฉ ุจูุงูุงุช ูููุชูุฑุงุช ุนุงููุฉ ุงูุฃุฏุงุก
        - **FAISS**: ููุฑุณุฉ ูุจุญุซ ุณุฑูุน ูู ุงููููุชูุฑุงุช
        - **NLTK**: ูุนุงูุฌุฉ ูุชูุฏูุฉ ููุบุฉ ุงูุทุจูุนูุฉ
        
        **ุชุฏูู ุงููุนุงูุฌุฉ:**
        1. ุงุณุชุฎุฑุงุฌ ุงููุต ูู ุงููููุงุช
        2. ุชูุธูู ูุชุญุณูู ุงููุต ุงูุนุฑุจู
        3. ุชูุณูู ุฐูู ูููุต (Intelligent Chunking)
        4. ุชุดููุฑ ุงููุทุน ููููุชูุฑุงุช
        5. ููุฑุณุฉ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
        6. ุงูุจุญุซ ุงูุฏูุงูู ุนูุฏ ุงูุงุณุชุนูุงู
        7. ุฅุนุงุฏุฉ ุชุฑุชูุจ ุงููุชุงุฆุฌ
        8. ุชูููุฏ ุงูุฅุฌุงุจุฉ ุจุงูุณูุงู
        """)
    
    with st.expander("๐ ููุงููุณ ุงูุฃุฏุงุก"):
        st.markdown("""
        **ูุคุดุฑุงุช ุงูุฌูุฏุฉ:**
        - **ุฏุฑุฌุฉ ุงูุชุดุงุจู**: 0.8+ ููุชุงุฒุ 0.6-0.8 ุฌูุฏุ 0.4-0.6 ููุจูู
        - **ููุช ุงูุงุณุชุฌุงุจุฉ**: <2 ุซุงููุฉ ุณุฑูุนุ 2-5 ููุจููุ >5 ุจุทูุก
        - **ุนุฏุฏ ุงููุตุงุฏุฑ**: 3-5 ููุฃุณุฆูุฉ ุงูุจุณูุทุฉุ 5-10 ูููุนูุฏุฉ
        
        **ุนูุงูู ุชุคุซุฑ ุนูู ุงูุฃุฏุงุก:**
        - ุญุฌู ุงููุซุงุฆู ูุนุฏุฏูุง
        - ุชุนููุฏ ุงูุงุณุชุนูุงู
        - ุฅุนุฏุงุฏุงุช ุงูุจุญุซ
        - ููุฉ ุงูุงุชุตุงู ุจุงูุฅูุชุฑูุช
        - ููุงุตูุงุช ุงูุฌูุงุฒ
        """)
    
    # ุฃุณุฆูุฉ ุดุงุฆุนุฉ
    st.subheader("โ ุฃุณุฆูุฉ ุดุงุฆุนุฉ")
    
    faqs = [
        {
            "q": "ูู ูุฏุนู ุงููุธุงู ุงููุบุฉ ุงูุนุฑุจูุฉ ุจุงููุงููุ",
            "a": "ูุนูุ ุงููุธุงู ููุญุณููู ุฎุตูุตุงู ููุนุฑุจูุฉ ูุน ูุนุงูุฌุฉ ูุชูุฏูุฉ ูููุตูุต ุงูุนุฑุจูุฉ ุชุดูู ุฅุฒุงูุฉ ุงูุชุดููู ูุชูุญูุฏ ุงูุฃุญุฑู ูุชูุณูู ุฐูู ููุฌูู."
        },
        {
            "q": "ูุง ูู ุงูุญุฏ ุงูุฃูุตู ูุญุฌู ุงููููุงุชุ", 
            "a": "ูุง ููุฌุฏ ุญุฏ ุตุงุฑูุ ููู ููุฃุฏุงุก ุงูุฃูุซู ููุตุญ ุจูููุงุช ุฃูู ูู 50MB. ุงููููุงุช ุงููุจูุฑุฉ ูุฏ ุชุญุชุงุฌ ููุช ูุนุงูุฌุฉ ุฃุทูู."
        },
        {
            "q": "ูู ุชูุญูุธ ุจูุงูุงุชู ูู ุงูุณูุฑูุฑุ",
            "a": "ูุงุ ุฌููุน ุงูุจูุงูุงุช ูุญููุฉ ูู ุฌูุณุชู. ุนูุฏ ุฅุบูุงู ุงูุชุทุจูู ุชูุญุฐู ุฌููุน ุงูุจูุงูุงุช. ููุตุญ ุจุชุตุฏูุฑ ุงููุญุงุฏุซุงุช ุงููููุฉ."
        },
        {
            "q": "ููุงุฐุง ุจุนุถ ุฅุฌุงุจุงุช GPT-4 ุฃูุถู ูู Groqุ",
            "a": "ูู ูููุฐุฌ ูู ููุงุท ููุฉ. GPT-4 ุฃูุถู ูู ุงูุชุญููู ุงููุนูุฏุ Groq ุฃุณุฑุน ูุฃูุซุฑ ููุงุกุฉ. ุฌุฑุจ ููุงููุง ูุงุฎุชุฑ ุงูููุงุณุจ."
        },
        {
            "q": "ููู ุฃุญุณู ุฏูุฉ ุงูุจุญุซ ูู ูุซุงุฆููุ",
            "a": "ุงุณุชุฎุฏู ูุตูุต ูุงุถุญุฉุ ุงูุชุจ ุฃุณุฆูุฉ ูุญุฏุฏุฉุ ูุงุณุชุฎุฏู ูููุงุช ููุชุงุญูุฉ ูู ุงููุต ุงูุฃุตูู. ุชุฃูุฏ ูู ูุนุงูุฌุฉ ุงููุต ุจุดูู ุตุญูุญ."
        }
    ]
    
    for faq in faqs:
        with st.expander(f"โ {faq['q']}"):
            st.markdown(faq['a'])
    
    # ุงูุฏุนู ูุงููุณุงููุฉ
    st.subheader("๐ค ุงูุฏุนู ูุงููุณุงููุฉ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **ุทูุจ ุงููุณุงุนุฏุฉ:**
        - ุตู ูุดููุชู ุจุงูุชูุตูู
        - ุฃุฑูู ููุทุงุช ุดุงุดุฉ
        - ุงุฐูุฑ ููุน ุงููููุงุช ุงููุณุชุฎุฏูุฉ
        - ุญุฏุฏ ุฑุณุงุฆู ุงูุฎุทุฃ ุจุฏูุฉ
        """)
    
    with col2:
        st.markdown("""
        **ุงููุณุงููุฉ ูู ุงูุชุทููุฑ:**
        - ุงูุชุฑุญ ููุฒุงุช ุฌุฏูุฏุฉ
        - ุฃุจูุบ ุนู ุงูุฃุฎุทุงุก
        - ุดุงุฑู ุชุฌุฑุจุชู ูู ุงูุงุณุชุฎุฏุงู
        - ูุฏู ุชุญุณููุงุช ุนูู ุงูููุฏ
        """)

# ุฏูุงู ุงููุณุงุนุฏุฉ
def generate_comprehensive_report():
    """ุชูููุฏ ุชูุฑูุฑ ุดุงูู"""
    report = {
        'metadata': {
            'generated_at': datetime.now().isoformat(),
            'system_version': '2.0',
            'report_type': 'comprehensive'
        },
        'system_stats': st.session_state.processing_stats,
        'documents': [
            {
                'name': doc['name'],
                'type': doc.get('type', 'unknown'),
                'word_count': doc.get('word_count', 0),
                'processed': doc.get('processed', False),
                'timestamp': doc['timestamp']
            }
            for doc in st.session_state.documents
        ],
        'conversations': st.session_state.conversation_manager.conversations,
        'conversation_stats': st.session_state.conversation_manager.get_statistics(),
        'vector_store_stats': st.session_state.vector_store.get_stats(),
        'settings': st.session_state.get('advanced_settings', {})
    }
    
    return json.dumps(report, ensure_ascii=False, indent=2)

def export_stats_csv():
    """ุชุตุฏูุฑ ุงูุฅุญุตุงุฆูุงุช ูู CSV"""
    data = []
    
    # ุฅุญุตุงุฆูุงุช ุงููุซุงุฆู
    for doc in st.session_state.documents:
        data.append({
            'ุงูููุน': 'ูุซููุฉ',
            'ุงูุงุณู': doc['name'],
            'ุงููููุฉ': doc.get('word_count', 0),
            'ุงููุญุฏุฉ': 'ูููุฉ',
            'ุงูุชุงุฑูุฎ': doc['timestamp'][:10]
        })
    
    # ุฅุญุตุงุฆูุงุช ุงููุญุงุฏุซุงุช  
    for conv in st.session_state.conversation_manager.conversations:
        data.append({
            'ุงูููุน': 'ูุญุงุฏุซุฉ',
            'ุงูุงุณู': conv['query'][:50] + '...',
            'ุงููููุฉ': len(conv.get('sources', [])),
            'ุงููุญุฏุฉ': 'ูุตุฏุฑ',
            'ุงูุชุงุฑูุฎ': conv['timestamp'][:10]
        })
    
    df = pd.DataFrame(data)
    return df.to_csv(index=False, encoding='utf-8-sig')

def delete_document(doc_name: str):
    """ุญุฐู ูุซููุฉ"""
    st.session_state.documents = [
        doc for doc in st.session_state.documents 
        if doc['name'] != doc_name
    ]
    st.success(f"ุชู ุญุฐู {doc_name}")
    st.rerun()

def reprocess_all_documents():
    """ุฅุนุงุฏุฉ ูุนุงูุฌุฉ ุฌููุน ุงููุซุงุฆู"""
    if st.session_state.documents:
        with st.spinner("ุฅุนุงุฏุฉ ูุนุงูุฌุฉ ุฌููุน ุงููุซุงุฆู..."):
            # ุฅุนุงุฏุฉ ุชุนููู ุญุงูุฉ ุงููุนุงูุฌุฉ
            for doc in st.session_state.documents:
                doc['processed'] = False
            
            # ุฅูุดุงุก ููุฑุณ ุฌุฏูุฏ
            create_search_index()
        
        st.success("ุชู ุฅุนุงุฏุฉ ูุนุงูุฌุฉ ุฌููุน ุงููุซุงุฆู!")

def show_detailed_stats():
    """ุนุฑุถ ุฅุญุตุงุฆูุงุช ููุตูุฉ"""
    st.subheader("๐ ุฅุญุตุงุฆูุงุช ููุตูุฉ")
    
    # ุฅุญุตุงุฆูุงุช ุงููุซุงุฆู
    total_words = sum(doc.get('word_count', 0) for doc in st.session_state.documents)
    total_chars = sum(doc.get('char_count', 0) for doc in st.session_state.documents)
    processed_docs = sum(1 for doc in st.session_state.documents if doc.get('processed', False))
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ุฅุฌูุงูู ุงููููุงุช", f"{total_words:,}")
    with col2:
        st.metric("ุฅุฌูุงูู ุงูุฃุญุฑู", f"{total_chars:,}")
    with col3:
        st.metric("ุงููุซุงุฆู ุงููุนุงูุฌุฉ", f"{processed_docs}/{len(st.session_state.documents)}")
    with col4:
        avg_words = total_words / len(st.session_state.documents) if st.session_state.documents else 0
        st.metric("ูุชูุณุท ุงููููุงุช", f"{avg_words:.0f}")

def export_documents():
    """ุชุตุฏูุฑ ุงููุซุงุฆู"""
    export_data = {
        'export_info': {
            'timestamp': datetime.now().isoformat(),
            'total_documents': len(st.session_state.documents),
            'version': '2.0'
        },
        'documents': st.session_state.documents
    }
    
    json_data = json.dumps(export_data, ensure_ascii=False, indent=2)
    
    st.download_button(
        "๐พ ุชุญููู ุงููุซุงุฆู (JSON)",
        data=json_data,
        file_name=f"rag_documents_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )

# ุชุดุบูู ุงูุชุทุจูู
if __name__ == "__main__":
    main()format()
               ]
                for chunk in chunks
            ]
            
            # ุชุดููุฑ ุงููุตูุต
            embeddings = self.embedding_model.encode(texts)
            
            if len(embeddings) == 0:
                return False
            
            # ุฅุถุงูุฉ ูููุฌููุนุฉ
            self.collection.add(
                embeddings=embeddings.tolist(),
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            return True
            
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ุฅุถุงูุฉ ุงููุซุงุฆู: {e}")
            return False
    
    def search(self, query: str, k: int = 5, min_score: float = 0.3) -> List[Dict]:
        """ุงูุจุญุซ ูู ุงููุซุงุฆู"""
        if not self.collection:
            return []
        
        try:
            # ุชุดููุฑ ุงูุงุณุชุนูุงู
            query_embedding = self.embedding_model.encode([query])
            
            if len(query_embedding) == 0:
                return []
            
            # ุงูุจุญุซ
            results = self.collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=k,
                include=['documents', 'metadatas', 'distances']
            )
            
            # ุชูุณูู ุงููุชุงุฆุฌ
            formatted_results = []
            
            for i in range(len(results['ids'][0])):
                # ุชุญููู ุงููุณุงูุฉ ุฅูู ูุชูุฌุฉ ุชุดุงุจู
                distance = results['distances'][0][i]
                similarity_score = 1 - distance  # ูููุง ููุช ุงููุณุงูุฉุ ุฒุงุฏ ุงูุชุดุงุจู
                
                if similarity_score >= min_score:
                    formatted_results.append({
                        'id': results['ids'][0][i],
                        'text': results['documents'][0][i],
                        'metadata': results['metadatas'][0][i],
                        'score': similarity_score,
                        'distance': distance
                    })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ุงูุจุญุซ: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """ุฅุญุตุงุฆูุงุช ูุงุนุฏุฉ ุงูุจูุงูุงุช"""
        if not self.collection:
            return {}
        
        try:
            count = self.collection.count()
            return {
                'total_documents': count,
                'collection_name': self.collection_name,
                'embedding_dimension': self.embedding_model.dimension
            }
        except:
            return {}

class AdvancedAPIClient:
    """ุนููู API ูุชูุฏู ูุน ุฅุนุงุฏุฉ ุงููุญุงููุฉ ูุงูุชุฎุฒูู ุงููุคูุช"""
    
    def __init__(self):
        self.api_key = None
        self.provider = None
        self.base_url = None
        self.model = None
        self.session = requests.Session()
        self.cache = {}  # ุชุฎุฒูู ูุคูุช ุจุณูุท
        self.rate_limit_delay = 1.0
    
    def setup(self, provider: str, api_key: str) -> bool:
        """ุฅุนุฏุงุฏ ููุฏู ุงูุฎุฏูุฉ"""
        try:
            self.api_key = api_key.strip()
            self.provider = provider.lower()
            
            if self.provider == "openai":
                self.base_url = "https://api.openai.com/v1/chat/completions"
                self.model = "gpt-4o-mini"
            elif self.provider == "groq":
                self.base_url = "https://api.groq.com/openai/v1/chat/completions"
                self.model = "llama-3.1-70b-versatile"
            else:
                return False
            
            # ุฅุนุฏุงุฏ headers ููุฌูุณุฉ
            self.session.headers.update({
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            })
            
            return self._test_connection()
            
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ุงูุฅุนุฏุงุฏ: {e}")
            return False
    
    def _test_connection(self) -> bool:
        """ุงุฎุชุจุงุฑ ุงูุงุชุตุงู"""
        try:
            response = self.generate_response(
                query="ูุฑุญุจุง",
                context="ุงุฎุชุจุงุฑ",
                max_tokens=10
            )
            return not response.startswith("ุฎุทุฃ")
        except:
            return False
    
    def _make_request_with_retry(self, data: Dict, max_retries: int = 3) -> requests.Response:
        """ุทูุจ ูุน ุฅุนุงุฏุฉ ุงููุญุงููุฉ"""
        for attempt in range(max_retries):
            try:
                response = self.session.post(
                    self.base_url,
                    json=data,
                    timeout=30
                )
                
                if response.status_code == 429:  # Rate limit
                    wait_time = self.rate_limit_delay * (2 ** attempt)
                    time.sleep(wait_time)
                    continue
                
                return response
                
            except requests.exceptions.RequestException as e:
                if attempt == max_retries - 1:
                    raise e
                time.sleep(1 * (attempt + 1))
        
        raise requests.exceptions.RequestException("ูุดู ุจุนุฏ ุนุฏุฉ ูุญุงููุงุช")
    
    def generate_response(self, query: str, context: str, max_tokens: int = 800) -> str:
        """ุชูููุฏ ุงูุฅุฌุงุจุฉ ูุน ุงูุชุญุณููุงุช"""
        if not self.api_key or not self.base_url:
            return "ุฎุทุฃ: ูู ูุชู ุฅุนุฏุงุฏ API"
        
        # ุงูุชุญูู ูู ุงูุชุฎุฒูู ุงููุคูุช
        cache_key = hashlib.md5(f"{query}{context}".encode()).hexdigest()
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            system_prompt = """ุฃูุช ูุณุงุนุฏ ุฐูู ูุชุฎุตุต ูู ุชุญููู ุงููุซุงุฆู ูุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ.

ุงููุจุงุฏุฆ ุงูุชูุฌูููุฉ:
- ุงุฌุจ ุจุงููุบุฉ ุงูุนุฑุจูุฉ ุจูุถูุญ ูุชูุธูู
- ุงุณุชุฎุฏู ุงููุนูููุงุช ูู ุงูุณูุงู ุงูููุฏู ุจุฏูุฉ
- ุฅุฐุง ูู ุชุฌุฏ ุฅุฌุงุจุฉ ูุงููุฉุ ุงุฐูุฑ ุฐูู ุตุฑุงุญุฉ
- ูุธู ุฅุฌุงุจุชู ุจููุฑุงุช ูุงุถุญุฉ
- ุงูุชุจุณ ูู ุงููุตุงุฏุฑ ุนูุฏ ุงูุญุงุฌุฉ
- ุชุฌูุจ ุงูุชูุฑุงุฑ ูุงูุญุดู

ุฅุฐุง ูุงู ุงูุณุคุงู ูุชุทูุจ ุฑุฃูุงู ุฃู ุชุญูููุงูุ ูุฏู ููุธูุฑุงู ูุชูุงุฒูุงู ูุจููุงู ุนูู ุงููุนูููุงุช ุงููุชููุฑุฉ."""

            user_message = f"""ุงูุณูุงู ูุงููุตุงุฏุฑ:
{context}

ุงูุณุคุงู ุงููุทุฑูุญ:
{query}

ูุฑุฌู ุชูุฏูู ุฅุฌุงุจุฉ ุดุงููุฉ ููููุฏุฉ ูุจููุฉ ุนูู ุงููุนูููุงุช ุงููุชููุฑุฉ ุฃุนูุงู."""

            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                "max_tokens": max_tokens,
                "temperature": 0.3,
                "top_p": 0.9,
                "frequency_penalty": 0.1,
                "presence_penalty": 0.1
            }
            
            response = self._make_request_with_retry(data)
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    answer = result['choices'][0]['message']['content'].strip()
                    
                    # ุญูุธ ูู ุงูุชุฎุฒูู ุงููุคูุช
                    self.cache[cache_key] = answer
                    
                    # ุชูุธูู ุงูุชุฎุฒูู ุงููุคูุช ุฅุฐุง ุงูุชูุฃ
                    if len(self.cache) > 100:
                        # ุญุฐู ุฃูุฏู 20 ุนูุตุฑ
                        for _ in range(20):
                            self.cache.pop(next(iter(self.cache)))
                    
                    return answer
                else:
                    return "ุฎุทุฃ: ูู ูุชู ุงูุญุตูู ุนูู ุฅุฌุงุจุฉ ุตุงูุญุฉ"
            else:
                error_msg = f"HTTP {response.status_code}"
                try:
                    error_detail = response.json()
                    if 'error' in error_detail:
                        error_msg += f": {error_detail['error'].get('message', 'ุฎุทุฃ ุบูุฑ ูุนุฑูู')}"
                except:
                    pass
                return f"ุฎุทุฃ ูู API: {error_msg}"
                
        except Exception as e:
            return f"ุฎุทุฃ ุบูุฑ ูุชููุน: {str(e)}"

class AdvancedDocumentProcessor:
    """ูุนุงูุฌ ูุซุงุฆู ูุชูุฏู"""
    
    def __init__(self):
        self.supported_formats = {
            'text/plain': self._process_txt,
            'application/pdf': self._process_pdf,
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document': self._process_docx,
            'text/csv': self._process_csv
        }
    
    def process_file(self, file_content: bytes, file_type: str, file_name: str) -> Tuple[str, Dict]:
        """ูุนุงูุฌุฉ ุงูููู ุญุณุจ ููุนู"""
        try:
            if file_type in self.supported_formats:
                content, metadata = self.supported_formats[file_type](file_content, file_name)
            else:
                # ูุญุงููุฉ ูุนุงูุฌุฉ ููุต ุนุงุฏู
                content = file_content.decode('utf-8', errors='ignore')
                metadata = {'extracted_method': 'fallback_text'}
            
            # ุชูุธูู ูุชุญุณูู ุงููุต
            content = self._enhance_arabic_text(content)
            
            # ุฅุญุตุงุฆูุงุช
            metadata.update({
                'character_count': len(content),
                'word_count': len(content.split()),
                'processed_at': datetime.now().isoformat(),
                'file_type': file_type,
                'file_name': file_name
            })
            
            return content, metadata
            
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ูุนุงูุฌุฉ ุงูููู {file_name}: {e}")
            return "", {'error': str(e)}
    
    def _process_txt(self, content: bytes, file_name: str) -> Tuple[str, Dict]:
        """ูุนุงูุฌุฉ ุงููููุงุช ุงููุตูุฉ"""
        try:
            # ูุญุงููุฉ ุชุญุฏูุฏ ุงูุชุดููุฑ
            for encoding in ['utf-8', 'utf-16', 'cp1256', 'iso-8859-6']:
                try:
                    text = content.decode(encoding)
                    return text, {'encoding': encoding, 'method': 'text_decode'}
                except UnicodeDecodeError:
                    continue
            
            # ุงููุตูู ุงูุฃุฎูุฑ
            text = content.decode('utf-8', errors='ignore')
            return text, {'encoding': 'utf-8_ignore', 'method': 'text_fallback'}
            
        except Exception as e:
            return "", {'error': str(e)}
    
    def _process_pdf(self, content: bytes, file_name: str) -> Tuple[str, Dict]:
        """ูุนุงูุฌุฉ ูููุงุช PDF"""
        if not HAS_ADVANCED_LIBS:
            return self._process_txt(content, file_name)
        
        try:
            pdf_file = io.BytesIO(content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            text_parts = []
            page_count = len(pdf_reader.pages)
            
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text.strip():
                        text_parts.append(page_text)
                except Exception as e:
                    logger.warning(f"ุฎุทุฃ ูู ุงุณุชุฎุฑุงุฌ ุตูุญุฉ {page_num}: {e}")
                    continue
            
            full_text = '\n\n'.join(text_parts)
            
            metadata = {
                'page_count': page_count,
                'extraction_method': 'PyPDF2',
                'extracted_pages': len(text_parts)
            }
            
            return full_text, metadata
            
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ูุนุงูุฌุฉ PDF: {e}")
            return "", {'error': str(e)}
    
    def _process_docx(self, content: bytes, file_name: str) -> Tuple[str, Dict]:
        """ูุนุงูุฌุฉ ูููุงุช Word"""
        if not HAS_ADVANCED_LIBS:
            return self._process_txt(content, file_name)
        
        try:
            docx_file = io.BytesIO(content)
            text = docx2txt.process(docx_file)
            
            metadata = {
                'extraction_method': 'docx2txt'
            }
            
            return text, metadata
            
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ูุนุงูุฌุฉ DOCX: {e}")
            return "", {'error': str(e)}
    
    def _process_csv(self, content: bytes, file_name: str) -> Tuple[str, Dict]:
        """ูุนุงูุฌุฉ ูููุงุช CSV"""
        try:
            text_content = content.decode('utf-8', errors='ignore')
            lines = text_content.split('\n')
            
            # ุชุญููู CSV ูุชูุณูู ูุตู ูุงุจู ูููุฑุงุกุฉ
            readable_lines = []
            for i, line in enumerate(lines[:100]):  # ุฃูู 100 ุณุทุฑ
                if line.strip():
                    if i == 0:
                        readable_lines.append(f"ุงูุนูุงููู: {line}")
                    else:
                        readable_lines.append(f"ุงูุณุทุฑ {i}: {line}")
            
            full_text = '\n'.join(readable_lines)
            
            metadata = {
                'total_lines': len(lines),
                'processed_lines': len(readable_lines),
                'extraction_method': 'csv_text_conversion'
            }
            
            return full_text, metadata
            
        except Exception as e:
            return "", {'error': str(e)}
    
    def _enhance_arabic_text(self, text: str) -> str:
        """ุชุญุณูู ุงููุตูุต ุงูุนุฑุจูุฉ"""
        if not text:
            return ""
        
        # ุฅุฒุงูุฉ ุงูุชุดููู
        arabic_diacritics = 'ูููููููููฐูฑูฒูณูดูตูถูทูธูนูบูปูผูฝูพูฟฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺฺกฺขฺฃฺคฺฅฺฆฺงฺจฺฉฺชฺซฺฌฺญฺฎฺฏฺฐฺฑฺฒฺณฺดฺตฺถฺทฺธฺนฺฺปฺผฺฝฺพฺฟกขฃคฅฆงจฉชญ'
        
        for diacritic in arabic_diacritics:
            text = text.replace(diacritic, '')
        
        # ุชูุญูุฏ ุงูุฃุญุฑู ุงูุนุฑุจูุฉ ุงููุชุดุงุจูุฉ
        replacements = {
            'ุฃ': 'ุง', 'ุฅ': 'ุง', 'ุข': 'ุง', 'ุก': 'ุง',
            'ุฉ': 'ู', 'ู': 'ู', 'ู': 'ู'
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # ุชูุธูู ุงููุณุงูุงุช ูุงูุฃุณุทุฑ
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if len(line) > 5:  # ุชุฌุงูู ุงูุฃุณุทุฑ ุงููุตูุฑุฉ ุฌุฏุงู
                cleaned_lines.append(line)
        
        # ุฏูุฌ ุงูุฃุณุทุฑ ูุน ูุณุงูุงุช ููุงุณุจุฉ
        clean_text = ' '.join(cleaned_lines)
        
        # ุชูุธูู ุงููุณุงูุงุช ุงููุชุนุฏุฏุฉ
        import re
        clean_text = re.sub(r'\s+', ' ', clean_text)
        
        return clean_text.strip()
    
    def intelligent_chunk(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:
        """ุชูุณูู ุฐูู ูููุต ูุญุชุฑู ุญุฏูุฏ ุงูุฌูู ูุงูููุฑุงุช"""
        if not text:
            return []
        
        try:
            # ุชูุณูู ุฃููู ููุฌูู
            if HAS_ADVANCED_LIBS:
                try:
                    nltk.download('punkt', quiet=True)
                    sentences = sent_tokenize(text, language='arabic')
                except:
                    sentences = self._simple_sentence_split(text)
            else:
                sentences = self._simple_sentence_split(text)
            
            chunks = []
            current_chunk = ""
            current_word_count = 0
            chunk_id = 0
            
            for sentence in sentences:
                sentence_words = len(sentence.split())
                
                # ุฅุฐุง ุงูุฌููุฉ ูุญุฏูุง ุฃูุจุฑ ูู ุงูุญุฌู ุงููุทููุจ
                if sentence_words > chunk_size:
                    # ุญูุธ ุงููุทุนุฉ ุงูุญุงููุฉ ุฅุฐุง ูุงูุช ููุฌูุฏุฉ
                    if current_chunk:
                        chunks.append(self._create_chunk(current_chunk, chunk_id))
                        chunk_id += 1
                    
                    # ุชูุณูู ุงูุฌููุฉ ุงูุทูููุฉ
                    word_chunks = self._split_long_sentence(sentence, chunk_size)
                    for word_chunk in word_chunks:
                        chunks.append(self._create_chunk(word_chunk, chunk_id))
                        chunk_id += 1
                    
                    current_chunk = ""
                    current_word_count = 0
                    continue
                
                # ุฅุฐุง ุฅุถุงูุฉ ุงูุฌููุฉ ุณุชุชุฌุงูุฒ ุงูุญุฏ
                if current_word_count + sentence_words > chunk_size and current_chunk:
                    chunks.append(self._create_chunk(current_chunk, chunk_id))
                    chunk_id += 1
                    
                    # ุจุฏุงูุฉ ุฌุฏูุฏุฉ ูุน ุชุฏุงุฎู
                    if overlap > 0 and current_chunk:
                        words = current_chunk.split()
                        overlap_text = ' '.join(words[-overlap:]) if len(words) > overlap else current_chunk
                        current_chunk = overlap_text + " " + sentence
                        current_word_count = len(overlap_text.split()) + sentence_words
                    else:
                        current_chunk = sentence
                        current_word_count = sentence_words
                else:
                    # ุฅุถุงูุฉ ุงูุฌููุฉ ูููุทุนุฉ ุงูุญุงููุฉ
                    if current_chunk:
                        current_chunk += " " + sentence
                    else:
                        current_chunk = sentence
                    current_word_count += sentence_words
            
            # ุฅุถุงูุฉ ุงููุทุนุฉ ุงูุฃุฎูุฑุฉ
            if current_chunk.strip():
                chunks.append(self._create_chunk(current_chunk, chunk_id))
            
            return chunks
            
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ุงูุชูุณูู ุงูุฐูู: {e}")
            return self._fallback_chunk(text, chunk_size)
    
    def _simple_sentence_split(self, text: str) -> List[str]:
        """ุชูุณูู ุจุณูุท ููุฌูู"""
        import re
        # ุนูุงูุงุช ููุงูุฉ ุงูุฌููุฉ ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ
        sentence_endings = r'[.!?ุเฅค]\s+'
        sentences = re.split(sentence_endings, text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    
    def _split_long_sentence(self, sentence: str, max_size: int) -> List[str]:
        """ุชูุณูู ุงูุฌููุฉ ุงูุทูููุฉ"""
        words = sentence.split()
        chunks = []
        
        current_chunk = []
        for word in words:
            if len(current_chunk) >= max_size:
                chunks.append(' '.join(current_chunk))
                current_chunk = [word]
            else:
                current_chunk.append(word)
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def _create_chunk(self, text: str, chunk_id: int) -> Dict:
        """ุฅูุดุงุก ูุงุฆู ุงููุทุนุฉ"""
        return {
            'id': chunk_id,
            'text': text.strip(),
            'word_count': len(text.split()),
            'char_count': len(text),
            'created_at': datetime.now().isoformat()
        }
    
    def _fallback_chunk(self, text: str, chunk_size: int) -> List[Dict]:
        """ุชูุณูู ุงุญุชูุงุทู ุจุณูุท"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk_words = words[i:i + chunk_size]
            chunk_text = ' '.join(chunk_words)
            
            chunks.append({
                'id': len(chunks),
                'text': chunk_text,
                'word_count': len(chunk_words),
                'char_count': len(chunk_text),
                'created_at': datetime.now().isoformat()
            })
        
        return chunks

class ConversationManager:
    """ูุฏูุฑ ุงููุญุงุฏุซุงุช ุงููุชูุฏู"""
    
    def __init__(self):
        self.conversations = []
        self.current_session_id = self._generate_session_id()
        self.max_history = 50
    
    def _generate_session_id(self) -> str:
        """ุชูููุฏ ูุนุฑู ุฌูุณุฉ ูุฑูุฏ"""
        return f"session_{int(time.time())}_{hashlib.md5(str(time.time()).encode()).hexdigest()[:8]}"
    
    def add_conversation(self, query: str, response: str, sources: List[Dict], 
                        metadata: Optional[Dict] = None) -> None:
        """ุฅุถุงูุฉ ูุญุงุฏุซุฉ ุฌุฏูุฏุฉ"""
        conversation = {
            'id': len(self.conversations),
            'session_id': self.current_session_id,
            'timestamp': datetime.now().isoformat(),
            'query': query,
            'response': response,
            'sources': sources,
            'metadata': metadata or {},
            'feedback': None,
            'response_time': metadata.get('response_time', 0) if metadata else 0
        }
        
        self.conversations.append(conversation)
        
        # ุชูุธูู ุงูุชุงุฑูุฎ ุงููุฏูู
        if len(self.conversations) > self.max_history:
            self.conversations = self.conversations[-self.max_history:]
    
    def get_recent_conversations(self, limit: int = 10) -> List[Dict]:
        """ุงูุญุตูู ุนูู ุฃุญุฏุซ ุงููุญุงุฏุซุงุช"""
        return list(reversed(self.conversations[-limit:]))
    
    def get_conversation_by_id(self, conv_id: int) -> Optional[Dict]:
        """ุงูุญุตูู ุนูู ูุญุงุฏุซุฉ ุจูุนุฑููุง"""
        for conv in self.conversations:
            if conv['id'] == conv_id:
                return conv
        return None
    
    def add_feedback(self, conv_id: int, feedback: Dict) -> bool:
        """ุฅุถุงูุฉ ุชูููู ููุญุงุฏุซุฉ"""
        conv = self.get_conversation_by_id(conv_id)
        if conv:
            conv['feedback'] = feedback
            return True
        return False
    
    def get_statistics(self) -> Dict:
        """ุฅุญุตุงุฆูุงุช ุงููุญุงุฏุซุงุช"""
        if not self.conversations:
            return {}
        
        total_conversations = len(self.conversations)
        avg_response_time = sum(c.get('response_time', 0) for c in self.conversations) / total_conversations
        
        # ุชุญููู ุงููุตุงุฏุฑ
        total_sources = sum(len(c.get('sources', [])) for c in self.conversations)
        avg_sources = total_sources / total_conversations if total_conversations > 0 else 0
        
        # ุชุญููู ุฃุทูุงู ุงูุงุณุชุนูุงูุงุช ูุงูุฑุฏูุฏ
        query_lengths = [len(c['query'].split()) for c in self.conversations]
        response_lengths = [len(c['response'].split()) for c in self.conversations]
        
        return {
            'total_conversations': total_conversations,
            'avg_response_time': round(avg_response_time, 2),
            'avg_sources_per_query': round(avg_sources, 1),
            'avg_query_length': round(sum(query_lengths) / len(query_lengths), 1),
            'avg_response_length': round(sum(response_lengths) / len(response_lengths), 1),
            'session_id': self.current_session_id
        }
    
    def export_conversations(self, format: str = 'json') -> str:
        """ุชุตุฏูุฑ ุงููุญุงุฏุซุงุช"""
        export_data = {
            'export_timestamp': datetime.now().isoformat(),
            'session_id': self.current_session_id,
            'total_conversations': len(self.conversations),
            'conversations': self.conversations,
            'statistics': self.get_statistics()
        }
        
        if format.lower() == 'json':
            return json.dumps(export_data, ensure_ascii=False, indent=2)
        else:
            # ุชุญููู ูู CSV
            df = pd.DataFrame([
                {
                    'timestamp': c['timestamp'],
                    'query': c['query'],
                    'response': c['response'],
                    'sources_count': len(c.get('sources', [])),
                    'response_time': c.get('response_time', 0)
                }
                for c in self.conversations
            ])
            return df.to_csv(index=False, encoding='utf-8-sig')

# ======================== ุชููุฆุฉ ุงููุธุงู ========================

def init_session_state():
    """ุชููุฆุฉ ูุชุบูุฑุงุช ุงูุฌูุณุฉ ุงููุชูุฏูุฉ"""
    if 'documents' not in st.session_state:
        st.session_state.documents = []
    
    if 'vector_store' not in st.session_state:
        st.session_state.vector_store = ChromaVectorStore()
    
    if 'api_client' not in st.session_state:
        st.session_state.api_client = AdvancedAPIClient()
    
    if 'doc_processor' not in st.session_state:
        st.session_state.doc_processor = AdvancedDocumentProcessor()
    
    if 'conversation_manager' not in st.session_state:
        st.session_state.conversation_manager = ConversationManager()
    
    if 'system_ready' not in st.session_state:
        st.session_state.system_ready = False
    
    if 'processing_stats' not in st.session_state:
        st.session_state.processing_stats = {
            'documents_processed': 0,
            'chunks_created': 0,
            'queries_processed': 0,
            'average_response_time': 0,
            'last_update': datetime.now().isoformat()
        }

def check_system_requirements():
    """ูุญุต ูุชุทูุจุงุช ุงููุธุงู"""
    requirements_status = {
        'advanced_libraries': HAS_ADVANCED_LIBS,
        'vector_store': False,
        'api_connection': False,
        'documents_loaded': len(st.session_state.documents) > 0
    }
    
    # ูุญุต ูุฎุฒู ุงููููุชูุฑุงุช
    if st.session_state.vector_store.collection is not None:
        requirements_status['vector_store'] = True
    
    # ูุญุต ุงุชุตุงู API
    if st.session_state.api_client.api_key and st.session_state.api_client.provider:
        requirements_status['api_connection'] = True
    
    return requirements_status

# ======================== ุงููุงุฌูุฉ ุงูุฑุฆูุณูุฉ ========================

def render_header():
    """ุฑุณู ุงูููุฏุฑ ุงููุชูุฏู"""
    st.markdown("""
    <div class="main-header">
        <h1>๐ ูุธุงู RAG ุงููุชูุฏู</h1>
        <p>ุชูููุฉ ูุชุทูุฑุฉ ููุฐูุงุก ุงูุงุตุทูุงุนู ูุชุญููู ุงููุซุงุฆู</p>
        <p>ูุฏุนู ChromaDBุ Sentence Transformersุ ููุนุงูุฌุฉ ุงููุตูุต ุงููุชูุฏูุฉ</p>
    </div>
    """, unsafe_allow_html=True)

def render_sidebar():
    """ุงูุดุฑูุท ุงูุฌุงูุจู ุงููุชูุฏู"""
    with st.sidebar:
        st.header("โ๏ธ ููุญุฉ ุงูุชุญูู")
        
        # ุญุงูุฉ ุงููุธุงู
        st.subheader("๐ ุญุงูุฉ ุงููุธุงู")
        
        requirements = check_system_requirements()
        
        # ูุคุดุฑุงุช ุงูุญุงูุฉ
        status_html = """
        <div style='margin: 1rem 0;'>
        """
        
        for req, status in requirements.items():
            status_class = "status-online" if status else "status-offline"
            status_text = "ูุชุตู" if status else "ุบูุฑ ูุชุตู"
            
            req_names = {
                'advanced_libraries': 'ุงูููุชุจุงุช ุงููุชูุฏูุฉ',
                'vector_store': 'ูุงุนุฏุฉ ุงููููุชูุฑุงุช',
                'api_connection': 'ุงุชุตุงู AI',
                'documents_loaded': 'ุงููุซุงุฆู ุงููุญููุฉ'
            }
            
            status_html += f"""
            <div style='margin: 0.5rem 0;'>
                <span class="status-indicator {status_class}"></span>
                <strong>{req_names.get(req, req)}:</strong> {status_text}
            </div>
            """
        
        status_html += "</div>"
        st.markdown(status_html, unsafe_allow_html=True)
        
        # ุชุญุฐูุฑ ุฅุฐุง ูู ุชูู ุงูููุชุจุงุช ุงููุชูุฏูุฉ ูุชุงุญุฉ
        if not HAS_ADVANCED_LIBS:
            st.error("""
            โ๏ธ **ุงูููุชุจุงุช ุงููุชูุฏูุฉ ุบูุฑ ูุซุจุชุฉ**
            
            ููุงุณุชูุงุฏุฉ ูู ุฌููุน ุงูููุฒุงุชุ ูู ุจุชุซุจูุช:
            ```
            pip install sentence-transformers
            pip install chromadb
            pip install PyPDF2
            pip install python-docx
            pip install nltk
            ```
            """)
        
        st.divider()
        
        # ุฅุนุฏุงุฏ API
        st.subheader("๐ค ุฅุนุฏุงุฏ ุงูุฐูุงุก ุงูุงุตุทูุงุนู")
        
        provider = st.selectbox(
            "ููุฏู ุงูุฎุฏูุฉ:",
            ["ุงุฎุชุฑ...", "OpenAI", "Groq"],
            help="ุงุฎุชุฑ ููุฏู ุฎุฏูุฉ AI"
        )
        
        if provider != "ุงุฎุชุฑ...":
            api_key = st.text_input(
                f"ููุชุงุญ {provider}:",
                type="password",
                help=f"ุฃุฏุฎู ููุชุงุญ API"
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("๐ ุงุชุตุงู", type="primary"):
                    if api_key:
                        with st.spinner("ุฌุงุฑู ุงูุงุชุตุงู..."):
                            success = st.session_state.api_client.setup(provider, api_key)
                            
                            if success:
                                st.success(f"โ ูุชุตู ุจู {provider}")
                                st.rerun()
                            else:
                                st.error("โ ูุดู ุงูุงุชุตุงู")
                    else:
                        st.error("ูุฑุฌู ุฅุฏุฎุงู ููุชุงุญ API")
            
            with col2:
                if st.button("๐งช ุงุฎุชุจุงุฑ"):
                    if st.session_state.api_client.api_key:
                        with st.spinner("ุฌุงุฑู ุงูุงุฎุชุจุงุฑ..."):
                            response = st.session_state.api_client.generate_response(
                                "ูุฑุญุจุง", "ุงุฎุชุจุงุฑ ุงูุงุชุตุงู", max_tokens=20
                            )
                            
                            if not response.startswith("ุฎุทุฃ"):
                                st.success("โ ุงูุงุชุตุงู ูุนูู")
                            else:
                                st.error(f"โ {response}")
                    else:
                        st.warning("ูุฑุฌู ุงูุงุชุตุงู ุฃููุงู")
        
        st.divider()
        
        # ุฅุนุฏุงุฏุงุช ุงููุนุงูุฌุฉ
        st.subheader("โ๏ธ ุฅุนุฏุงุฏุงุช ุงููุนุงูุฌุฉ")
        
        chunk_size = st.slider("ุญุฌู ุงููุทุนุฉ", 200, 1000, 500)
        overlap_size = st.slider("ุงูุชุฏุงุฎู", 20, 200, 50)
        max_results = st.slider("ุฃูุตู ูุชุงุฆุฌ ุจุญุซ", 3, 15, 8)
        min_similarity = st.slider("ุญุฏ ุงูุชุดุงุจู ุงูุฃุฏูู", 0.1, 0.9, 0.4, 0.1)
        
        st.divider()
        
        # ุฅุญุตุงุฆูุงุช ุณุฑูุนุฉ
        st.subheader("๐ ุฅุญุตุงุฆูุงุช ุณุฑูุนุฉ")
        
        stats = st.session_state.processing_stats
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric("ูุซุงุฆู", stats['documents_processed'])
            st.metric("ุงุณุชุนูุงูุงุช", stats['queries_processed'])
        
        with col2:
            st.metric("ูุทุน", stats['chunks_created'])
            st.metric("ูุชูุณุท ุงูููุช", f"{stats['average_response_time']:.1f}s")
        
        st.divider()
        
        # ุฃุฏูุงุช ุงููุธุงู
        st.subheader("๐๏ธ ุฃุฏูุงุช ุงููุธุงู")
        
        if st.button("๐ ุฅุนุงุฏุฉ ุชุดุบูู"):
            st.cache_data.clear()
            st.cache_resource.clear()
            st.rerun()
        
        if st.button("๐๏ธ ูุณุญ ุฌููุน ุงูุจูุงูุงุช"):
            for key in list(st.session_state.keys()):
                if key not in ['vector_store', 'api_client', 'doc_processor']:
                    del st.session_state[key]
            init_session_state()
            st.success("ุชู ูุณุญ ุงูุจูุงูุงุช")
            st.rerun()
        
        # ูุนูููุงุช ุงููุณุฎุฉ
        st.markdown("---")
        st.caption("ูุธุงู RAG ุงููุชูุฏู v2.0")
        st.caption("ูุฏุนูู ุจู Streamlit & ChromaDB")

def main():
    """ุงูุฏุงูุฉ ุงูุฑุฆูุณูุฉ"""
    init_session_state()
    
    render_header()
    render_sidebar()
    
    # ุงูุชุญูู ูู ุงููุชุทูุจุงุช
    requirements = check_system_requirements()
    
    # ุงูุชุจููุจุงุช ุงูุฑุฆูุณูุฉ
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "๐ ุฅุฏุงุฑุฉ ุงููุซุงุฆู", 
        "๐ฌ ุงููุญุงุฏุซุฉ", 
        "๐ ุงูุชุญูููุงุช", 
        "โ๏ธ ุงูุฅุนุฏุงุฏุงุช ุงููุชูุฏูุฉ",
        "โ ุงููุณุงุนุฏุฉ"
    ])
    
    with tab1:
        render_document_management_tab()
    
    with tab2:
        render_chat_tab()
    
    with tab3:
        render_analytics_tab()
    
    with tab4:
        render_advanced_settings_tab()
    
    with tab5:
        render_help_tab()

def render_document_management_tab():
    """ุชุจููุจ ุฅุฏุงุฑุฉ ุงููุซุงุฆู ุงููุชูุฏู"""
    st.header("๐ ุฅุฏุงุฑุฉ ุงููุซุงุฆู ุงููุชูุฏูุฉ")
    
    # ุฑูุน ุงููููุงุช
    with st.container():
        st.subheader("๐ค ุฑูุน ููุนุงูุฌุฉ ุงููููุงุช")
        
        uploaded_files = st.file_uploader(
            "ุงุฎุชุฑ ุงููููุงุช:",
            accept_multiple_files=True,
            type=['txt', 'pdf', 'docx', 'csv'],
            help="ูุฏุนู ุงููุธุงู: TXT, PDF, DOCX, CSV"
        )
        
        if uploaded_files:
            st.subheader(f"๐ ุงููููุงุช ุงููุญุฏุฏุฉ ({len(uploaded_files)})")
            
            # ุนุฑุถ ุชูุงุตูู ุงููููุงุช
            for i, file in enumerate(uploaded_files):
                with st.expander(f"๐ {file.name} ({file.size/1024:.1f} KB)"):
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.write(f"**ุงูููุน:** {file.type}")
                    with col2:
                        st.write(f"**ุงูุญุฌู:** {file.size:,} ุจุงูุช")
                    with col3:
                        if st.button(f"๐ ูุนุงูุฌุฉ", key=f"process_{i}"):
                            process_single_file(file)
            
            # ูุนุงูุฌุฉ ุฌููุน ุงููููุงุช
            col1, col2, col3 = st.columns([2, 1, 1])
            
            with col1:
                if st.button("๐ ูุนุงูุฌุฉ ุฌููุน ุงููููุงุช", type="primary"):
                    process_all_files(uploaded_files)
            
            with col2:
                chunk_size = st.number_input("ุญุฌู ุงููุทุนุฉ", 200, 1000, 500, 50)
            
            with col3:
                overlap = st.number_input("ุงูุชุฏุงุฎู", 20, 200, 50, 10)
    
    # ุฅุฏุฎุงู ูุต ูุจุงุดุฑ
    with st.expander("โ๏ธ ุฅุฏุฎุงู ูุต ูุจุงุดุฑ", expanded=False):
        direct_text = st.text_area(
            "ุงููุต:",
            height=200,
            placeholder="ุงูุตู ุงููุต ููุง ูููุนุงูุฌุฉ ุงููุจุงุดุฑุฉ..."
        )
        
        col1, col2 = st.columns([1, 3])
        with col1:
            if st.button("โ ุฅุถุงูุฉ ุงููุต"):
                if direct_text.strip():
                    add_direct_text(direct_text)
        
        with col2:
            if direct_text:
                word_count = len(direct_text.split())
                char_count = len(direct_text)
                st.info(f"ุงููููุงุช: {word_count} | ุงูุฃุญุฑู: {char_count}")
    
    # ุนุฑุถ ุงููุซุงุฆู ุงููุญููุธุฉ
    if st.session_state.documents:
        st.divider()
        st.subheader(f"๐ ุงููุซุงุฆู ุงููุญููุธุฉ ({len(st.session_state.documents)})")
        
        # ุฌุฏูู ุงููุซุงุฆู ุงูููุตู
        docs_data = []
        for doc in st.session_state.documents:
            docs_data.append({
                'ุงูุงุณู': doc['name'],
                'ุงูููุน': doc.get('type', 'ุบูุฑ ูุญุฏุฏ'),
                'ุงููููุงุช': doc.get('word_count', 0),
                'ุงูุฃุญุฑู': doc.get('char_count', 0),
                'ุงูุญุงูุฉ': 'โ ูุนุงูุฌ' if doc.get('processed', False) else 'โณ ุบูุฑ ูุนุงูุฌ',
                'ุงูุชุงุฑูุฎ': doc['timestamp'][:16].replace('T', ' ')
            })
        
        df = pd.DataFrame(docs_data)
        st.dataframe(df, use_container_width=True)
        
        # ุฃุฏูุงุช ุงูุฅุฏุงุฑุฉ
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("๐ ุฅุนุงุฏุฉ ูุนุงูุฌุฉ ุงููู"):
                reprocess_all_documents()
        
        with col2:
            doc_to_delete = st.selectbox(
                "ุญุฐู ูุซููุฉ:",
                ["ุงุฎุชุฑ..."] + [doc['name'] for doc in st.session_state.documents]
            )
            if doc_to_delete != "ุงุฎุชุฑ..." and st.button("๐๏ธ ุญุฐู"):
                delete_document(doc_to_delete)
        
        with col3:
            if st.button("๐ ุฅุญุตุงุฆูุงุช ููุตูุฉ"):
                show_detailed_stats()
        
        with col4:
            if st.button("๐พ ุชุตุฏูุฑ ุงููุซุงุฆู"):
                export_documents()

def process_single_file(uploaded_file):
    """ูุนุงูุฌุฉ ููู ูุงุญุฏ"""
    with st.spinner(f"ุฌุงุฑู ูุนุงูุฌุฉ {uploaded_file.name}..."):
        try:
            # ูุฑุงุกุฉ ูุญุชูู ุงูููู
            file_content = uploaded_file.read()
            
            # ูุนุงูุฌุฉ ุงูููู
            text_content, metadata = st.session_state.doc_processor.process_file(
                file_content, uploaded_file.type, uploaded_file.name
            )
            
            if text_content and 'error' not in metadata:
                # ุญูุธ ุงููุซููุฉ
                doc_data = {
                    'id': len(st.session_state.documents),
                    'name': uploaded_file.name,
                    'type': uploaded_file.type,
                    'content': text_content,
                    'metadata': metadata,
                    'timestamp': datetime.now().isoformat(),
                    'word_count': metadata.get('word_count', 0),
                    'char_count': metadata.get('character_count', 0),
                    'processed': False
                }
                
                st.session_state.documents.append(doc_data)
                st.session_state.processing_stats['documents_processed'] += 1
                
                st.success(f"โ ุชู ูุนุงูุฌุฉ {uploaded_file.name} ุจูุฌุงุญ!")
                st.json(metadata)
            else:
                st.error(f"โ ุฎุทุฃ ูู ูุนุงูุฌุฉ {uploaded_file.name}: {metadata.get('error', 'ุฎุทุฃ ุบูุฑ ูุนุฑูู')}")
                
        except Exception as e:
            st.error(f"โ ุฎุทุฃ ูู ูุนุงูุฌุฉ {uploaded_file.name}: {str(e)}")

def process_all_files(uploaded_files):
    """ูุนุงูุฌุฉ ุฌููุน ุงููููุงุช"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    successful = 0
    failed = 0
    
    for i, uploaded_file in enumerate(uploaded_files):
        status_text.text(f"ูุนุงูุฌุฉ: {uploaded_file.name} ({i+1}/{len(uploaded_files)})")
        
        try:
            file_content = uploaded_file.read()
            text_content, metadata = st.session_state.doc_processor.process_file(
                file_content, uploaded_file.type, uploaded_file.name
            )
            
            if text_content and 'error' not in metadata:
                doc_data = {
                    'id': len(st.session_state.documents),
                    'name': uploaded_file.name,
                    'type': uploaded_file.type,
                    'content': text_content,
                    'metadata': metadata,
                    'timestamp': datetime.now().isoformat(),
                    'word_count': metadata.get('word_count', 0),
                    'char_count': metadata.get('character_count', 0),
                    'processed': False
                }
                
                st.session_state.documents.append(doc_data)
                successful += 1
            else:
                failed += 1
                st.error(f"ูุดู ูู ูุนุงูุฌุฉ {uploaded_file.name}")
                
        except Exception as e:
            failed += 1
            st.error(f"ุฎุทุฃ ูู {uploaded_file.name}: {str(e)}")
        
        progress_bar.progress((i + 1) / len(uploaded_files))
    
    status_text.empty()
    progress_bar.empty()
    
    st.session_state.processing_stats['documents_processed'] += successful
    
    if successful > 0:
        st.success(f"โ ุชู ูุนุงูุฌุฉ {successful} ููู ุจูุฌุงุญ!")
    if failed > 0:
        st.error(f"โ ูุดู ูู ูุนุงูุฌุฉ {failed} ููู")
    
    # ุฅูุดุงุก ุงูููุฑุณ ุชููุงุฆูุงู
    if successful > 0 and HAS_ADVANCED_LIBS:
        if st.button("๐ ุฅูุดุงุก ููุฑุณ ุงูุจุญุซ ุงูุขู"):
            create_search_index()

def add_direct_text(text_content: str):
    """ุฅุถุงูุฉ ูุต ูุจุงุดุฑ"""
    try:
        # ูุนุงูุฌุฉ ุงููุต
        clean_text = st.session_state.doc_processor._enhance_arabic_text(text_content)
        
        doc_data = {
            'id': len(st.session_state.documents),
            'name': f'ูุต_ูุจุงุดุฑ_{len(st.session_state.documents) + 1}',
            'type': 'ูุต ูุจุงุดุฑ',
            'content': clean_text,
            'metadata': {
                'source': 'direct_input',
                'word_count': len(clean_text.split()),
                'character_count': len(clean_text)
            },
            'timestamp': datetime.now().iso
