"""
Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ - ØªØ·Ø¨ÙŠÙ‚ Ù…Ø­Ø³Ù†
Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:
- ÙˆØ§Ø¬Ù‡Ø© Ø¹Ø±Ø¨ÙŠØ© ÙƒØ§Ù…Ù„Ø©
- Ø¯Ø¹Ù… OCR Ù„Ù„ØµÙˆØ±
- Ø¨Ø­Ø« ØµÙˆØªÙŠ
- ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
- Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ©
- ØªØµØ¯ÙŠØ± Ù…Ø­Ø³Ù†
- ØªØ­Ù„ÙŠÙ„ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
- Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø°ÙƒÙŠØ©
- ØªØªØ¨Ø¹ Ø§Ù„ØªÙ‚Ø¯Ù…
- ØªØ­Ù„ÙŠÙ„Ø§Øª Ø¨ØµØ±ÙŠØ©
- Ø­ÙØ¸ Ø§Ù„Ø¬Ù„Ø³Ø§Øª
- ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
"""
import os
import io
import time
import math
import tempfile
import traceback
import json
import base64
import hashlib
import pickle
import re
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime, timedelta
from collections import defaultdict, Counter

import streamlit as st
import pandas as pd
import numpy as np

# Ø§Ù„ÙˆØ§Ø±Ø¯Ø§Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
try:
    from googletrans import Translator
    TRANSLATOR_AVAILABLE = True
except Exception:
    TRANSLATOR_AVAILABLE = False

try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
except Exception:
    SPEECH_RECOGNITION_AVAILABLE = False

try:
    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except Exception:
    PLOTLY_AVAILABLE = False

try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except Exception:
    OCR_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except Exception:
    OPENCV_AVAILABLE = False

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except Exception:
    NETWORKX_AVAILABLE = False

try:
    from wordcloud import WordCloud
    import arabic_reshaper
    from bidi.algorithm import get_display
    WORDCLOUD_AVAILABLE = True
except Exception:
    WORDCLOUD_AVAILABLE = False

try:
    import textstat
    TEXTSTAT_AVAILABLE = True
except Exception:
    TEXTSTAT_AVAILABLE = False

# Ø§Ù„ÙˆØ§Ø±Ø¯Ø§Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©
try:
    from modules.file_handler import load_file
    from modules.utils import clean_text, chunk_text, sentences_from_text, tfidf_sentence_ranking
    from modules.ai_engine import RAGEngine, get_embedding as module_get_embedding, chat_with_ai as module_chat_with_ai
    from modules.exporter import export_txt, export_docx, export_pdf
    from modules.ui_components import render_upload_box, render_chat_ui
    MODULES_AVAILABLE = True
except Exception:
    MODULES_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

try:
    import faiss
except Exception:
    faiss = None

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

try:
    from PyPDF2 import PdfReader
except Exception:
    PdfReader = None

try:
    import docx
except Exception:
    docx = None

try:
    import openpyxl
except Exception:
    openpyxl = None

try:
    from pptx import Presentation
    from pptx.util import Inches, Pt
except Exception:
    Presentation = None

try:
    from docx import Document as DocxDoc
except Exception:
    DocxDoc = None

try:
    import whisper
    WHISPER_AVAILABLE = True
except Exception:
    WHISPER_AVAILABLE = False

# ---------------------- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„Ø£Ø³Ø±Ø§Ø± ----------------------
st.set_page_config(
    page_title="Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚", 
    page_icon="ğŸŒ", 
    layout="wide", 
    initial_sidebar_state="expanded"
)

# CSS Ù…Ø®ØµØµ Ù„Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
st.markdown("""
<style>
    .rtl-text {
        direction: rtl;
        text-align: right;
        font-family: 'Arial', sans-serif;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .success-card {
        background-color: #d4edda;
        border-color: #28a745;
    }
    .warning-card {
        background-color: #fff3cd;
        border-color: #ffc107;
    }
    .error-card {
        background-color: #f8d7da;
        border-color: #dc3545;
    }
    .sidebar .sidebar-content {
        background-image: linear-gradient(#2e7bcf,#2e7bcf);
    }
    .stProgress .st-bo {
        background-color: #1f77b4;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .user-message {
        background-color: #e3f2fd;
        margin-left: 2rem;
    }
    .assistant-message {
        background-color: #f5f5f5;
        margin-right: 2rem;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸŒ Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ â€” Ù…Ù†ØµØ© RAG Ø§Ù„Ù…ØªØ·ÙˆØ±Ø©")

# Ù‚Ø±Ø§Ø¡Ø© Ù…ÙØªØ§Ø­ OpenAI
OPENAI_API_KEY = None
try:
    OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")
except Exception:
    pass
if not OPENAI_API_KEY:
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙŠÙ„ OpenAI
openai_client = None
if OPENAI_API_KEY and OpenAI is not None:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception:
        openai_client = None

# Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ±Ø¬Ù…
translator = None
if TRANSLATOR_AVAILABLE:
    try:
        translator = Translator()
    except Exception:
        translator = None

# ---------------------- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ----------------------
def initialize_session_state():
    """ØªÙ‡ÙŠØ¦Ø© Ø­Ø§Ù„Ø© Ø§Ù„Ø¬Ù„Ø³Ø© Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©"""
    defaults = {
        "docs": [],
        "index_built": False,
        "faiss_index": None,
        "embeddings": None,
        "id_map": [],
        "model_name": "all-MiniLM-L6-v2",
        "chat_history": [],
        "query_count": 0,
        "processing_stats": {
            "total_docs": 0,
            "total_chars": 0,
            "total_words": 0,
            "avg_query_time": 0,
            "languages_detected": set(),
            "file_types": {},
            "sessions_count": 0,
            "total_queries": 0,
            "successful_queries": 0
        },
        "user_preferences": {
            "language": "ar",
            "theme": "light",
            "chunk_size": 300,
            "chunk_overlap": 50,
            "top_k_results": 5,
            "temperature": 0.7,
            "max_tokens": 1000
        },
        "current_session": {
            "start_time": datetime.now(),
            "queries": [],
            "docs_processed": 0
        },
        "advanced_analytics": {
            "query_patterns": defaultdict(int),
            "response_ratings": [],
            "most_accessed_docs": defaultdict(int),
            "search_history": [],
            "user_feedback": []
        },
        "cache": {},
        "bookmarks": [],
        "export_history": []
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

initialize_session_state()

# ---------------------- Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ----------------------
def app_log(msg: str):
    """ØªØ³Ø¬ÙŠÙ„ Ù…Ø­Ø³Ù† Ù…Ø¹ Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ ÙˆØ§Ù„Ù…Ø³ØªÙˆÙ‰"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    log_entry = {
        "timestamp": timestamp,
        "message": msg,
        "level": "INFO"
    }
    st.session_state.setdefault("_logs", []).append(log_entry)
    if len(st.session_state["_logs"]) > 100:  # Ø§Ù„Ø­Ø¯ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„Ø³Ø¬Ù„
        st.session_state["_logs"] = st.session_state["_logs"][-100:]

def generate_doc_hash(content: str) -> str:
    """ØªÙˆÙ„ÙŠØ¯ hash Ù„Ù„ÙˆØ«ÙŠÙ‚Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±"""
    return hashlib.md5(content.encode()).hexdigest()

def detect_language(text: str) -> str:
    """ÙƒØ´Ù Ù„ØºØ© Ø§Ù„Ù†Øµ Ù…Ø­Ø³Ù†"""
    if not translator:
        # ÙƒØ´Ù Ø¨Ø³ÙŠØ· Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        arabic_chars = sum(1 for c in text[:200] if '\u0600' <= c <= '\u06FF')
        if arabic_chars > 10:
            return "ar"
        return "en"
    
    try:
        result = translator.detect(text[:200])
        st.session_state.processing_stats["languages_detected"].add(result.lang)
        return result.lang
    except Exception:
        return "unknown"

def translate_text(text: str, target_lang: str = "ar") -> str:
    """ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª"""
    if not translator:
        return text
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
    cache_key = f"translate_{hash(text)}_{target_lang}"
    if cache_key in st.session_state.cache:
        return st.session_state.cache[cache_key]
    
    try:
        result = translator.translate(text, dest=target_lang)
        translated = result.text
        st.session_state.cache[cache_key] = translated
        return translated
    except Exception as e:
        app_log(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø©: {e}")
        return text

def extract_text_from_image(image) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OCR Ù…Ø­Ø³Ù†"""
    if not OCR_AVAILABLE:
        return "[OCR ØºÙŠØ± Ù…ØªØ§Ø­: ØªØ«Ø¨ÙŠØª pytesseract Ù…Ø·Ù„ÙˆØ¨]"
    
    try:
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù€ PIL
        if isinstance(image, bytes):
            img = Image.open(io.BytesIO(image))
        else:
            img = Image.open(image)
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù€ OCR
        if OPENCV_AVAILABLE:
            img_array = np.array(img)
            if len(img_array.shape) == 3:
                img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¨Ø§ÙŠÙ†
            img_array = cv2.equalizeHist(img_array)
            img = Image.fromarray(img_array)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ø¹ Ø¯Ø¹Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª
        text = pytesseract.image_to_string(img, lang='ara+eng+fra')
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    except Exception as e:
        app_log(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©: {e}")
        return f"[Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©: {e}]"

def process_audio_input(audio_file) -> str:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ"""
    if not WHISPER_AVAILABLE and not SPEECH_RECOGNITION_AVAILABLE:
        return "[Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª ØºÙŠØ± Ù…ØªØ§Ø­Ø©]"
    
    try:
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ Ù…Ø¤Ù‚ØªØ§Ù‹
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_file.write(audio_file.getvalue())
            tmp_path = tmp_file.name
        
        if WHISPER_AVAILABLE:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Whisper Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª
            model = whisper.load_model("base")
            result = model.transcribe(tmp_path, language="ar")
            text = result["text"]
        elif SPEECH_RECOGNITION_AVAILABLE:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… speech_recognition
            r = sr.Recognizer()
            with sr.AudioFile(tmp_path) as source:
                audio = r.record(source)
                text = r.recognize_google(audio, language='ar-SA')
        else:
            text = "[ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª]"
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
        os.unlink(tmp_path)
        
        return text
    except Exception as e:
        app_log(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª: {e}")
        return f"[Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª: {e}]"

def analyze_text_statistics(text: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
    stats = {
        "char_count": len(text),
        "word_count": len(text.split()),
        "sentence_count": len(fallback_sentences_from_text(text)),
        "paragraph_count": len([p for p in text.split('\n\n') if p.strip()]),
        "language": detect_language(text)
    }
    
    if TEXTSTAT_AVAILABLE:
        try:
            stats.update({
                "readability_score": textstat.flesch_reading_ease(text),
                "grade_level": textstat.flesch_kincaid_grade(text),
                "avg_sentence_length": textstat.avg_sentence_length(text),
                "difficult_words": textstat.difficult_words(text)
            })
        except Exception:
            pass
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹
    words = re.findall(r'\b\w+\b', text.lower())
    word_freq = Counter(words)
    stats["most_common_words"] = word_freq.most_common(10)
    
    return stats

def fallback_load_file_bytes(fileobj) -> Tuple[str, Dict[str, Any]]:
    """Ù‚Ø§Ø±Ø¦ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"""
    name = getattr(fileobj, "name", "Ù…Ø±ÙÙˆØ¹")
    ext = os.path.splitext(name)[1].lower()
    
    try:
        raw = fileobj.read()
    except Exception as e:
        return f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}]", {"error": str(e)}
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù„Ù
    file_size = len(raw)
    file_info = {
        "name": name,
        "extension": ext,
        "size_bytes": file_size,
        "size_mb": file_size / (1024 * 1024),
        "processed_at": datetime.now().isoformat()
    }
    
    st.session_state.processing_stats["file_types"][ext] = st.session_state.processing_stats["file_types"].get(ext, 0) + 1
    
    content = ""
    
    if ext == ".pdf":
        if PdfReader is None:
            content = "[Ù‚Ø±Ø§Ø¡Ø© PDF ØºÙŠØ± Ù…ØªØ§Ø­Ø©: ØªØ«Ø¨ÙŠØª PyPDF2 Ù…Ø·Ù„ÙˆØ¨]"
        else:
            try:
                reader = PdfReader(io.BytesIO(raw))
                pages = []
                file_info["pages"] = len(reader.pages)
                
                for i, page in enumerate(reader.pages):
                    txt = page.extract_text() or ""
                    if txt.strip():
                        pages.append(f"--- ØµÙØ­Ø© {i+1} ---\n{txt}")
                
                content = "\n\n".join(pages)
                file_info["extracted_pages"] = len(pages)
            except Exception as e:
                content = f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© PDF: {e}]"
                file_info["error"] = str(e)
    
    elif ext == ".docx":
        if docx is None:
            content = "[Ù‚Ø±Ø§Ø¡Ø© DOCX ØºÙŠØ± Ù…ØªØ§Ø­Ø©: ØªØ«Ø¨ÙŠØª python-docx Ù…Ø·Ù„ÙˆØ¨]"
        else:
            try:
                tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
                tmp.write(raw)
                tmp.flush()
                tmp.close()
                
                d = docx.Document(tmp.name)
                paragraphs = [p.text for p in d.paragraphs if p.text.strip()]
                content = "\n".join(paragraphs)
                
                file_info["paragraphs"] = len(paragraphs)
                
                try:
                    os.unlink(tmp.name)
                except Exception:
                    pass
                    
            except Exception as e:
                content = f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© DOCX: {e}]"
                file_info["error"] = str(e)
    
    elif ext in [".txt", ".md"]:
        try:
            content = raw.decode("utf-8", errors="ignore")
        except Exception as e:
            content = f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†Øµ: {e}]"
    
    elif ext in [".xls", ".xlsx", ".csv"]:
        try:
            if ext == ".csv":
                df = pd.read_csv(io.BytesIO(raw))
            else:
                if openpyxl is None:
                    content = "[Ù‚Ø±Ø§Ø¡Ø© Excel ØºÙŠØ± Ù…ØªØ§Ø­Ø©: ØªØ«Ø¨ÙŠØª openpyxl Ù…Ø·Ù„ÙˆØ¨]"
                else:
                    df = pd.read_excel(io.BytesIO(raw))
            
            if isinstance(df, pd.DataFrame):
                content = df.fillna("").to_string()
                file_info["rows"] = len(df)
                file_info["columns"] = len(df.columns)
                
        except Exception as e:
            content = f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„: {e}]"
            file_info["error"] = str(e)
    
    elif ext in [".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".gif"]:
        content = extract_text_from_image(raw)
        file_info["type"] = "image"
    
    elif ext == ".pptx":
        if Presentation is None:
            content = "[Ù‚Ø±Ø§Ø¡Ø© PowerPoint ØºÙŠØ± Ù…ØªØ§Ø­Ø©: ØªØ«Ø¨ÙŠØª python-pptx Ù…Ø·Ù„ÙˆØ¨]"
        else:
            try:
                tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pptx")
                tmp.write(raw)
                tmp.flush()
                tmp.close()
                
                prs = Presentation(tmp.name)
                slides_text = []
                
                for i, slide in enumerate(prs.slides):
                    slide_text = f"--- Ø´Ø±ÙŠØ­Ø© {i+1} ---\n"
                    for shape in slide.shapes:
                        if hasattr(shape, "text") and shape.text.strip():
                            slide_text += shape.text + "\n"
                    if slide_text.strip() != f"--- Ø´Ø±ÙŠØ­Ø© {i+1} ---":
                        slides_text.append(slide_text)
                
                content = "\n\n".join(slides_text)
                file_info["slides"] = len(prs.slides)
                file_info["extracted_slides"] = len(slides_text)
                
                try:
                    os.unlink(tmp.name)
                except Exception:
                    pass
                    
            except Exception as e:
                content = f"[Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© PowerPoint: {e}]"
                file_info["error"] = str(e)
    
    else:
        try:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ†Øµ
            content = raw.decode("utf-8", errors="ignore")
        except Exception:
            content = "[Ù†ÙˆØ¹ Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… Ø£Ùˆ Ù…Ø­ØªÙˆÙ‰ Ø«Ù†Ø§Ø¦ÙŠ]"
    
    return content, file_info

def fallback_clean_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ù…Ø­Ø³Ù† Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    if not text:
        return ""
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø²Ø§Ø¦Ø¯ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    text = re.sub(r'[\u064B-\u0652]', '', text)
    
    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„ÙÙˆØ§ØµÙ„
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"[\r\n]+", "\n", text)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ© Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ù…Ø¹ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ù…Ù‡Ù…Ø©
    text = re.sub(r'[^\w\s\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFE70-\uFEFF\u0750-\u077FØŒØ›ØŸ!.()ØŒ]', ' ', text)
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def fallback_chunk_text(text: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø³ÙŠØ§Ù‚"""
    if not text:
        return []
    
    # ØªÙ‚Ø³ÙŠÙ… Ø¨Ø§Ù„Ø¬Ù…Ù„ Ø£ÙˆÙ„Ø§Ù‹
    sentences = fallback_sentences_from_text(text)
    
    if len(sentences) <= 3:
        return [text]
    
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence_words = len(sentence.split())
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¬Ù…Ù„Ø© Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ù‚Ø³Ù…Ù‡Ø§
        if sentence_words > chunk_size:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_length = 0
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
            words = sentence.split()
            for i in range(0, len(words), chunk_size - overlap):
                chunk_words = words[i:i + chunk_size]
                chunks.append(" ".join(chunk_words))
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ù…Ù„Ø© Ø³ØªØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
        elif current_length + sentence_words > chunk_size and current_chunk:
            chunks.append(" ".join(current_chunk))
            # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¨Ø¹Ø¶ Ø§Ù„Ø¬Ù…Ù„ Ù„Ù„ØªØ¯Ø§Ø®Ù„
            overlap_sentences = current_chunk[-min(2, len(current_chunk)):]
            current_chunk = overlap_sentences + [sentence]
            current_length = sum(len(s.split()) for s in current_chunk)
        else:
            current_chunk.append(sentence)
            current_length += sentence_words
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 20]

def fallback_sentences_from_text(text: str) -> List[str]:
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ù„Ø¬Ù…Ù„ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    if not text:
        return []
    
    try:
        import nltk
        nltk.download('punkt', quiet=True)
        from nltk.tokenize import sent_tokenize
        sentences = sent_tokenize(text)
        return [s.strip() for s in sentences if len(s.strip()) > 15]
    except Exception:
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù…Ø­Ø³Ù† Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        # Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        text = text.replace('ØŸ', 'ØŸ\n')
        text = text.replace('!', '!\n') 
        text = text.replace('.', '.\n')
        text = text.replace('Ø›', 'Ø›\n')
        
        sentences = [s.strip() for s in text.split('\n') if s.strip()]
        return [s for s in sentences if len(s.strip()) > 15]

def fallback_tfidf_sentence_ranking(document_texts: List[str], top_k_sentences_per_doc: int = 3):
    """ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©"""
try:
    from sklearn.feature_extraction.text import TfidfVectorizer

    all_sentences = []
    doc_mapping = []

    # ================================================
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø¨Ù„ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ FAISS/Chroma Index
    # ================================================

    processed_docs = []

    for i, doc in enumerate(document_texts):
        clean_doc = doc.strip().replace("\n", " ").replace("\r", "")
        doc_id = f"doc_{i}"
        processed_docs.append({
            "id": doc_id,
            "text": clean_doc
        })
        print(f"[RAG] Processed document {doc_id}")

except Exception as e:
    st.error(f"Error processing documents: {e}")
