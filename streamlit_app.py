import streamlit as st
import os
import tempfile
from pathlib import Path
import PyPDF2
import docx
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import openai
from typing import List, Tuple
import re
import json
from datetime import datetime

# ØªÙƒÙˆÙŠÙ† Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="ğŸŒ Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Ù…Ø®ØµØµ Ù„Ù„ØªØµÙ…ÙŠÙ…
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        text-align: center;
        color: white;
    }
    
    .upload-section {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    
    .question-section {
        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    
    .answer-section {
        background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    
    .stButton button {
        background: linear-gradient(135deg, #ff6b6b, #feca57);
        border: none;
        border-radius: 25px;
        padding: 0.5rem 2rem;
        color: white;
        font-weight: bold;
        transition: all 0.3s ease;
    }
    
    .stButton button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    }
    
    .document-card {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        margin: 0.5rem 0;
        border-right: 4px solid #667eea;
    }
    
    .rtl {
        direction: rtl;
        text-align: right;
    }
</style>
""", unsafe_allow_html=True)

class RAGSystem:
    def __init__(self):
        self.model = None
        self.index = None
        self.documents = []
        self.embeddings = None
        
    @st.cache_resource
    def load_model(_self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†"""
        try:
            model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            return model
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
            return None
    
    def extract_text_from_pdf(self, file) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù PDF"""
        try:
            pdf_reader = PyPDF2.PdfReader(file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù PDF: {str(e)}")
            return ""
    
    def extract_text_from_docx(self, file) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù DOCX"""
        try:
            doc = docx.Document(file)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù DOCX: {str(e)}")
            return ""
    
    def extract_text_from_txt(self, file) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù TXT"""
        try:
            return str(file.read(), "utf-8")
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù TXT: {str(e)}")
            return ""
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ ØµØºÙŠØ±Ø©"""
        if not text:
            return []
        
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)
        
        return chunks
    
    def process_documents(self, uploaded_files):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©"""
        self.documents = []
        
        for file in uploaded_files:
            try:
                if file.type == "application/pdf":
                    text = self.extract_text_from_pdf(file)
                elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    text = self.extract_text_from_docx(file)
                elif file.type == "text/plain":
                    text = self.extract_text_from_txt(file)
                else:
                    st.warning(f"Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {file.name}")
                    continue
                
                if text:
                    chunks = self.chunk_text(text)
                    for i, chunk in enumerate(chunks):
                        self.documents.append({
                            'filename': file.name,
                            'chunk_id': i,
                            'content': chunk,
                            'type': file.type
                        })
                
            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù {file.name}: {str(e)}")
        
        if self.documents:
            self.create_embeddings()
    
    def create_embeddings(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³"""
        if not self.model:
            self.model = self.load_model()
        
        if not self.model or not self.documents:
            return
        
        try:
            texts = [doc['content'] for doc in self.documents]
            self.embeddings = self.model.encode(texts)
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
            faiss.normalize_L2(self.embeddings)
            self.index.add(self.embeddings)
            
            st.success(f"ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {len(self.documents)} Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ!")
            
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª: {str(e)}")
    
    def search_documents(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        if not self.model or not self.index or not self.documents:
            return []
        
        try:
            query_embedding = self.model.encode([query])
            faiss.normalize_L2(query_embedding)
            
            scores, indices = self.index.search(query_embedding, top_k)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.documents):
                    results.append((self.documents[idx], float(score)))
            
            return results
        
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {str(e)}")
            return []
    
    def generate_answer(self, query: str, context_docs: List[Tuple[dict, float]]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹"""
        if not context_docs:
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©."
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø³ÙŠØ§Ù‚
        context = "\n\n".join([doc[0]['content'] for doc in context_docs[:3]])
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø³ÙŠØ·Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚
        prompt = f"""
        Ø§Ù„Ø³ÙŠØ§Ù‚: {context}
        
        Ø§Ù„Ø³Ø¤Ø§Ù„: {query}
        
        Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§ØªØŒ """
        
        # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù„ØºÙˆÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ø£ÙØ¶Ù„
        # Ù„ÙƒÙ† Ø§Ù„Ø¢Ù† Ø³Ù†Ø³ØªØ®Ø¯Ù… Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø³ÙŠØ·Ø©
        
        answer = f"Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:\n\n"
        
        for i, (doc, score) in enumerate(context_docs[:2], 1):
            answer += f"ğŸ“„ Ù…Ù† Ø§Ù„Ù…Ù„Ù: {doc['filename']}\n"
            answer += f"Ø§Ù„Ù†Øµ Ø°Ùˆ Ø§Ù„ØµÙ„Ø©: {doc['content'][:300]}...\n"
            answer += f"Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚: {score:.2f}\n\n"
        
        return answer

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
@st.cache_resource
def get_rag_system():
    return RAGSystem()

# Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    st.markdown("""
    <div class="main-header">
        <h1>ğŸŒ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ RAG - Intelligent Retrieval & Generation</h1>
        <h3>ğŸš€ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª + ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Streamlit</h3>
    </div>
    """, unsafe_allow_html=True)
    
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
    rag_system = get_rag_system()
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        if rag_system.documents:
            st.success(f"ğŸ“š Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: {len(set([doc['filename'] for doc in rag_system.documents]))}")
            st.info(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù†Øµ: {len(rag_system.documents)}")
        
        st.header("ğŸ“‹ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…")
        st.markdown("""
        1. **Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª**: PDF, DOCX, TXT
        2. **Ø§Ù†ØªØ¸Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©**: Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ
        3. **Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ**: Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        4. **Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©**: Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
        """)
    
    # Ù‚Ø³Ù… Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
    st.markdown("""
    <div class="upload-section">
        <h2>ğŸ“¤ Ø§Ø±ÙØ¹ Ù…Ø³ØªÙ†Ø¯Ø§ØªÙƒ (PDF / DOCX / TXT)</h2>
    </div>
    """, unsafe_allow_html=True)
    
    uploaded_files = st.file_uploader(
        "Ø§Ø®ØªØ± Ø§Ù„Ù…Ù„ÙØ§Øª",
        accept_multiple_files=True,
        type=['pdf', 'docx', 'txt'],
        help="ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª Ù…Ù† Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ©"
    )
    
    if uploaded_files:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª..."):
            rag_system.process_documents(uploaded_files)
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©
        st.success("ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
        
        col1, col2, col3 = st.columns(3)
        for i, file in enumerate(uploaded_files):
            with [col1, col2, col3][i % 3]:
                st.markdown(f"""
                <div class="document-card">
                    <h4>ğŸ“„ {file.name}</h4>
                    <p>Ø§Ù„Ù†ÙˆØ¹: {file.type}</p>
                    <p>Ø§Ù„Ø­Ø¬Ù…: {file.size / 1024:.1f} KB</p>
                </div>
                """, unsafe_allow_html=True)
    
    # Ù‚Ø³Ù… Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
    if rag_system.documents:
        st.markdown("""
        <div class="question-section">
            <h2>ğŸ’¡ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§</h2>
        </div>
        """, unsafe_allow_html=True)
        
        query = st.text_area(
            "Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ:",
            placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ØŸ",
            height=100,
            help="ÙŠÙ…ÙƒÙ†Ùƒ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©"
        )
        
        col1, col2, col3 = st.columns([1, 1, 1])
        with col2:
            search_button = st.button("ğŸ” Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©", use_container_width=True)
        
        if search_button and query.strip():
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
                results = rag_system.search_documents(query)
                
                if results:
                    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                    answer = rag_system.generate_answer(query, results)
                    
                    st.markdown("""
                    <div class="answer-section">
                        <h2>âœ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©</h2>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    st.markdown(f'<div class="rtl">{answer}</div>', unsafe_allow_html=True)
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
                    with st.expander("ğŸ“š Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"):
                        for i, (doc, score) in enumerate(results, 1):
                            st.markdown(f"""
                            **ğŸ“„ Ø§Ù„Ù…Ø±Ø¬Ø¹ {i}:**
                            - Ø§Ù„Ù…Ù„Ù: {doc['filename']}
                            - Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚: {score:.3f}
                            - Ø§Ù„Ù†Øµ: {doc['content'][:200]}...
                            """)
                
                else:
                    st.warning("Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©.")
    
    else:
        st.info("ğŸ‘† Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø£ÙˆÙ„Ø§Ù‹ Ù„Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù…")
    
    # ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: #666;'>"
        "ğŸ¤– Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ - ØªÙ… ØªØ·ÙˆÙŠØ±Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Streamlit & AI"
        "</div>", 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
