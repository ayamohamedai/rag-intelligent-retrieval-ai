"""
Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ - Ù†Ø³Ø®Ø© ÙØ¹Ø§Ù„Ø© Ù…Ø¹ AI
ØªØ·Ø¨ÙŠÙ‚ ÙŠØ¹Ù…Ù„ Ù…Ø¹ OpenAI API Ø£Ùˆ Groq Ø£Ùˆ Ø£ÙŠ LLM Ø¢Ø®Ø±
"""

import streamlit as st
import pandas as pd
import numpy as np
import re
import json
import hashlib
from typing import List, Dict, Any, Optional
from datetime import datetime
import requests
import os
from collections import Counter
import math
import io
import base64
from sentence_transformers import SentenceTransformer
import faiss
import openai
from groq import Groq

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", 
    page_icon="ğŸ¤–", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Ù„Ù„ØªØµÙ…ÙŠÙ…
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 10px 30px rgba(0,0,0,0.2);
    }
    
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
        border: 1px solid #e0e0e0;
    }
    
    .user-message {
        background: #e3f2fd;
        margin-right: 20px;
    }
    
    .ai-message {
        background: #f3e5f5;
        margin-left: 20px;
    }
    
    .doc-chunk {
        background: #fff3e0;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        border-left: 4px solid #ff9800;
    }
    
    .metric-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        text-align: center;
        border: 1px solid #e0e0e0;
    }
    
    .rtl {
        direction: rtl;
        text-align: right;
    }
</style>
""", unsafe_allow_html=True)

# Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
st.markdown("""
<div class="main-header">
    <h1>ğŸ¤– Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚</h1>
    <p>ØªØ·Ø¨ÙŠÙ‚ ÙØ¹Ø§Ù„ Ù…Ø¹ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«Ø©</p>
    <p>ÙŠØ¯Ø¹Ù… OpenAIØŒ GroqØŒ ÙˆÙ…ÙˆØ¯ÙŠÙ„Ø§Øª Ø£Ø®Ø±Ù‰</p>
</div>
""", unsafe_allow_html=True)

# ======================== Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª ÙˆØ§Ù„ÙˆØ¸Ø§Ø¦Ù ========================

class DocumentProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù…Ø¹ Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø°ÙƒÙŠ"""
    
    def __init__(self):
        self.embeddings_model = None
        self.load_embeddings_model()
    
    def load_embeddings_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ±"""
        try:
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø±Ø¨ÙŠ
            self.embeddings_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            st.success("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ± Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            st.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ±: {e}")
            self.embeddings_model = None
    
    def clean_arabic_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙ‚Ø¯Ù…Ø©"""
        if not text:
            return ""
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        text = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù”Ù•Ù–Ù—Ù˜Ù™ÙšÙ›ÙœÙÙÙŸÙ°Ù±]', '', text)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        text = text.replace('Ø£', 'Ø§').replace('Ø¥', 'Ø§').replace('Ø¢', 'Ø§')
        text = text.replace('Ø©', 'Ù‡').replace('Ù‰', 'ÙŠ')
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø±Ù…ÙˆØ²
        text = re.sub(r'[^\w\s\u0600-\u06FF]', ' ', text)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù‚Ø·Ø¹ Ù…Ø¹ ØªØ¯Ø§Ø®Ù„"""
        if not text:
            return []
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø¬Ù…Ù„
        sentences = self.split_into_sentences(text)
        chunks = []
        current_chunk = ""
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence.split())
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¬Ù…Ù„Ø© Ø³ØªØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­
            if current_size + sentence_size > chunk_size and current_chunk:
                chunks.append({
                    'text': current_chunk.strip(),
                    'size': current_size,
                    'id': len(chunks)
                })
                
                # Ø¨Ø¯Ø§ÙŠØ© Ù‚Ø·Ø¹Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø§Ù„ØªØ¯Ø§Ø®Ù„
                overlap_words = current_chunk.split()[-overlap:] if current_chunk else []
                current_chunk = ' '.join(overlap_words) + ' ' + sentence
                current_size = len(overlap_words) + sentence_size
            else:
                current_chunk += ' ' + sentence
                current_size += sentence_size
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'size': current_size,
                'id': len(chunks)
            })
        
        return chunks
    
    def split_into_sentences(self, text: str) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø¬Ù…Ù„"""
        # Ø¹Ù„Ø§Ù…Ø§Øª Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©
        sentence_endings = r'[.!?ØŸà¥¤Û”]'
        sentences = re.split(sentence_endings, text)
        
        # ØªÙ†Ø¸ÙŠÙ ÙˆÙÙ„ØªØ±Ø© Ø§Ù„Ø¬Ù…Ù„
        clean_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                clean_sentences.append(sentence)
        
        return clean_sentences
    
    def generate_embeddings(self, chunks: List[Dict]) -> Optional[np.ndarray]:
        """ØªÙˆÙ„ÙŠØ¯ embeddings Ù„Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù†ØµÙŠØ©"""
        if not self.embeddings_model or not chunks:
            return None
        
        try:
            texts = [chunk['text'] for chunk in chunks]
            embeddings = self.embeddings_model.encode(texts, convert_to_numpy=True)
            return embeddings
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ embeddings: {e}")
            return None

class VectorStore:
    """Ù…Ø®Ø²Ù† Ø§Ù„ÙÙŠÙƒØªÙˆØ±Ø§Øª Ù…Ø¹ FAISS"""
    
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        self.index = None
        self.chunks = []
        self.embeddings = None
        self.setup_faiss()
    
    def setup_faiss(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ ÙÙ‡Ø±Ø³ FAISS"""
        try:
            self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product Ù„Ù„Ø´Ø¨Ù‡
            st.success("âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø« Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ FAISS: {e}")
            self.index = None
    
    def add_documents(self, chunks: List[Dict], embeddings: np.ndarray):
        """Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù„Ù„ÙÙ‡Ø±Ø³"""
        if self.index is None or embeddings is None:
            return False
        
        try:
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ÙÙŠÙƒØªÙˆØ±Ø§Øª
            normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
            
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù„ÙÙ‡Ø±Ø³
            self.index.add(normalized_embeddings.astype('float32'))
            self.chunks.extend(chunks)
            self.embeddings = normalized_embeddings
            
            return True
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚: {e}")
            return False
    
    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³"""
        if self.index is None or self.index.ntotal == 0:
            return []
        
        try:
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            query_norm = query_embedding / np.linalg.norm(query_embedding)
            query_norm = query_norm.reshape(1, -1).astype('float32')
            
            # Ø§Ù„Ø¨Ø­Ø«
            scores, indices = self.index.search(query_norm, min(k, self.index.ntotal))
            
            # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.chunks):
                    results.append({
                        'chunk': self.chunks[idx],
                        'score': float(score),
                        'index': int(idx)
                    })
            
            return results
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
            return []

class LLMInterface:
    """ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ©"""
    
    def __init__(self):
        self.client = None
        self.model_type = None
    
    def setup_openai(self, api_key: str):
        """Ø¥Ø¹Ø¯Ø§Ø¯ OpenAI"""
        try:
            openai.api_key = api_key
            self.client = openai
            self.model_type = "openai"
            return True
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ OpenAI: {e}")
            return False
    
    def setup_groq(self, api_key: str):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Groq"""
        try:
            self.client = Groq(api_key=api_key)
            self.model_type = "groq"
            return True
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Groq: {e}")
            return False
    
    def generate_response(self, prompt: str, context: str, model: str = "gpt-3.5-turbo") -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
        if not self.client:
            return "âŒ Ù„Ù… ÙŠØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ©"
        
        try:
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
            system_message = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©.
            Ù‚Ù… Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø· ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù….
            Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­."""
            
            user_message = f"""Ø§Ù„Ø³ÙŠØ§Ù‚: {context}
            
            Ø§Ù„Ø³Ø¤Ø§Ù„: {prompt}
            
            Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""
            
            if self.model_type == "openai":
                response = self.client.ChatCompletion.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_message},
                        {"role": "user", "content": user_message}
                    ],
                    max_tokens=500,
                    temperature=0.3
                )
                return response.choices[0].message.content
                
            elif self.model_type == "groq":
                response = self.client.chat.completions.create(
                    model="llama-3.1-70b-versatile",
                    messages=[
                        {"role": "system", "content": system_message},
                        {"role": "user", "content": user_message}
                    ],
                    max_tokens=500,
                    temperature=0.3
                )
                return response.choices[0].message.content
            
        except Exception as e:
            return f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {e}"
        
        return "âŒ Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…"

# ======================== ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ========================

def init_session_state():
    """ØªÙ‡ÙŠØ¦Ø© Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©"""
    if 'processor' not in st.session_state:
        st.session_state.processor = DocumentProcessor()
    
    if 'vector_store' not in st.session_state:
        st.session_state.vector_store = VectorStore()
    
    if 'llm' not in st.session_state:
        st.session_state.llm = LLMInterface()
    
    if 'documents' not in st.session_state:
        st.session_state.documents = []
    
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    if 'is_ready' not in st.session_state:
        st.session_state.is_ready = False

# ======================== Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ========================

def main():
    init_session_state()
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.header("ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ©
        st.subheader("ğŸ¤– Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª AI")
        
        llm_provider = st.selectbox(
            "Ø§Ø®ØªØ± Ù…Ù‚Ø¯Ù… Ø§Ù„Ø®Ø¯Ù…Ø©:",
            ["OpenAI", "Groq", "None"]
        )
        
        if llm_provider != "None":
            api_key = st.text_input(
                f"Ù…ÙØªØ§Ø­ {llm_provider} API:",
                type="password",
                help=f"Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ API Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ {llm_provider}"
            )
            
            if api_key and st.button(f"ğŸ”— Ø±Ø¨Ø· {llm_provider}"):
                with st.spinner(f"Ø¬Ø§Ø±ÙŠ Ø±Ø¨Ø· {llm_provider}..."):
                    if llm_provider == "OpenAI":
                        success = st.session_state.llm.setup_openai(api_key)
                    elif llm_provider == "Groq":
                        success = st.session_state.llm.setup_groq(api_key)
                    
                    if success:
                        st.success(f"âœ… ØªÙ… Ø±Ø¨Ø· {llm_provider} Ø¨Ù†Ø¬Ø§Ø­!")
                        st.session_state.is_ready = True
                    else:
                        st.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø±Ø¨Ø· {llm_provider}")
        
        st.divider()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        st.subheader("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        chunk_size = st.slider("Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„Ù†ØµÙŠØ©", 200, 1000, 500)
        overlap_size = st.slider("Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§Ø®Ù„", 20, 200, 50)
        similarity_threshold = st.slider("Ø­Ø¯ Ø§Ù„Ø´Ø¨Ù‡", 0.1, 1.0, 0.7)
        
        st.divider()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        st.subheader("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
        st.metric("Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­Ù…Ù„Ø©", len(st.session_state.documents))
        
        if st.session_state.vector_store.index:
            st.metric("Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…ÙÙ‡Ø±Ø³Ø©", st.session_state.vector_store.index.ntotal)
        
        st.metric("Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª", len(st.session_state.chat_history))
    
    # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    tab1, tab2, tab3 = st.tabs(["ğŸ“š Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚", "ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", "ğŸ“ˆ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª"])
    
    with tab1:
        st.header("ğŸ“š Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
        
        # Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        uploaded_files = st.file_uploader(
            "Ø§Ø®ØªØ± Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ø±ÙØ¹:",
            accept_multiple_files=True,
            type=['txt', 'pdf', 'docx', 'csv'],
            help="ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª"
        )
        
        # Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù…Ø¨Ø§Ø´Ø±
        with st.expander("âœï¸ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù…Ø¨Ø§Ø´Ø±"):
            direct_text = st.text_area(
                "Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§:",
                height=200,
                placeholder="Ø§ÙƒØªØ¨ Ø£Ùˆ Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ ØªØ­Ù„ÙŠÙ„Ù‡..."
            )
            
            if st.button("â• Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Øµ"):
                if direct_text.strip():
                    st.session_state.documents.append({
                        'name': f'Ù†Øµ_Ù…Ø¨Ø§Ø´Ø±_{len(st.session_state.documents) + 1}',
                        'content': direct_text,
                        'type': 'text',
                        'timestamp': datetime.now().isoformat()
                    })
                    st.success("âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Øµ!")
                    st.rerun()
                else:
                    st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ ØµØ§Ù„Ø­")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©
        if uploaded_files:
            st.subheader("ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©:")
            
            for uploaded_file in uploaded_files:
                with st.expander(f"ğŸ“„ {uploaded_file.name}"):
                    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
                    try:
                        if uploaded_file.type == "text/plain":
                            content = str(uploaded_file.read(), "utf-8")
                        else:
                            content = str(uploaded_file.read(), "utf-8", errors='ignore')
                        
                        # Ù…Ø¹Ø§ÙŠÙ†Ø©
                        preview = content[:300] + "..." if len(content) > 300 else content
                        st.text_area("Ù…Ø¹Ø§ÙŠÙ†Ø©:", preview, height=100)
                        
                        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Ø§Ù„Ø­Ø¬Ù…", f"{uploaded_file.size/1024:.1f} KB")
                        with col2:
                            st.metric("Ø§Ù„Ù†ÙˆØ¹", uploaded_file.type)
                        with col3:
                            st.metric("Ø§Ù„ÙƒÙ„Ù…Ø§Øª", len(content.split()))
                        
                        if st.button(f"ğŸ“¥ Ø­ÙØ¸ {uploaded_file.name}", key=f"save_{uploaded_file.name}"):
                            st.session_state.documents.append({
                                'name': uploaded_file.name,
                                'content': content,
                                'type': uploaded_file.type,
                                'size': uploaded_file.size,
                                'timestamp': datetime.now().isoformat()
                            })
                            st.success(f"âœ… ØªÙ… Ø­ÙØ¸ {uploaded_file.name}!")
                            st.rerun()
                            
                    except Exception as e:
                        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
        if st.session_state.documents:
            st.subheader(f"ğŸ“‹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© ({len(st.session_state.documents)})")
            
            # Ø¹Ø±Ø¶ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
            for i, doc in enumerate(st.session_state.documents):
                with st.expander(f"ğŸ“„ {doc['name']}"):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        preview = doc['content'][:200] + "..." if len(doc['content']) > 200 else doc['content']
                        st.text(preview)
                    
                    with col2:
                        st.metric("Ø§Ù„ÙƒÙ„Ù…Ø§Øª", len(doc['content'].split()))
                        if st.button("ğŸ—‘ï¸ Ø­Ø°Ù", key=f"del_{i}"):
                            st.session_state.documents.pop(i)
                            st.rerun()
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
            st.divider()
            
            if st.button("ğŸš€ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆÙÙ‡Ø±Ø³Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚", type="primary"):
                if not st.session_state.processor.embeddings_model:
                    st.error("âŒ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ± ØºÙŠØ± Ù…Ø­Ù…Ù„!")
                    return
                
                with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚..."):
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    all_chunks = []
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©
                    for i, doc in enumerate(st.session_state.documents):
                        status_text.text(f"Ù…Ø¹Ø§Ù„Ø¬Ø©: {doc['name']}")
                        
                        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
                        clean_text = st.session_state.processor.clean_arabic_text(doc['content'])
                        
                        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù‚Ø·Ø¹
                        chunks = st.session_state.processor.chunk_text(
                            clean_text, 
                            chunk_size=chunk_size, 
                            overlap=overlap_size
                        )
                        
                        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ù„ÙƒÙ„ Ù‚Ø·Ø¹Ø©
                        for chunk in chunks:
                            chunk['doc_name'] = doc['name']
                            chunk['doc_index'] = i
                        
                        all_chunks.extend(chunks)
                        progress_bar.progress((i + 1) / len(st.session_state.documents))
                    
                    # ØªÙˆÙ„ÙŠØ¯ embeddings
                    status_text.text("ğŸ”„ ØªÙˆÙ„ÙŠØ¯ embeddings...")
                    embeddings = st.session_state.processor.generate_embeddings(all_chunks)
                    
                    if embeddings is not None:
                        # Ø¥Ø¶Ø§ÙØ© Ù„Ù„ÙÙ‡Ø±Ø³
                        status_text.text("ğŸ”„ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«...")
                        success = st.session_state.vector_store.add_documents(all_chunks, embeddings)
                        
                        if success:
                            st.success(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {len(all_chunks)} Ù‚Ø·Ø¹Ø© Ù†ØµÙŠØ© Ù…Ù† {len(st.session_state.documents)} ÙˆØ«ÙŠÙ‚Ø©!")
                        else:
                            st.error("âŒ ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«")
                    else:
                        st.error("âŒ ÙØ´Ù„ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ embeddings")
                    
                    progress_bar.empty()
                    status_text.empty()
    
    with tab2:
        st.header("ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
        
        if not st.session_state.is_ready:
            st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© Ø£ÙˆÙ„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ")
            return
        
        if st.session_state.vector_store.index is None or st.session_state.vector_store.index.ntotal == 0:
            st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø£ÙˆÙ„Ø§Ù‹ Ù…Ù† ØªØ¨ÙˆÙŠØ¨ 'Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚'")
            return
        
        # Ø¹Ø±Ø¶ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        for chat in st.session_state.chat_history:
            # Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            st.markdown(f"""
            <div class="chat-message user-message rtl">
                <strong>ğŸ‘¤ Ø£Ù†Øª:</strong><br>
                {chat['question']}
            </div>
            """, unsafe_allow_html=True)
            
            # Ø±Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
            st.markdown(f"""
            <div class="chat-message ai-message rtl">
                <strong>ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:</strong><br>
                {chat['answer']}
            </div>
            """, unsafe_allow_html=True)
            
            # Ø§Ù„Ù…ØµØ§Ø¯Ø±
            if 'sources' in chat and chat['sources']:
                with st.expander("ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"):
                    for i, source in enumerate(chat['sources'], 1):
                        st.markdown(f"""
                        <div class="doc-chunk">
                            <strong>Ù…ØµØ¯Ø± {i} - {source['doc_name']} (Ù†Ù‚Ø§Ø· Ø§Ù„Ø´Ø¨Ù‡: {source['score']:.3f})</strong><br>
                            {source['text'][:200]}...
                        </div>
                        """, unsafe_allow_html=True)
        
        st.divider()
        
        # Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
        with st.form("chat_form"):
            user_question = st.text_area(
                "Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„Ùƒ:",
                height=100,
                placeholder="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù‡Ù†Ø§..."
            )
            
            col1, col2 = st.columns([1, 4])
            with col1:
                submitted = st.form_submit_button("ğŸš€ Ø¥Ø±Ø³Ø§Ù„", type="primary")
            with col2:
                num_sources = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø±", 1, 10, 3)
        
        if submitted and user_question.strip():
            with st.spinner("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
                # ØªÙˆÙ„ÙŠØ¯ embedding Ù„Ù„Ø³Ø¤Ø§Ù„
                if st.session_state.processor.embeddings_model:
                    question_embedding = st.session_state.processor.embeddings_model.encode([user_question])
                    
                    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³
                    search_results = st.session_state.vector_store.search(
                        question_embedding[0], 
                        k=num_sources
                    )
                    
                    if search_results:
                        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚
                        context_parts = []
                        sources = []
                        
                        for result in search_results:
                            if result['score'] >= similarity_threshold:
                                context_parts.append(result['chunk']['text'])
                                sources.append({
                                    'doc_name': result['chunk']['doc_name'],
                                    'text': result['chunk']['text'],
                                    'score': result['score']
                                })
                        
                        if context_parts:
                            context = '\n\n'.join(context_parts)
                            
                            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                            answer = st.session_state.llm.generate_response(
                                user_question, 
                                context
                            )
                            
                            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®
                            st.session_state.chat_history.append({
                                'question': user_question,
                                'answer': answer,
                                'sources': sources,
                                'timestamp': datetime.now().isoformat()
                            })
                            
                            st.rerun()
                        else:
                            st.warning("âš ï¸ Ù„Ù… Ø£Ø¬Ø¯ Ù…ØµØ§Ø¯Ø± Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ")
                    else:
                        st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬")
                else:
                    st.error("âŒ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ± ØºÙŠØ± Ù…ØªÙˆÙØ±")
    
    with tab3:
        st.header("ğŸ“ˆ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
        
        if not st.session_state.documents:
            st.info("ğŸ“Š Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ­Ù„ÙŠÙ„")
            return
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
        st.subheader("ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_docs = len(st.session_state.documents)
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ“š</h3>
                <h2>{total_docs}</h2>
                <p>ÙˆØ«ÙŠÙ‚Ø©</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            total_words = sum(len(doc['content'].split()) for doc in st.session_state.documents)
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ“</h3>
                <h2>{total_words:,}</h2>
                <p>ÙƒÙ„Ù…Ø©</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            total_chats = len(st.session_state.chat_history)
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ’¬</h3>
                <h2>{total_chats}</h2>
                <p>Ù…Ø­Ø§Ø¯Ø«Ø©</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            indexed_chunks = st.session_state.vector_store.index.ntotal if st.session_state.vector_store.index else 0
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ”</h3>
                <h2>{indexed_chunks}</h2>
                <p>Ù‚Ø·Ø¹Ø© Ù…ÙÙ‡Ø±Ø³Ø©</p>
            </div>
            """, unsafe_allow_html=True)
        
        st.divider()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
        st.subheader("ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
        
        docs_data = []
        for i, doc in enumerate(st.session_state.documents):
            words = len(doc['content'].split())
            chars = len(doc['content'])
            sentences = len(st.session_state.processor.split_into_sentences(doc['content']))
            
            docs_data.append({
                'Ø§Ù„Ø§Ø³Ù…': doc['name'],
                'Ø§Ù„Ù†ÙˆØ¹': doc.get('type', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                'Ø§Ù„ÙƒÙ„Ù…Ø§Øª': words,
                'Ø§Ù„Ø£Ø­Ø±Ù': chars,
                'Ø§Ù„Ø¬Ù…Ù„': sentences,
                'Ø§Ù„ØªØ§Ø±ÙŠØ®': doc.get('timestamp', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')[:10] if doc.get('timestamp') else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'
            })
        
        if docs_data:
            df = pd.DataFrame(docs_data)
            st.dataframe(df, use_container_width=True)
            
            # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ
            st.subheader("ğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª")
            chart_data = df[['Ø§Ù„Ø§Ø³Ù…', 'Ø§Ù„ÙƒÙ„Ù…Ø§Øª']].set_index('Ø§Ù„Ø§Ø³Ù…')
            st.bar_chart(chart_data)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
        if st.session_state.chat_history:
            st.divider()
            st.subheader("ğŸ’¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª")
            
            # Ø£ÙƒØ«Ø± Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ØªÙƒØ±Ø§Ø±Ø§Ù‹
            questions = [chat['question'] for chat in st.session_state.chat_history]
            question_lengths = [len(q.split()) for q in questions]
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„", f"{np.mean(question_lengths):.1f} ÙƒÙ„Ù…Ø©")
            
            with col2:
                avg_sources = np.mean([len(chat.get('sources', [])) for chat in st.session_state.chat_history])
                st.metric("Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØµØ§Ø¯Ø±/Ø¥Ø¬Ø§Ø¨Ø©", f"{avg_sources:.1f}")
            
            # Ø¢Ø®Ø± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
            st.subheader("ğŸ•’ Ø¢Ø®Ø± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª")
            for chat in st.session_state.chat_history[-3:]:
                with st.expander(f"â“ {chat['question'][:50]}..."):
                    st.write(f"**Ø§Ù„Ø³Ø¤Ø§Ù„:** {chat['question']}")
                    st.write(f"**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:** {chat['answer'][:200]}...")
                    if 'sources' in chat:
                        st.write(f"**Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø±:** {len(chat['sources'])}")
        
        st.divider()
        
        # ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        st.subheader("ğŸ’¾ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ“Š ØªØµØ¯ÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª CSV"):
                if docs_data:
                    csv = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ’¾ ØªØ­Ù…ÙŠÙ„ CSV",
                        data=csv,
                        file_name=f"rag_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
        
        with col2:
            if st.button("ğŸ’¬ ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª JSON"):
                if st.session_state.chat_history:
                    chat_export = {
                        'export_date': datetime.now().isoformat(),
                        'total_chats': len(st.session_state.chat_history),
                        'conversations': st.session_state.chat_history
                    }
                    
                    json_str = json.dumps(chat_export, ensure_ascii=False, indent=2)
                    st.download_button(
                        label="ğŸ’¾ ØªØ­Ù…ÙŠÙ„ JSON",
                        data=json_str,
                        file_name=f"rag_conversations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
        
        with col3:
            if st.button("ğŸ“‹ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"):
                # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„
                report = {
                    'metadata': {
                        'generated_at': datetime.now().isoformat(),
                        'app_version': '2.0',
                        'total_documents': len(st.session_state.documents),
                        'total_conversations': len(st.session_state.chat_history),
                        'indexed_chunks': indexed_chunks
                    },
                    'document_stats': docs_data,
                    'conversations': st.session_state.chat_history[-10:],  # Ø¢Ø®Ø± 10 Ù…Ø­Ø§Ø¯Ø«Ø§Øª
                    'system_info': {
                        'embeddings_model_loaded': st.session_state.processor.embeddings_model is not None,
                        'vector_store_ready': st.session_state.vector_store.index is not None,
                        'llm_ready': st.session_state.is_ready
                    }
                }
                
                report_str = json.dumps(report, ensure_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ’¾ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ±",
                    data=report_str,
                    file_name=f"rag_full_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )

# ======================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© ========================

def show_help():
    """Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©"""
    with st.expander("â„¹ï¸ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"):
        st.markdown("""
        ### ğŸš€ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
        
        1. **Ø¥Ø¹Ø¯Ø§Ø¯ AI API:**
           - Ø§Ø®ØªØ± Ù…Ù‚Ø¯Ù… Ø§Ù„Ø®Ø¯Ù…Ø© (OpenAI Ø£Ùˆ Groq)
           - Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ API Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ
           - Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ "Ø±Ø¨Ø·"
        
        2. **Ø±ÙØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚:**
           - Ø§Ø°Ù‡Ø¨ Ù„ØªØ¨ÙˆÙŠØ¨ "Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"
           - Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§ØªÙƒ Ø£Ùˆ Ø£Ø¯Ø®Ù„ Ù†Øµ Ù…Ø¨Ø§Ø´Ø±
           - Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ "Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆÙÙ‡Ø±Ø³Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"
        
        3. **Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:**
           - Ø§Ø°Ù‡Ø¨ Ù„ØªØ¨ÙˆÙŠØ¨ "Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"
           - Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ
           - Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ ÙˆØ«Ø§Ø¦Ù‚Ùƒ
        
        4. **Ø§Ù„ØªØ­Ù„ÙŠÙ„:**
           - Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙÙŠ ØªØ¨ÙˆÙŠØ¨ "Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª"
           - ØµØ¯Ù‘Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
        
        ### ğŸ”§ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:
        - Ù…ÙØªØ§Ø­ API Ù…Ù† OpenAI Ø£Ùˆ Groq
        - Ø¥Ù†ØªØ±Ù†Øª Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ±
        - Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        
        ### ğŸ’¡ Ù†ØµØ§Ø¦Ø­:
        - Ø§Ø³ØªØ®Ø¯Ù… Ù†ØµÙˆØµ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ù†Ø¸Ù…Ø©
        - Ø§Ø¬Ø¹Ù„ Ø£Ø³Ø¦Ù„ØªÙƒ Ù…Ø­Ø¯Ø¯Ø©
        - Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
        """)

def main():
    init_session_state()
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.header("ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ©
        st.subheader("ğŸ¤– Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª AI")
        
        llm_provider = st.selectbox(
            "Ø§Ø®ØªØ± Ù…Ù‚Ø¯Ù… Ø§Ù„Ø®Ø¯Ù…Ø©:",
            ["OpenAI", "Groq", "None"]
        )
        
        if llm_provider != "None":
            api_key = st.text_input(
                f"Ù…ÙØªØ§Ø­ {llm_provider} API:",
                type="password",
                help=f"Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ API Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ {llm_provider}"
            )
            
            if api_key and st.button(f"ğŸ”— Ø±Ø¨Ø· {llm_provider}"):
                with st.spinner(f"Ø¬Ø§Ø±ÙŠ Ø±Ø¨Ø· {llm_provider}..."):
                    if llm_provider == "OpenAI":
                        success = st.session_state.llm.setup_openai(api_key)
                    elif llm_provider == "Groq":
                        success = st.session_state.llm.setup_groq(api_key)
                    
                    if success:
                        st.success(f"âœ… ØªÙ… Ø±Ø¨Ø· {llm_provider} Ø¨Ù†Ø¬Ø§Ø­!")
                        st.session_state.is_ready = True
                    else:
                        st.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø±Ø¨Ø· {llm_provider}")
        
        st.divider()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        st.subheader("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        chunk_size = st.slider("Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„Ù†ØµÙŠØ©", 200, 1000, 500)
        overlap_size = st.slider("Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§Ø®Ù„", 20, 200, 50)
        similarity_threshold = st.slider("Ø­Ø¯ Ø§Ù„Ø´Ø¨Ù‡", 0.1, 1.0, 0.7)
        
        st.divider()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        st.subheader("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
        st.metric("Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­Ù…Ù„Ø©", len(st.session_state.documents))
        
        if st.session_state.vector_store.index:
            st.metric("Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…ÙÙ‡Ø±Ø³Ø©", st.session_state.vector_store.index.ntotal)
        
        st.metric("Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª", len(st.session_state.chat_history))
        
        st.divider()
        
        # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­ÙƒÙ…
        if st.button("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„"):
            st.rerun()
        
        if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"):
            for key in ['documents', 'chat_history']:
                if key in st.session_state:
                    st.session_state[key] = []
            st.session_state.vector_store = VectorStore()
            st.session_state.is_ready = False
            st.success("âœ… ØªÙ… Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
            st.rerun()
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
    show_help()
    
    # Ø¨Ø§Ù‚ÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚...
    # (Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø£Ø¹Ù„Ø§Ù‡)

if __name__ == "__main__":
    main()
