"""
Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¨Ø³Ø·Ø© ÙˆØ§Ù„ÙØ¹Ø§Ù„Ø©
Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ø¹ ÙˆØ§Ø¬Ù‡Ø© Ù†Ø¸ÙŠÙØ©
"""

import streamlit as st
import pandas as pd
import re
import json
import hashlib
from typing import List, Dict, Tuple, Any
from datetime import datetime
from collections import Counter
import math
import io
import os

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚", 
    page_icon="ğŸŒ", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# Ø§Ù„Ø£Ù†Ù…Ø§Ø· CSS Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-box {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #007bff;
        margin: 0.5rem 0;
    }
    .success-box {
        background: #d4edda;
        border-left-color: #28a745;
    }
    .warning-box {
        background: #fff3cd;
        border-left-color: #ffc107;
    }
    .rtl {
        direction: rtl;
        text-align: right;
    }
    .doc-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin: 1rem 0;
        border: 1px solid #e9ecef;
    }
</style>
""", unsafe_allow_html=True)

# Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
st.markdown("""
<div class="main-header">
    <h1>ğŸŒ Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚</h1>
    <p>Ù…Ù†ØµØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙØ¹Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</p>
</div>
""", unsafe_allow_html=True)

# ======================== Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ========================

def init_session_state():
    """ØªÙ‡ÙŠØ¦Ø© Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©"""
    if 'documents' not in st.session_state:
        st.session_state.documents = []
    if 'processed_docs' not in st.session_state:
        st.session_state.processed_docs = []
    if 'search_index' not in st.session_state:
        st.session_state.search_index = {}
    if 'stats' not in st.session_state:
        st.session_state.stats = {
            'total_docs': 0,
            'total_words': 0,
            'total_chars': 0,
            'processing_time': 0
        }

def clean_arabic_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    if not text:
        return ""
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’]', '', text)
    
    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø±Ù
    replacements = {
        'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§',
        'Ø©': 'Ù‡', 'Ù‰': 'ÙŠ'
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø£Ø³Ø·Ø±
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text

def extract_sentences(text: str) -> List[str]:
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø¬Ù…Ù„"""
    if not text:
        return []
    
    # Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙ‚Ø³ÙŠÙ…
    for punct in ['ØŸ', '!', '.', 'Ø›']:
        text = text.replace(punct, f'{punct}\n')
    
    sentences = []
    for line in text.split('\n'):
        sentence = line.strip()
        if sentence and len(sentence) > 10:
            sentences.append(sentence)
    
    return sentences

def calculate_tfidf_scores(documents: List[str]) -> Dict[str, Dict[str, float]]:
    """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· TF-IDF Ù„Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"""
    if not documents:
        return {}
    
    # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†ØµÙˆØµ
    clean_docs = [clean_arabic_text(doc) for doc in documents]
    all_words = []
    doc_words = []
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
    for doc in clean_docs:
        words = [word for word in doc.split() if len(word) > 2]
        doc_words.append(words)
        all_words.extend(words)
    
    # Ø­Ø³Ø§Ø¨ ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©
    word_doc_count = Counter()
    doc_word_counts = []
    
    for words in doc_words:
        word_count = Counter(words)
        doc_word_counts.append(word_count)
        for word in set(words):
            word_doc_count[word] += 1
    
    # Ø­Ø³Ø§Ø¨ TF-IDF
    tfidf_scores = {}
    total_docs = len(documents)
    
    for i, word_count in enumerate(doc_word_counts):
        doc_scores = {}
        total_words = sum(word_count.values())
        
        for word, count in word_count.items():
            if total_words > 0 and word_doc_count[word] > 0:
                tf = count / total_words
                idf = math.log(total_docs / word_doc_count[word])
                doc_scores[word] = tf * idf
        
        tfidf_scores[f'doc_{i}'] = doc_scores
    
    return tfidf_scores

def rank_sentences(documents: List[str], top_k: int = 3) -> Dict[str, List[Dict]]:
    """ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©"""
    results = {}
    
    try:
        for i, doc in enumerate(documents):
            sentences = extract_sentences(doc)
            if not sentences:
                continue
            
            # Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬Ù…Ù„
            sentence_scores = []
            doc_words = clean_arabic_text(doc).split()
            word_freq = Counter(word for word in doc_words if len(word) > 2)
            
            for sentence in sentences:
                clean_sent = clean_arabic_text(sentence)
                sent_words = clean_sent.split()
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
                score = 0
                if sent_words:
                    for word in sent_words:
                        if word in word_freq and len(word) > 2:
                            score += word_freq[word] / len(doc_words)
                    score = score / len(sent_words)  # Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Ù‚Ø§Ø·
                
                sentence_scores.append({
                    'text': sentence,
                    'score': score,
                    'length': len(sentence),
                    'word_count': len(sent_words)
                })
            
            # ØªØ±ØªÙŠØ¨ ÙˆØ£Ø®Ø° Ø§Ù„Ø£ÙØ¶Ù„
            sentence_scores.sort(key=lambda x: x['score'], reverse=True)
            results[f'doc_{i}'] = sentence_scores[:top_k]
            
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„: {e}")
    
    return results

def read_uploaded_file(uploaded_file) -> Tuple[str, Dict]:
    """Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹"""
    file_info = {
        'name': uploaded_file.name,
        'size': uploaded_file.size,
        'type': uploaded_file.type
    }
    
    try:
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†Øµ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
        if uploaded_file.type == "text/plain":
            content = str(uploaded_file.read(), "utf-8")
        elif uploaded_file.type == "application/pdf":
            try:
                import PyPDF2
                pdf_reader = PyPDF2.PdfReader(uploaded_file)
                content = ""
                for page in pdf_reader.pages:
                    content += page.extract_text()
                file_info['pages'] = len(pdf_reader.pages)
            except ImportError:
                content = "Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª PDF ØªØªØ·Ù„Ø¨ ØªØ«Ø¨ÙŠØª PyPDF2"
        else:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ†Øµ
            content = str(uploaded_file.read(), "utf-8", errors='ignore')
        
        file_info['success'] = True
        return content, file_info
        
    except Exception as e:
        file_info['error'] = str(e)
        return f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}", file_info

def create_search_index(processed_docs: List[Dict]) -> Dict:
    """Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«"""
    index = {
        'words': {},
        'documents': {},
        'sentences': []
    }
    
    for doc in processed_docs:
        doc_id = doc['id']
        content = doc['clean_text']
        sentences = doc['sentences']
        
        # ÙÙ‡Ø±Ø³Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©
        index['documents'][doc_id] = {
            'title': doc.get('title', f'ÙˆØ«ÙŠÙ‚Ø© {doc_id}'),
            'content': content,
            'word_count': len(content.split()),
            'sentence_count': len(sentences)
        }
        
        # ÙÙ‡Ø±Ø³Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        for word in content.split():
            if len(word) > 2:
                if word not in index['words']:
                    index['words'][word] = []
                index['words'][word].append(doc_id)
        
        # ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ø¬Ù…Ù„
        for sentence in sentences:
            index['sentences'].append({
                'text': sentence,
                'doc_id': doc_id,
                'words': clean_arabic_text(sentence).split()
            })
    
    return index

def search_documents(query: str, search_index: Dict, max_results: int = 5) -> List[Dict]:
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"""
    if not query or not search_index:
        return []
    
    query_words = clean_arabic_text(query).split()
    results = []
    doc_scores = {}
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ù„ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©
    for word in query_words:
        if word in search_index.get('words', {}):
            for doc_id in search_index['words'][word]:
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = 0
                doc_scores[doc_id] += 1
    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„
    for doc_id, score in sorted_docs[:max_results]:
        if doc_id in search_index['documents']:
            doc_info = search_index['documents'][doc_id]
            results.append({
                'doc_id': doc_id,
                'title': doc_info['title'],
                'score': score,
                'word_count': doc_info['word_count'],
                'sentence_count': doc_info['sentence_count'],
                'content_preview': doc_info['content'][:200] + '...'
            })
    
    return results

# ======================== Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ========================

def main():
    init_session_state()
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.header("ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        st.subheader("Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
        chunk_size = st.slider("Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹", 100, 1000, 300)
        top_sentences = st.slider("Ø¹Ø¯Ø¯ Ø£Ù‡Ù… Ø§Ù„Ø¬Ù…Ù„", 1, 10, 3)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        st.subheader("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
        stats = st.session_state.stats
        st.metric("Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚", stats['total_docs'])
        st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", f"{stats['total_words']:,}")
        st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø­Ø±Ù", f"{stats['total_chars']:,}")
        
        # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­ÙƒÙ…
        if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"):
            for key in ['documents', 'processed_docs', 'search_index']:
                if key in st.session_state:
                    del st.session_state[key]
            st.session_state.stats = {'total_docs': 0, 'total_words': 0, 'total_chars': 0, 'processing_time': 0}
            st.rerun()
    
    # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    tab1, tab2, tab3 = st.tabs(["ğŸ“¤ Ø±ÙØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚", "ğŸ” Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„", "ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬"])
    
    with tab1:
        st.header("Ø±ÙØ¹ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
        
        # Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        uploaded_files = st.file_uploader(
            "Ø§Ø®ØªØ± Ø§Ù„Ù…Ù„ÙØ§Øª",
            accept_multiple_files=True,
            type=['txt', 'pdf', 'docx', 'csv']
        )
        
        # Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù…Ø¨Ø§Ø´Ø±
        with st.expander("ğŸ“ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù…Ø¨Ø§Ø´Ø±"):
            direct_text = st.text_area("Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§", height=200)
            if st.button("Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Øµ"):
                if direct_text:
                    st.session_state.documents.append({
                        'content': direct_text,
                        'name': f'Ù†Øµ_Ù…Ø¨Ø§Ø´Ø±_{len(st.session_state.documents) + 1}',
                        'type': 'text'
                    })
                    st.success("ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Øµ!")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©
        if uploaded_files:
            st.subheader("Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©:")
            
            for uploaded_file in uploaded_files:
                with st.expander(f"ğŸ“„ {uploaded_file.name}"):
                    content, file_info = read_uploaded_file(uploaded_file)
                    
                    st.json({
                        'Ø§Ù„Ø§Ø³Ù…': file_info['name'],
                        'Ø§Ù„Ø­Ø¬Ù…': f"{file_info['size']/1024:.1f} KB",
                        'Ø§Ù„Ù†ÙˆØ¹': file_info['type']
                    })
                    
                    if content and not content.startswith("Ø®Ø·Ø£"):
                        preview = content[:300] + "..." if len(content) > 300 else content
                        st.text_area("Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰:", preview, height=100)
                        
                        if st.button(f"Ø¥Ø¶Ø§ÙØ© {uploaded_file.name}", key=uploaded_file.name):
                            st.session_state.documents.append({
                                'content': content,
                                'name': uploaded_file.name,
                                'type': file_info.get('type', 'unknown'),
                                'info': file_info
                            })
                            st.success(f"ØªÙ… Ø¥Ø¶Ø§ÙØ© {uploaded_file.name}!")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
        if st.session_state.documents:
            st.subheader(f"ğŸ“š Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­Ù…Ù„Ø© ({len(st.session_state.documents)})")
            
            if st.button("ğŸš€ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚", type="primary"):
                with st.spinner("Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚..."):
                    start_time = datetime.now()
                    
                    processed_docs = []
                    total_words = 0
                    total_chars = 0
                    
                    progress_bar = st.progress(0)
                    
                    for i, doc in enumerate(st.session_state.documents):
                        content = doc['content']
                        clean_content = clean_arabic_text(content)
                        sentences = extract_sentences(content)
                        
                        doc_stats = {
                            'words': len(content.split()),
                            'chars': len(content),
                            'sentences': len(sentences)
                        }
                        
                        total_words += doc_stats['words']
                        total_chars += doc_stats['chars']
                        
                        processed_doc = {
                            'id': i,
                            'title': doc['name'],
                            'original_text': content,
                            'clean_text': clean_content,
                            'sentences': sentences,
                            'stats': doc_stats
                        }
                        
                        processed_docs.append(processed_doc)
                        
                        # ØªØ­Ø¯ÙŠØ« Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
                        progress_bar.progress((i + 1) / len(st.session_state.documents))
                    
                    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                    st.session_state.processed_docs = processed_docs
                    st.session_state.search_index = create_search_index(processed_docs)
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                    processing_time = (datetime.now() - start_time).total_seconds()
                    st.session_state.stats = {
                        'total_docs': len(processed_docs),
                        'total_words': total_words,
                        'total_chars': total_chars,
                        'processing_time': processing_time
                    }
                    
                    st.success(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {len(processed_docs)} ÙˆØ«ÙŠÙ‚Ø© ÙÙŠ {processing_time:.2f} Ø«Ø§Ù†ÙŠØ©!")
    
    with tab2:
        st.header("Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„")
        
        if not st.session_state.processed_docs:
            st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø£ÙˆÙ„Ø§Ù‹ ÙÙŠ ØªØ¨ÙˆÙŠØ¨ 'Ø±ÙØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚'")
            return
        
        # Ø§Ù„Ø¨Ø­Ø«
        search_query = st.text_input("ğŸ” Ø§Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚:")
        
        if search_query:
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø«..."):
                results = search_documents(search_query, st.session_state.search_index)
                
                if results:
                    st.subheader(f"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« ({len(results)}):")
                    
                    for result in results:
                        with st.container():
                            st.markdown(f"""
                            <div class="doc-card">
                                <h4>ğŸ“„ {result['title']}</h4>
                                <p><strong>Ø§Ù„Ù†Ù‚Ø§Ø·:</strong> {result['score']} | 
                                <strong>Ø§Ù„ÙƒÙ„Ù…Ø§Øª:</strong> {result['word_count']} | 
                                <strong>Ø§Ù„Ø¬Ù…Ù„:</strong> {result['sentence_count']}</p>
                                <p class="rtl">{result['content_preview']}</p>
                            </div>
                            """, unsafe_allow_html=True)
                else:
                    st.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©
        if st.button("ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ù‡Ù… Ø§Ù„Ø¬Ù…Ù„"):
            with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„..."):
                documents_text = [doc['original_text'] for doc in st.session_state.processed_docs]
                ranked_sentences = rank_sentences(documents_text, top_sentences)
                
                st.subheader("Ø£Ù‡Ù… Ø§Ù„Ø¬Ù…Ù„ Ù…Ù† ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©:")
                
                for doc_id, sentences in ranked_sentences.items():
                    doc_title = st.session_state.processed_docs[int(doc_id.split('_')[1])]['title']
                    
                    with st.expander(f"ğŸ“„ {doc_title}"):
                        for i, sent_info in enumerate(sentences, 1):
                            st.markdown(f"""
                            **{i}.** {sent_info['text']}
                            
                            *Ø§Ù„Ù†Ù‚Ø§Ø·: {sent_info['score']:.3f} | Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {sent_info['word_count']}*
                            """)
    
    with tab3:
        st.header("Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª")
        
        if not st.session_state.processed_docs:
            st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ«Ø§Ø¦Ù‚ Ù„Ù„ØªØ­Ù„ÙŠÙ„")
            return
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©
        st.subheader("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        docs_data = []
        for doc in st.session_state.processed_docs:
            docs_data.append({
                'Ø§Ù„Ø¹Ù†ÙˆØ§Ù†': doc['title'],
                'Ø§Ù„ÙƒÙ„Ù…Ø§Øª': doc['stats']['words'],
                'Ø§Ù„Ø£Ø­Ø±Ù': doc['stats']['chars'],
                'Ø§Ù„Ø¬Ù…Ù„': doc['stats']['sentences']
            })
        
        df = pd.DataFrame(docs_data)
        st.dataframe(df, use_container_width=True)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚", len(st.session_state.processed_docs))
        
        with col2:
            total_words = sum(doc['stats']['words'] for doc in st.session_state.processed_docs)
            st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", f"{total_words:,}")
        
        with col3:
            total_sentences = sum(doc['stats']['sentences'] for doc in st.session_state.processed_docs)
            st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¬Ù…Ù„", total_sentences)
        
        with col4:
            avg_words = total_words / len(st.session_state.processed_docs) if st.session_state.processed_docs else 0
            st.metric("Ù…ØªÙˆØ³Ø· Ø§Ù„ÙƒÙ„Ù…Ø§Øª/ÙˆØ«ÙŠÙ‚Ø©", f"{avg_words:.0f}")
        
        # ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        st.subheader("ğŸ“¤ ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ“Š ØªØµØ¯ÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª CSV"):
                csv = df.to_csv(index=False)
                st.download_button(
                    label="ØªØ­Ù…ÙŠÙ„ CSV",
                    data=csv,
                    file_name="document_stats.csv",
                    mime="text/csv"
                )
        
        with col2:
            if st.button("ğŸ“„ ØªØµØ¯ÙŠØ± ØªÙ‚Ø±ÙŠØ± JSON"):
                report = {
                    'timestamp': datetime.now().isoformat(),
                    'summary': st.session_state.stats,
                    'documents': docs_data
                }
                
                json_str = json.dumps(report, ensure_ascii=False, indent=2)
                st.download_button(
                    label="ØªØ­Ù…ÙŠÙ„ JSON",
                    data=json_str,
                    file_name="rag_report.json",
                    mime="application/json"
                )

if __name__ == "__main__":
    main()
