"""
Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ - Ù†Ø³Ø®Ø© Ù…Ø¨Ø³Ø·Ø© ØªØ¹Ù…Ù„ Ø¨Ø¯ÙˆÙ† Ù…ÙƒØªØ¨Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©
ØªØ·Ø¨ÙŠÙ‚ ÙØ¹Ø§Ù„ Ù…Ø¹ ØªÙ‚Ù†ÙŠØ§Øª RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… APIs Ù…Ø¨Ø§Ø´Ø±Ø©
"""

import streamlit as st
import pandas as pd
import numpy as np
import re
import json
import hashlib
from typing import List, Dict, Any, Optional
from datetime import datetime
import requests
import os
from collections import Counter
import math
import io
import base64

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", 
    page_icon="ğŸ¤–", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Ù„Ù„ØªØµÙ…ÙŠÙ…
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 10px 30px rgba(0,0,0,0.2);
    }
    
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
        border: 1px solid #e0e0e0;
    }
    
    .user-message {
        background: #e3f2fd;
        margin-right: 20px;
        border-left: 4px solid #2196f3;
    }
    
    .ai-message {
        background: #f3e5f5;
        margin-left: 20px;
        border-left: 4px solid #9c27b0;
    }
    
    .doc-chunk {
        background: #fff3e0;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        border-left: 4px solid #ff9800;
        font-size: 0.9rem;
    }
    
    .metric-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        text-align: center;
        border: 1px solid #e0e0e0;
    }
    
    .status-success {
        background: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #28a745;
        margin: 1rem 0;
    }
    
    .status-warning {
        background: #fff3cd;
        color: #856404;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #ffc107;
        margin: 1rem 0;
    }
    
    .rtl {
        direction: rtl;
        text-align: right;
    }
    
    .similarity-score {
        background: #e8f5e8;
        padding: 0.5rem;
        border-radius: 5px;
        font-size: 0.8rem;
        color: #2e7d32;
        display: inline-block;
        margin: 0.2rem;
    }
</style>
""", unsafe_allow_html=True)

# Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
st.markdown("""
<div class="main-header">
    <h1>ğŸ¤– Ø±Ø§Ø¬ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚</h1>
    <p>ØªØ·Ø¨ÙŠÙ‚ ÙØ¹Ø§Ù„ Ù…Ø¹ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«Ø©</p>
    <p>ÙŠØ¯Ø¹Ù… OpenAIØŒ GroqØŒ ÙˆØ£ÙŠ API Ù…ØªÙˆØ§ÙÙ‚</p>
</div>
""", unsafe_allow_html=True)

# ======================== Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª ÙˆØ§Ù„ÙˆØ¸Ø§Ø¦Ù ========================

class SimpleEmbedding:
    """Ù†Ø¸Ø§Ù… ØªØ´ÙÙŠØ± Ù…Ø¨Ø³Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF"""
    
    def __init__(self):
        self.vocabulary = {}
        self.idf_scores = {}
        self.is_fitted = False
    
    def clean_text(self, text: str) -> List[str]:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ù„ÙƒÙ„Ù…Ø§Øª"""
        if not text:
            return []
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        text = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°]', '', text)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø±Ù
        text = text.replace('Ø£', 'Ø§').replace('Ø¥', 'Ø§').replace('Ø¢', 'Ø§')
        text = text.replace('Ø©', 'Ù‡').replace('Ù‰', 'ÙŠ')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        words = re.findall(r'\b[\u0600-\u06FF\w]+\b', text)
        
        # ÙÙ„ØªØ±Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
        words = [word for word in words if len(word) > 2]
        
        return words
    
    def fit(self, documents: List[str]):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"""
        all_words = set()
        doc_word_sets = []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…Ù† ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©
        for doc in documents:
            words = self.clean_text(doc)
            word_set = set(words)
            doc_word_sets.append(word_set)
            all_words.update(words)
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø¬Ù…
        self.vocabulary = {word: idx for idx, word in enumerate(all_words)}
        
        # Ø­Ø³Ø§Ø¨ IDF
        total_docs = len(documents)
        for word in all_words:
            doc_count = sum(1 for word_set in doc_word_sets if word in word_set)
            self.idf_scores[word] = math.log(total_docs / (doc_count + 1))
        
        self.is_fitted = True
    
    def transform(self, documents: List[str]) -> np.ndarray:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¥Ù„Ù‰ ÙÙŠÙƒØªÙˆØ±Ø§Øª"""
        if not self.is_fitted:
            return np.array([])
        
        vectors = []
        vocab_size = len(self.vocabulary)
        
        for doc in documents:
            words = self.clean_text(doc)
            word_count = Counter(words)
            total_words = len(words)
            
            vector = np.zeros(vocab_size)
            
            for word, count in word_count.items():
                if word in self.vocabulary:
                    tf = count / total_words
                    idf = self.idf_scores[word]
                    vector[self.vocabulary[word]] = tf * idf
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ÙÙŠÙƒØªÙˆØ±
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm
            
            vectors.append(vector)
        
        return np.array(vectors)
    
    def fit_transform(self, documents: List[str]) -> np.ndarray:
        """ØªØ¯Ø±ÙŠØ¨ ÙˆØªØ­ÙˆÙŠÙ„ ÙÙŠ Ø®Ø·ÙˆØ© ÙˆØ§Ø­Ø¯Ø©"""
        self.fit(documents)
        return self.transform(documents)

class SimpleVectorStore:
    """Ù…Ø®Ø²Ù† ÙÙŠÙƒØªÙˆØ±Ø§Øª Ù…Ø¨Ø³Ø·"""
    
    def __init__(self):
        self.vectors = None
        self.chunks = []
        self.embedder = SimpleEmbedding()
    
    def add_documents(self, chunks: List[Dict], texts: List[str]) -> bool:
        """Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"""
        try:
            self.chunks = chunks
            self.vectors = self.embedder.fit_transform(texts)
            return True
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚: {e}")
            return False
    
    def search(self, query: str, k: int = 5) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙÙŠÙƒØªÙˆØ±Ø§Øª"""
        if self.vectors is None or len(self.vectors) == 0:
            return []
        
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù„ÙÙŠÙƒØªÙˆØ±
            query_vector = self.embedder.transform([query])
            if len(query_vector) == 0:
                return []
            
            query_vector = query_vector[0]
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            similarities = np.dot(self.vectors, query_vector)
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            top_indices = np.argsort(similarities)[::-1][:k]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.1:  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡
                    results.append({
                        'chunk': self.chunks[idx],
                        'score': float(similarities[idx]),
                        'index': int(idx)
                    })
            
            return results
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
            return []

class APIClient:
    """Ø¹Ù…ÙŠÙ„ API Ù…ÙˆØ­Ø¯"""
    
    def __init__(self):
        self.api_key = None
        self.provider = None
        self.base_url = None
    
    def setup(self, provider: str, api_key: str) -> bool:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù‚Ø¯Ù… Ø§Ù„Ø®Ø¯Ù…Ø©"""
        try:
            self.api_key = api_key
            self.provider = provider.lower()
            
            if self.provider == "openai":
                self.base_url = "https://api.openai.com/v1/chat/completions"
            elif self.provider == "groq":
                self.base_url = "https://api.groq.com/openai/v1/chat/completions"
            else:
                return False
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
            return self.test_connection()
            
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯: {e}")
            return False
    
    def test_connection(self) -> bool:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„"""
        try:
            response = self.generate_response("Ù…Ø±Ø­Ø¨Ø§", "Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„", max_tokens=10)
            return not response.startswith("âŒ")
        except:
            return False
    
    def generate_response(self, prompt: str, context: str, max_tokens: int = 500) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
        if not self.api_key or not self.base_url:
            return "âŒ Ù„Ù… ÙŠØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ API"
        
        try:
            system_message = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©.
            Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©:
            - Ø§Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
            - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…
            - Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ
            - Ø§Ø¬Ø¹Ù„ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙÙŠØ¯Ø©"""
            
            user_message = f"""Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªÙˆÙØ±:
            {context}
            
            Ø§Ù„Ø³Ø¤Ø§Ù„: {prompt}
            
            ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£Ø¹Ù„Ø§Ù‡:"""
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ù…
            if self.provider == "openai":
                model = "gpt-3.5-turbo"
            elif self.provider == "groq":
                model = "llama-3.1-70b-versatile"
            else:
                model = "gpt-3.5-turbo"
            
            data = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message}
                ],
                "max_tokens": max_tokens,
                "temperature": 0.3,
                "stream": False
            }
            
            response = requests.post(
                self.base_url, 
                headers=headers, 
                json=data, 
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    return result['choices'][0]['message']['content'].strip()
                else:
                    return "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©"
            else:
                error_msg = f"HTTP {response.status_code}"
                try:
                    error_detail = response.json()
                    if 'error' in error_detail:
                        error_msg += f": {error_detail['error'].get('message', 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}"
                except:
                    pass
                return f"âŒ Ø®Ø·Ø£ ÙÙŠ API: {error_msg}"
                
        except requests.exceptions.Timeout:
            return "âŒ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø·Ù„Ø¨"
        except requests.exceptions.ConnectionError:
            return "âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„"
        except Exception as e:
            return f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}"

class DocumentProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"""
    
    def clean_arabic_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        if not text:
            return ""
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        text = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°Ù•Ù–ÙœÙŸÙ”Ù—Ù˜Ù™ÙšÙ›ÙÙÙ±]', '', text)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        replacements = {
            'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§',
            'Ø©': 'Ù‡', 'Ù‰': 'ÙŠ'
        }
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØºØ±ÙŠØ¨Ø©
        text = re.sub(r'[^\w\s\u0600-\u06FF]', ' ', text)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def split_into_sentences(self, text: str) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø¬Ù…Ù„"""
        if not text:
            return []
        
        # Ø¹Ù„Ø§Ù…Ø§Øª Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©
        sentence_endings = r'[.!?ØŸà¥¤Û”\n]+'
        sentences = re.split(sentence_endings, text)
        
        clean_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 15:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù‚ØµÙŠØ±Ø©
                clean_sentences.append(sentence)
        
        return clean_sentences
    
    def chunk_text(self, text: str, chunk_size: int = 400, overlap: int = 50) -> List[Dict]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù‚Ø·Ø¹"""
        if not text:
            return []
        
        sentences = self.split_into_sentences(text)
        chunks = []
        current_chunk = ""
        current_words = 0
        
        for sentence in sentences:
            sentence_words = len(sentence.split())
            
            # Ø¥Ø°Ø§ ØªØ¬Ø§ÙˆØ²Ù†Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­
            if current_words + sentence_words > chunk_size and current_chunk:
                chunks.append({
                    'text': current_chunk.strip(),
                    'word_count': current_words,
                    'id': len(chunks)
                })
                
                # Ø¨Ø¯Ø§ÙŠØ© Ù‚Ø·Ø¹Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ ØªØ¯Ø§Ø®Ù„
                words = current_chunk.split()
                overlap_text = ' '.join(words[-overlap:]) if len(words) > overlap else current_chunk
                current_chunk = overlap_text + ' ' + sentence
                current_words = len(overlap_text.split()) + sentence_words
            else:
                current_chunk += ' ' + sentence if current_chunk else sentence
                current_words += sentence_words
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'word_count': current_words,
                'id': len(chunks)
            })
        
        return chunks

# ======================== ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… ========================

def init_session_state():
    """ØªÙ‡ÙŠØ¦Ø© Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©"""
    if 'documents' not in st.session_state:
        st.session_state.documents = []
    
    if 'processed_chunks' not in st.session_state:
        st.session_state.processed_chunks = []
    
    if 'vector_store' not in st.session_state:
        st.session_state.vector_store = SimpleVectorStore()
    
    if 'api_client' not in st.session_state:
        st.session_state.api_client = APIClient()
    
    if 'processor' not in st.session_state:
        st.session_state.processor = DocumentProcessor()
    
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    if 'is_ready' not in st.session_state:
        st.session_state.is_ready = False
    
    if 'system_stats' not in st.session_state:
        st.session_state.system_stats = {
            'total_processed': 0,
            'total_searches': 0,
            'total_responses': 0
        }

def read_uploaded_file(uploaded_file) -> tuple[str, dict]:
    """Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹"""
    file_info = {
        'name': uploaded_file.name,
        'size': uploaded_file.size,
        'type': uploaded_file.type
    }
    
    try:
        if uploaded_file.type == "text/plain":
            content = str(uploaded_file.read(), "utf-8")
        elif uploaded_file.type == "application/pdf":
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† PDF (Ù…Ø­Ø¯ÙˆØ¯)
            content = str(uploaded_file.read(), "utf-8", errors='ignore')
            if not content.strip():
                content = "ØªØ­Ø°ÙŠØ±: Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† PDF Ù…ÙƒØªÙ…Ù„Ø§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ù†ØµÙŠ."
        else:
            content = str(uploaded_file.read(), "utf-8", errors='ignore')
        
        return content, file_info
        
    except Exception as e:
        return f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}", file_info

# ======================== Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ========================

def main():
    init_session_state()
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.header("ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        st.subheader("ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # Ø¹Ø±Ø¶ Ø­Ø§Ù„Ø© Ù…ÙØµÙ„Ø©
        if st.session_state.is_ready:
            st.markdown("ğŸŸ¢ **AI Ø¬Ø§Ù‡Ø²:** Ù…ØªØµÙ„ ÙˆÙŠØ¹Ù…Ù„")
            if hasattr(st.session_state.api_client, 'provider'):
                st.write(f"ğŸ“¡ **Ø§Ù„Ù…Ù‚Ø¯Ù…:** {st.session_state.api_client.provider.upper()}")
        else:
            st.markdown("ğŸ”´ **AI ØºÙŠØ± Ø¬Ø§Ù‡Ø²:** ÙŠØ­ØªØ§Ø¬ Ø¥Ø¹Ø¯Ø§Ø¯")
        
        docs_count = len(st.session_state.documents)
        chunks_count = len(st.session_state.processed_chunks)
        st.write(f"ğŸ“š **Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚:** {docs_count}")
        st.write(f"ğŸ“„ **Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:** {chunks_count}")
        st.write(f"ğŸ’¬ **Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª:** {len(st.session_state.chat_history)}")
        
        # Ù…Ø¤Ø´Ø± Ø§Ù„Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
        if st.session_state.is_ready and chunks_count > 0:
            st.success("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©!")
        elif st.session_state.is_ready and chunks_count == 0:
            st.warning("âš ï¸ API Ø¬Ø§Ù‡Ø² - ÙŠØ­ØªØ§Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ«Ø§Ø¦Ù‚")
        elif not st.session_state.is_ready and chunks_count > 0:
            st.warning("âš ï¸ ÙˆØ«Ø§Ø¦Ù‚ Ø¬Ø§Ù‡Ø²Ø© - ÙŠØ­ØªØ§Ø¬ Ø¥Ø¹Ø¯Ø§Ø¯ AI")
        else:
            st.error("âŒ ÙŠØ­ØªØ§Ø¬ Ø¥Ø¹Ø¯Ø§Ø¯ AI ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ«Ø§Ø¦Ù‚")
        
        st.divider()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ API
        st.subheader("ğŸ¤– Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
        
        api_provider = st.selectbox(
            "Ø§Ø®ØªØ± Ù…Ù‚Ø¯Ù… Ø§Ù„Ø®Ø¯Ù…Ø©:",
            ["Ø§Ø®ØªØ±...", "OpenAI", "Groq"],
            help="Ø§Ø®ØªØ± Ù…Ù‚Ø¯Ù… Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"
        )
        
        if api_provider != "Ø§Ø®ØªØ±...":
            api_key = st.text_input(
                f"ğŸ”‘ Ù…ÙØªØ§Ø­ {api_provider}:",
                type="password",
                help=f"Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ API Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ {api_provider}"
            )
            
            if api_key and st.button(f"ğŸ”— Ø§ØªØµØ§Ù„ Ø¨Ù€ {api_provider}"):
                with st.spinner(f"Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ {api_provider}..."):
                    success = st.session_state.api_client.setup(api_provider, api_key)
                    
                    if success:
                        st.session_state.is_ready = True
                        st.success(f"âœ… ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ {api_provider} Ø¨Ù†Ø¬Ø§Ø­!")
                    else:
                        st.error(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ {api_provider}")
        
        st.divider()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        st.subheader("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        chunk_size = st.slider("ğŸ“ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹Ø© (ÙƒÙ„Ù…Ø©)", 200, 800, 400)
        overlap_size = st.slider("ğŸ”„ Ø§Ù„ØªØ¯Ø§Ø®Ù„ (ÙƒÙ„Ù…Ø©)", 20, 100, 50)
        max_results = st.slider("ğŸ¯ Ø£Ù‚ØµÙ‰ Ù†ØªØ§Ø¦Ø¬", 3, 10, 5)
        
        st.divider()
        
        # Ø£Ø¯ÙˆØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        st.subheader("ğŸ› ï¸ Ø£Ø¯ÙˆØ§Øª")
        
        if st.button("ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØµÙØ­Ø©"):
            st.rerun()
        
        if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"):
            for key in ['documents', 'processed_chunks', 'chat_history']:
                if key in st.session_state:
                    st.session_state[key] = []
            st.session_state.vector_store = SimpleVectorStore()
            st.session_state.is_ready = False
            st.success("âœ… ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
            st.rerun()
    
    # Ø§Ù„ØªØ¨ÙˆÙŠØ¨Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“š Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚", "ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", "ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª", "â„¹ï¸ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©"])
    
    with tab1:
        st.header("ğŸ“š Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
        
        # Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        st.subheader("ğŸ“¤ Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª")
        uploaded_files = st.file_uploader(
            "Ø§Ø®ØªØ± Ø§Ù„Ù…Ù„ÙØ§Øª:",
            accept_multiple_files=True,
            type=['txt', 'pdf', 'docx', 'csv'],
            help="ÙŠÙ…ÙƒÙ† Ø±ÙØ¹ Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª"
        )
        
        # Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù…Ø¨Ø§Ø´Ø±
        with st.expander("âœï¸ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù…Ø¨Ø§Ø´Ø±"):
            direct_text = st.text_area(
                "Ø§Ù„Ù†Øµ:",
                height=200,
                placeholder="Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§..."
            )
            
            col1, col2 = st.columns([1, 3])
            with col1:
                if st.button("â• Ø¥Ø¶Ø§ÙØ©", type="primary"):
                    if direct_text.strip():
                        doc_id = len(st.session_state.documents) + 1
                        st.session_state.documents.append({
                            'id': doc_id,
                            'name': f'Ù†Øµ_Ù…Ø¨Ø§Ø´Ø±_{doc_id}',
                            'content': direct_text,
                            'type': 'Ù†Øµ Ù…Ø¨Ø§Ø´Ø±',
                            'timestamp': datetime.now().isoformat(),
                            'word_count': len(direct_text.split())
                        })
                        st.success("âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Øµ!")
                        st.rerun()
            
            with col2:
                if direct_text:
                    st.info(f"Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {len(direct_text.split())}")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©
        if uploaded_files:
            st.subheader("ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©")
            
            for uploaded_file in uploaded_files:
                with st.expander(f"ğŸ“„ {uploaded_file.name}"):
                    content, file_info = read_uploaded_file(uploaded_file)
                    
                    # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Ø§Ù„Ø­Ø¬Ù…", f"{file_info['size']/1024:.1f} KB")
                    with col2:
                        st.metric("Ø§Ù„Ù†ÙˆØ¹", file_info['type'])
                    with col3:
                        st.metric("Ø§Ù„ÙƒÙ„Ù…Ø§Øª", len(content.split()))
                    
                    # Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                    if not content.startswith("Ø®Ø·Ø£"):
                        preview = content[:300] + "..." if len(content) > 300 else content
                        st.text_area("Ù…Ø¹Ø§ÙŠÙ†Ø©:", preview, height=100, disabled=True)
                        
                        if st.button(f"ğŸ’¾ Ø­ÙØ¸ {uploaded_file.name}", key=f"save_{uploaded_file.name}"):
                            doc_id = len(st.session_state.documents) + 1
                            st.session_state.documents.append({
                                'id': doc_id,
                                'name': uploaded_file.name,
                                'content': content,
                                'type': file_info['type'],
                                'size': file_info['size'],
                                'timestamp': datetime.now().isoformat(),
                                'word_count': len(content.split())
                            })
                            st.success(f"âœ… ØªÙ… Ø­ÙØ¸ {uploaded_file.name}!")
                            st.rerun()
                    else:
                        st.error(content)
        
        # Ø¹Ø±Ø¶ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
        if st.session_state.documents:
            st.divider()
            st.subheader(f"ğŸ“‹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© ({len(st.session_state.documents)})")
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
            docs_df = pd.DataFrame([
                {
                    'Ø§Ù„Ø§Ø³Ù…': doc['name'],
                    'Ø§Ù„Ù†ÙˆØ¹': doc['type'],
                    'Ø§Ù„ÙƒÙ„Ù…Ø§Øª': doc['word_count'],
                    'Ø§Ù„ØªØ§Ø±ÙŠØ®': doc['timestamp'][:10]
                }
                for doc in st.session_state.documents
            ])
            
            st.dataframe(docs_df, use_container_width=True)
            
            # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­ÙƒÙ…
            col1, col2, col3 = st.columns([2, 2, 1])
            
            with col1:
                if st.button("ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚", type="primary"):
                    with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©..."):
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        all_chunks = []
                        all_texts = []
                        
                        for i, doc in enumerate(st.session_state.documents):
                            status_text.text(f"Ù…Ø¹Ø§Ù„Ø¬Ø©: {doc['name']}")
                            
                            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
                            clean_text = st.session_state.processor.clean_arabic_text(doc['content'])
                            
                            # ØªÙ‚Ø³ÙŠÙ… Ù„Ù‚Ø·Ø¹
                            chunks = st.session_state.processor.chunk_text(
                                clean_text, 
                                chunk_size=chunk_size, 
                                overlap=overlap_size
                            )
                            
                            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
                            for j, chunk in enumerate(chunks):
                                chunk.update({
                                    'doc_id': doc['id'],
                                    'doc_name': doc['name'],
                                    'chunk_id': f"{doc['id']}_{j}",
                                    'global_id': len(all_chunks)
                                })
                                all_chunks.append(chunk)
                                all_texts.append(chunk['text'])
                            
                            progress_bar.progress((i + 1) / len(st.session_state.documents))
                        
                        # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«
                        status_text.text("ğŸ” Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«...")
                        success = st.session_state.vector_store.add_documents(all_chunks, all_texts)
                        
                        if success:
                            st.session_state.processed_chunks = all_chunks
                            st.session_state.system_stats['total_processed'] = len(all_chunks)
                            
                            st.success(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {len(all_chunks)} Ù‚Ø·Ø¹Ø© Ù…Ù† {len(st.session_state.documents)} ÙˆØ«ÙŠÙ‚Ø©!")
                        else:
                            st.error("âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«")
                        
                        progress_bar.empty()
                        status_text.empty()
            
            with col2:
                doc_to_delete = st.selectbox(
                    "ğŸ—‘ï¸ Ø­Ø°Ù ÙˆØ«ÙŠÙ‚Ø©:",
                    ["Ø§Ø®ØªØ±..."] + [f"{doc['name']}" for doc in st.session_state.documents]
                )
                
                if doc_to_delete != "Ø§Ø®ØªØ±..." and st.button("ğŸ—‘ï¸ Ø­Ø°Ù"):
                    st.session_state.documents = [
                        doc for doc in st.session_state.documents 
                        if doc['name'] != doc_to_delete
                    ]
                    st.success(f"âœ… ØªÙ… Ø­Ø°Ù {doc_to_delete}")
                    st.rerun()
            
            with col3:
                total_words = sum(doc['word_count'] for doc in st.session_state.documents)
                st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", f"{total_words:,}")
    
    with tab2:
        st.header("ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
        
        if not st.session_state.is_ready:
            st.markdown("""
            <div class="status-warning">
                âš ï¸ <strong>ÙŠØªØ·Ù„Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø£ÙˆÙ„Ø§Ù‹</strong><br>
                ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯ API Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
            </div>
            """, unsafe_allow_html=True)
            return
        
        if not st.session_state.processed_chunks:
            st.markdown("""
            <div class="status-warning">
                âš ï¸ <strong>ÙŠØªØ·Ù„Ø¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø£ÙˆÙ„Ø§Ù‹</strong><br>
                ÙŠØ±Ø¬Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù…Ù† ØªØ¨ÙˆÙŠØ¨ "Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"
            </div>
            """, unsafe_allow_html=True)
            return
        
        st.markdown("""
        <div class="status-success">
            âœ… <strong>Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©!</strong><br>
            ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ ÙˆØ«Ø§Ø¦Ù‚Ùƒ
        </div>
        """, unsafe_allow_html=True)
        
        # Ø¹Ø±Ø¶ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        if st.session_state.chat_history:
            st.subheader("ğŸ“œ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
            
            for i, chat in enumerate(reversed(st.session_state.chat_history[-5:])):  # Ø¢Ø®Ø± 5 Ù…Ø­Ø§Ø¯Ø«Ø§Øª
                with st.container():
                    # Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
                    st.markdown(f"""
                    <div class="chat-message user-message">
                        <strong>ğŸ‘¤ Ø³Ø¤Ø§Ù„Ùƒ:</strong><br>
                        {chat['question']}
                        <br><small>â° {chat['timestamp'][:19].replace('T', ' ')}</small>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
                    st.markdown(f"""
                    <div class="chat-message ai-message">
                        <strong>ğŸ¤– Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:</strong><br>
                        {chat['answer']}
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
                    if chat.get('sources'):
                        with st.expander(f"ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ({len(chat['sources'])})"):
                            for j, source in enumerate(chat['sources'], 1):
                                st.markdown(f"""
                                <div class="doc-chunk">
                                    <strong>Ù…ØµØ¯Ø± {j} - {source['doc_name']}</strong>
                                    <span class="similarity-score">ØªØ´Ø§Ø¨Ù‡: {source['score']:.3f}</span>
                                    <br><br>
                                    {source['text'][:200]}{'...' if len(source['text']) > 200 else ''}
                                </div>
                                """, unsafe_allow_html=True)
                    
                    st.divider()
        
        # Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
        st.subheader("â“ Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„Ùƒ")
        
        with st.form("question_form", clear_on_submit=True):
            user_question = st.text_area(
                "Ø³Ø¤Ø§Ù„Ùƒ:",
                height=100,
                placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©ØŸ",
                help="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø©"
            )
            
            col1, col2, col3 = st.columns([2, 1, 1])
            
            with col1:
                submitted = st.form_submit_button("ğŸš€ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„", type="primary")
            
            with col2:
                search_depth = st.slider("Ø¹Ù…Ù‚ Ø§Ù„Ø¨Ø­Ø«", 3, 8, 5, help="Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©")
            
            with col3:
                min_similarity = st.slider("Ø­Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡", 0.1, 0.8, 0.3, step=0.1, help="Ø£Ù‚Ù„ Ø¯Ø±Ø¬Ø© ØªØ´Ø§Ø¨Ù‡ Ù…Ù‚Ø¨ÙˆÙ„Ø©")
        
        if submitted and user_question.strip():
            with st.spinner("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚..."):
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³
                search_results = st.session_state.vector_store.search(user_question, k=search_depth)
                
                if search_results:
                    # ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                    relevant_results = [r for r in search_results if r['score'] >= min_similarity]
                    
                    if relevant_results:
                        st.session_state.system_stats['total_searches'] += 1
                        
                        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚
                        context_parts = []
                        sources_info = []
                        
                        for result in relevant_results:
                            context_parts.append(result['chunk']['text'])
                            sources_info.append({
                                'doc_name': result['chunk']['doc_name'],
                                'text': result['chunk']['text'],
                                'score': result['score']
                            })
                        
                        context = '\n\n---\n\n'.join(context_parts)
                        
                        # Ø¹Ø±Ø¶ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø±
                        with st.expander(f"ğŸ‘€ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ({len(relevant_results)})"):
                            for i, result in enumerate(relevant_results, 1):
                                st.markdown(f"""
                                **Ù…ØµØ¯Ø± {i}:** {result['chunk']['doc_name']} 
                                **(ØªØ´Ø§Ø¨Ù‡: {result['score']:.3f})**
                                
                                {result['chunk']['text'][:150]}...
                                """)
                        
                        st.divider()
                        
                        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                        with st.spinner("ğŸ¤– Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
                            answer = st.session_state.api_client.generate_response(
                                user_question, 
                                context, 
                                max_tokens=600
                            )
                            
                            if not answer.startswith("âŒ"):
                                st.session_state.system_stats['total_responses'] += 1
                                
                                # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®
                                st.session_state.chat_history.append({
                                    'question': user_question,
                                    'answer': answer,
                                    'sources': sources_info,
                                    'timestamp': datetime.now().isoformat(),
                                    'search_results_count': len(relevant_results)
                                })
                                
                                st.success("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©!")
                                st.rerun()
                            else:
                                st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {answer}")
                    else:
                        st.warning(f"âš ï¸ Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ù…ØªØ´Ø§Ø¨Ù‡Ø© (Ø£Ù‚Ù„ Ù…Ù† {min_similarity:.1f}) Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ. Ø¬Ø±Ø¨:")
                        st.markdown("""
                        - ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                        - Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„
                        - Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
                        """)
                else:
                    st.error("âŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚. ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø£ÙˆÙ„Ø§Ù‹.")
    
    with tab3:
        st.header("ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª")
        
        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ“š</h3>
                <h2>{len(st.session_state.documents)}</h2>
                <p>ÙˆØ«ÙŠÙ‚Ø© Ù…Ø­Ù…Ù„Ø©</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ“„</h3>
                <h2>{len(st.session_state.processed_chunks)}</h2>
                <p>Ù‚Ø·Ø¹Ø© Ù…Ø¹Ø§Ù„Ø¬Ø©</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ’¬</h3>
                <h2>{len(st.session_state.chat_history)}</h2>
                <p>Ù…Ø­Ø§Ø¯Ø«Ø©</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            total_words = sum(doc.get('word_count', 0) for doc in st.session_state.documents)
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ“</h3>
                <h2>{total_words:,}</h2>
                <p>ÙƒÙ„Ù…Ø© Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©</p>
            </div>
            """, unsafe_allow_html=True)
        
        st.divider()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
        if st.session_state.documents:
            st.subheader("ğŸ“‹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
            
            # Ø¬Ø¯ÙˆÙ„ Ù…ÙØµÙ„
            docs_analysis = []
            for doc in st.session_state.documents:
                docs_analysis.append({
                    'Ø§Ù„Ø§Ø³Ù…': doc['name'],
                    'Ø§Ù„Ù†ÙˆØ¹': doc['type'],
                    'Ø§Ù„ÙƒÙ„Ù…Ø§Øª': doc.get('word_count', 0),
                    'Ø§Ù„Ø­Ø¬Ù… (KB)': round(doc.get('size', 0) / 1024, 1) if 'size' in doc else 0,
                    'Ø§Ù„ØªØ§Ø±ÙŠØ®': doc['timestamp'][:10] if 'timestamp' in doc else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'
                })
            
            df_docs = pd.DataFrame(docs_analysis)
            st.dataframe(df_docs, use_container_width=True)
            
            # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
            if len(df_docs) > 1:
                st.subheader("ğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª")
                chart_data = df_docs.set_index('Ø§Ù„Ø§Ø³Ù…')['Ø§Ù„ÙƒÙ„Ù…Ø§Øª']
                st.bar_chart(chart_data)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
        if st.session_state.chat_history:
            st.divider()
            st.subheader("ğŸ’¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª")
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
            questions = [chat['question'] for chat in st.session_state.chat_history]
            answers = [chat['answer'] for chat in st.session_state.chat_history]
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                avg_q_length = np.mean([len(q.split()) for q in questions])
                st.metric("Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„", f"{avg_q_length:.1f} ÙƒÙ„Ù…Ø©")
            
            with col2:
                avg_a_length = np.mean([len(a.split()) for a in answers])
                st.metric("Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©", f"{avg_a_length:.1f} ÙƒÙ„Ù…Ø©")
            
            with col3:
                avg_sources = np.mean([len(chat.get('sources', [])) for chat in st.session_state.chat_history])
                st.metric("Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØµØ§Ø¯Ø±/Ø¥Ø¬Ø§Ø¨Ø©", f"{avg_sources:.1f}")
            
            # Ø£Ø­Ø¯Ø« Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
            st.subheader("ğŸ•’ Ø£Ø­Ø¯Ø« Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª")
            recent_chats = st.session_state.chat_history[-3:] if len(st.session_state.chat_history) >= 3 else st.session_state.chat_history
            
            for i, chat in enumerate(reversed(recent_chats), 1):
                with st.expander(f"Ù…Ø­Ø§Ø¯Ø«Ø© {i}: {chat['question'][:50]}..."):
                    st.write(f"**Ø§Ù„Ø³Ø¤Ø§Ù„:** {chat['question']}")
                    st.write(f"**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:** {chat['answer']}")
                    st.write(f"**Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø±:** {len(chat.get('sources', []))}")
                    st.write(f"**Ø§Ù„ØªØ§Ø±ÙŠØ®:** {chat['timestamp'][:19].replace('T', ' ')}")
        
        st.divider()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        st.subheader("ğŸ–¥ï¸ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©", st.session_state.system_stats['total_processed'])
        
        with col2:
            st.metric("Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø«", st.session_state.system_stats['total_searches'])
        
        with col3:
            st.metric("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©", st.session_state.system_stats['total_responses'])
        
        # ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        st.divider()
        st.subheader("ğŸ’¾ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ“Š ØªØµØ¯ÙŠØ± Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª (CSV)") and st.session_state.documents:
                csv_data = pd.DataFrame(docs_analysis).to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ’¾ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù CSV",
                    data=csv_data,
                    file_name=f"rag_statistics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
        
        with col2:
            if st.button("ğŸ’¬ ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª (JSON)") and st.session_state.chat_history:
                export_data = {
                    'export_info': {
                        'date': datetime.now().isoformat(),
                        'total_conversations': len(st.session_state.chat_history),
                        'app_version': '2.0'
                    },
                    'conversations': st.session_state.chat_history,
                    'statistics': st.session_state.system_stats
                }
                
                json_data = json.dumps(export_data, ensure_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ’¾ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù JSON",
                    data=json_data,
                    file_name=f"rag_conversations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
    
    with tab4:
        st.header("â„¹ï¸ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙˆØ§Ù„Ø¯Ù„ÙŠÙ„")
        
        # Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        st.subheader("ğŸš€ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹")
        
        with st.expander("1ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…", expanded=True):
            st.markdown("""
            **Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ**
            
            1. Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØŒ Ø§Ø®ØªØ± Ù…Ù‚Ø¯Ù… Ø§Ù„Ø®Ø¯Ù…Ø© (OpenAI Ø£Ùˆ Groq)
            2. Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ API Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ
            3. Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ "Ø§ØªØµØ§Ù„" ÙˆØ§Ù†ØªØ¸Ø± Ø§Ù„ØªØ£ÙƒÙŠØ¯
            
            **Ù…ÙØ§ØªÙŠØ­ API Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:**
            - **OpenAI**: Ø§Ø­ØµÙ„ Ø¹Ù„ÙŠÙ‡ Ù…Ù† [platform.openai.com](https://platform.openai.com/api-keys)
            - **Groq**: Ø§Ø­ØµÙ„ Ø¹Ù„ÙŠÙ‡ Ù…Ù† [console.groq.com](https://console.groq.com/keys)
            """)
        
        with st.expander("2ï¸âƒ£ Ø±ÙØ¹ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"):
            st.markdown("""
            **Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚:**
            - Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© (TXT, PDF, DOC) Ø£Ùˆ
            - Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø§Ù„Ù…Ø±Ø¨Ø¹
            
            **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚:**
            1. Ø¨Ø¹Ø¯ Ø±ÙØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§ØªØŒ Ø§Ø¶ØºØ· "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"
            2. Ø³ÙŠØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ù„Ù‚Ø·Ø¹ ØµØºÙŠØ±Ø©
            3. Ø§Ù†ØªØ¸Ø± Ø­ØªÙ‰ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«
            
            **Ù†ØµØ§Ø¦Ø­:**
            - Ø§Ø³ØªØ®Ø¯Ù… Ù†ØµÙˆØµ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ù†Ø¸Ù…Ø©
            - ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØµÙˆØ± ÙÙ‚Ø·
            - Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„
            """)
        
        with st.expander("3ï¸âƒ£ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"):
            st.markdown("""
            **Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:**
            - Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ¯Ù‚Ø©
            - Ø§Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ ÙˆØ«Ø§Ø¦Ù‚Ùƒ
            - ÙŠÙ…ÙƒÙ† Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø©
            
            **Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨Ø­Ø«:**
            - **Ø¹Ù…Ù‚ Ø§Ù„Ø¨Ø­Ø«**: Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© (3-8)
            - **Ø­Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡**: Ø£Ù‚Ù„ Ø¯Ø±Ø¬Ø© ØªØ´Ø§Ø¨Ù‡ Ù…Ù‚Ø¨ÙˆÙ„Ø© (0.1-0.8)
            
            **Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:**
            - "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©ØŸ"
            - "Ø§Ø´Ø±Ø­ Ù„ÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„ÙÙ„Ø§Ù†ÙŠ"
            - "Ù…Ø§ Ù‡ÙŠ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©ØŸ"
            """)
        
        with st.expander("4ï¸âƒ£ ÙÙ‡Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"):
            st.markdown("""
            **Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª:**
            - ÙƒÙ„ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ù‚Ø·Ø¹ Ù…Ø­Ø¯Ø¯Ø© Ù…Ù† ÙˆØ«Ø§Ø¦Ù‚Ùƒ
            - Ø±Ø§Ø¬Ø¹ "Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©" Ù„ØªØ±Ù‰ Ù…Ù† Ø£ÙŠÙ† Ø¬Ø§Ø¡Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
            - Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ØªØ¸Ù‡Ø± Ù…Ø¯Ù‰ ØµÙ„Ø© Ø§Ù„Ù…ØµØ¯Ø± Ø¨Ø³Ø¤Ø§Ù„Ùƒ
            
            **Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:**
            - Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            - ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ÙˆØ¥Ø­ØµØ§Ø¦ÙŠØ§ØªÙ‡Ø§
            - Ø¥Ù…ÙƒØ§Ù†ÙŠØ© ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©
            """)
        
        st.divider()
        
        # Ù†ØµØ§Ø¦Ø­ ÙˆØ­Ù„ÙˆÙ„ Ù…Ø´Ø§ÙƒÙ„ Ø´Ø§Ø¦Ø¹Ø©
        st.subheader("ğŸ’¡ Ù†ØµØ§Ø¦Ø­ ÙˆØ­Ù„ÙˆÙ„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„")
        
        with st.expander("ğŸ”§ Ù…Ø´Ø§ÙƒÙ„ Ø´Ø§Ø¦Ø¹Ø© ÙˆØ­Ù„ÙˆÙ„Ù‡Ø§"):
            st.markdown("""
            **âŒ "ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ API":**
            - ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ù…ÙØªØ§Ø­ API
            - ØªØ­Ù‚Ù‚ Ù…Ù† Ø±ØµÙŠØ¯ Ø­Ø³Ø§Ø¨Ùƒ
            - Ø¬Ø±Ø¨ Ù…Ù‚Ø¯Ù… Ø®Ø¯Ù…Ø© Ø¢Ø®Ø±
            
            **âŒ "Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©":**
            - Ù‚Ù„Ù„ Ø­Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            - Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„
            - ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
            
            **âŒ "Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚":**
            - ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ©
            - Ø¬Ø±Ø¨ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ Ù…Ø¨Ø§Ø´Ø±Ø©
            - ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Øµ Ù„ÙŠØ³ ÙØ§Ø±ØºØ§Ù‹
            
            **âš ï¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù…ØªÙ„Ø¦Ø©:**
            - Ø§Ø­Ø°Ù Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©
            - Ø§Ù…Ø³Ø­ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
            - Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
            """)
        
        with st.expander("ğŸ“ˆ Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù†ØªØ§Ø¦Ø¬"):
            st.markdown("""
            **Ø¬ÙˆØ¯Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚:**
            - Ø§Ø³ØªØ®Ø¯Ù… Ù†ØµÙˆØµ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ù†Ø³Ù‚Ø©
            - ØªØ¬Ù†Ø¨ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù„ÙŠØ¦Ø© Ø¨Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
            - Ø±ØªØ¨ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ
            
            **ØµÙŠØ§ØºØ© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:**
            - Ø§Ø¬Ø¹Ù„ Ø£Ø³Ø¦Ù„ØªÙƒ Ù…Ø­Ø¯Ø¯Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
            - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ ÙˆØ«Ø§Ø¦Ù‚Ùƒ
            - Ø¬Ø±Ø¨ ØµÙŠØ§ØºØ§Øª Ù…Ø®ØªÙ„ÙØ© Ø¥Ø°Ø§ Ù„Ù… ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø©
            
            **Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø«Ù„Ù‰:**
            - Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©: Ø²Ø¯ Ø¹Ù…Ù‚ Ø§Ù„Ø¨Ø­Ø« (6-8)
            - Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù‚ÙŠÙ‚: Ø§Ø±ÙØ¹ Ø­Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (0.5-0.7)
            - Ù„Ù„Ø¨Ø­Ø« Ø§Ù„ÙˆØ§Ø³Ø¹: Ù‚Ù„Ù„ Ø­Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (0.2-0.4)
            """)
        
        st.divider()
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙ‚Ù†ÙŠØ©
        st.subheader("ğŸ”¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙ‚Ù†ÙŠØ©")
        
        st.markdown("""
        **Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:**
        - **TF-IDF**: Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù„ÙÙŠÙƒØªÙˆØ±Ø§Øª
        - **Cosine Similarity**: Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        - **Text Chunking**: Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
        - **RESTful APIs**: Ù„Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ©
        
        **Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:**
        - OpenAI: GPT-3.5-turbo (Ø§ÙØªØ±Ø§Ø¶ÙŠ)
        - Groq: Llama-3.1-70b-versatile
        
        **Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù†Ø¸Ø§Ù…:**
        - Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ù†Øµ: Ø­Ø³Ø¨ API Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        - Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
        - Ø¯Ù‚Ø© Ø§Ù„Ø¨Ø­Ø«: ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†ØµÙˆØµ
        """)
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ ÙˆØ§Ù„Ø¯Ø¹Ù…
        st.divider()
        st.subheader("ğŸ“ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **Ø¥Ø¨Ù„Ø§Øº Ø¹Ù† Ù…Ø´ÙƒÙ„Ø©:**
            - ØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„
            - Ø§Ø°ÙƒØ± Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙŠ Ù‚Ù…Øª Ø¨Ù‡Ø§
            - Ø£Ø±ÙÙ‚ Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© Ø¥Ù† Ø£Ù…ÙƒÙ†
            """)
        
        with col2:
            st.markdown("""
            **Ø·Ù„Ø¨ ØªØ­Ø³ÙŠÙ†Ø§Øª:**
            - Ø§Ù‚ØªØ±Ø­ Ù…ÙŠØ²Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©
            - Ø´Ø§Ø±Ùƒ ØªØ¬Ø±Ø¨ØªÙƒ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
            - Ù‚ÙŠÙ… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆÙ…Ø¯Ù‰ ÙØ§Ø¦Ø¯ØªÙ‡
            """)

if __name__ == "__main__":
    main()
